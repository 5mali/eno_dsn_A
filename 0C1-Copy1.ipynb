{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "from datetime import datetime\n",
    "tic = datetime.now()\n",
    "\n",
    "import os\n",
    "from os.path import dirname, abspath, join\n",
    "from os import getcwd\n",
    "import sys\n",
    "\n",
    "# THIS_DIR = getcwd()\n",
    "# CLASS_DIR = abspath(join(THIS_DIR, 'dsnclasses'))  #abspath(join(THIS_DIR, '../../..', 'dsnclasses'))\n",
    "# sys.path.append(CLASS_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import string\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.ticker as ticker\n",
    "import torch\n",
    "from IPython.display import clear_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 314\n",
    "random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "torch.cuda.manual_seed(seed)\n",
    "np.random.seed(seed)\n",
    "os.environ['PYTHONHASHSEED'] = str(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ENO(object):\n",
    "    \n",
    "    #no. of forecast types is 6 ranging from 0 to 5\n",
    "  \n",
    "    def __init__(self, location='tokyo', year=2010, shuffle=False, day_balance=False):\n",
    "        self.location = location\n",
    "        self.year = year\n",
    "        self.day = None\n",
    "        self.hr = None\n",
    "        \n",
    "        self.shuffle = shuffle\n",
    "        self.day_balance = day_balance\n",
    "\n",
    "        self.TIME_STEPS = None #no. of time steps in one episode\n",
    "        self.NO_OF_DAYS = None #no. of days in one year\n",
    "        \n",
    "        self.NO_OF_DAYTYPE = 10 #no. of daytypes\n",
    "        self.daycounter = 0 #to count number of days that have been passed\n",
    "        \n",
    "        self.sradiation = None #matrix with GSR for the entire year\n",
    "        self.senergy = None #matrix with harvested energy data for the entire year\n",
    "        self.fforecast = None #array with forecast values for each day\n",
    "        \n",
    "\n",
    "        self.henergy = None #harvested energy variable\n",
    "        self.fcast = None #forecast variable\n",
    "        self.sorted_days = [] #days sorted according to day type\n",
    "        \n",
    "        self.SMAX = 1000 # 1 Watt Solar Panel\n",
    "\n",
    "    \n",
    "    #function to get the solar data for the given location and year and prep it\n",
    "    def get_data(self):\n",
    "        #solar_data/CSV files contain the values of GSR (Global Solar Radiation in MegaJoules per meters squared per hour)\n",
    "        #weather_data/CSV files contain the weather summary from 06:00 to 18:00 and 18:00 to 06:00+1\n",
    "        location = self.location\n",
    "        year = self.year\n",
    "\n",
    "        THIS_DIR = getcwd()\n",
    "        SDATA_DIR = abspath(join(THIS_DIR, 'solar_data'))  #abspath(join(THIS_DIR, '../../..', 'data'))\n",
    "        \n",
    "        sfile = SDATA_DIR + '/' + location +'/' + str(year) + '.csv'\n",
    "        \n",
    "        #skiprows=4 to remove unnecessary title texts\n",
    "        #usecols=4 to read only the Global Solar Radiation (GSR) values\n",
    "        solar_radiation = pd.read_csv(sfile, skiprows=4, encoding='shift_jisx0213', usecols=[4])\n",
    "      \n",
    "        #convert dataframe to numpy array\n",
    "        solar_radiation = solar_radiation.values\n",
    "\n",
    "        #convert missing data in CSV files to zero\n",
    "        solar_radiation[np.isnan(solar_radiation)] = 0\n",
    "\n",
    "        #reshape solar_radiation into no_of_daysx24 array\n",
    "        solar_radiation = solar_radiation.reshape(-1,24)\n",
    "\n",
    "        if(self.shuffle): #if class instatiation calls for shuffling the day order. Required when learning\n",
    "            np.random.shuffle(solar_radiation) \n",
    "        self.sradiation = solar_radiation\n",
    "        \n",
    "        #GSR values (in MJ/sq.mts per hour) need to be expressed in mW\n",
    "        # Conversion is accomplished by \n",
    "        # solar_energy = GSR(in MJ/m2/hr) * 1e6 * size of solar cell * efficiency of solar cell /(60x60) *1000 (to express in mW)\n",
    "        # the factor of 2 in the end is assuming two solar cells\n",
    "        self.senergy = 2*self.sradiation * 1e6 * (55e-3 * 70e-3) * 0.15 * 1000/(60*60)\n",
    "\n",
    "        return 0\n",
    "    \n",
    "    #function to map total day radiation into type of day ranging from 0 to 5\n",
    "    #the classification into day types is quite arbitrary. There is no solid logic behind this type of classification.\n",
    "    \n",
    "    def get_day_state(self,tot_day_radiation):\n",
    "        bin_edges = np.array([0, 3.5, 6.5, 9.0, 12.5, 15.5, 18.5, 22.0, 25, 28])\n",
    "        for k in np.arange(1,bin_edges.size):\n",
    "            if (bin_edges[k-1] < tot_day_radiation <= bin_edges[k]):\n",
    "                day_state = k -1\n",
    "            else:\n",
    "                day_state = bin_edges.size - 1\n",
    "        return int(day_state)\n",
    "    \n",
    "    def get_forecast(self):\n",
    "        #create a perfect forecaster.\n",
    "        tot_day_radiation = np.sum(self.sradiation, axis=1) #contains total solar radiation for each day\n",
    "        get_day_state = np.vectorize(self.get_day_state)\n",
    "        self.fforecast = get_day_state(tot_day_radiation)\n",
    "        \n",
    "        #sort days depending on the type of day and shuffle them; maybe required when learning\n",
    "        for fcast in range(0,6):\n",
    "            fcast_days = ([i for i,x in enumerate(self.fforecast) if x == fcast])\n",
    "            np.random.shuffle(fcast_days)\n",
    "            self.sorted_days.append(fcast_days)\n",
    "        return 0\n",
    "    \n",
    "    def reset(self,day=0): #it is possible to reset to the beginning of a certain day\n",
    "        \n",
    "        self.get_data() #first get data for the given year\n",
    "        self.get_forecast() #calculate the forecast\n",
    "        \n",
    "        self.TIME_STEPS = self.senergy.shape[1]\n",
    "        self.NO_OF_DAYS = self.senergy.shape[0]\n",
    "        \n",
    "        self.day = day\n",
    "        self.hr = 0\n",
    "        \n",
    "        self.henergy = self.senergy[self.day][self.hr]\n",
    "        self.fcast = self.fforecast[self.day]\n",
    "        \n",
    "        end_of_day = False\n",
    "        end_of_year = False\n",
    "        return [self.henergy, self.fcast, end_of_day, end_of_year]\n",
    "\n",
    "    \n",
    "    def step(self):\n",
    "        end_of_day = False\n",
    "        end_of_year = False\n",
    "        if not(self.day_balance): #if daytype balance is not required\n",
    "            if(self.hr < self.TIME_STEPS - 1):\n",
    "                self.hr += 1\n",
    "                self.henergy = self.senergy[self.day][self.hr] \n",
    "                self.fcast = self.fforecast[self.day]\n",
    "            else:\n",
    "                if(self.day < self.NO_OF_DAYS -1):\n",
    "                    end_of_day = True\n",
    "                    self.hr = 0\n",
    "                    self.day += 1\n",
    "                    self.henergy = self.senergy[self.day][self.hr] \n",
    "                    self.fcast = self.fforecast[self.day]\n",
    "                else:\n",
    "                    end_of_day = True\n",
    "                    end_of_year = True\n",
    "                    \n",
    "        else: #when training, we want all daytypes to be equally represented for robust policy\n",
    "              #obviously, the days are going to be in random order\n",
    "            if(self.hr < self.TIME_STEPS - 1):\n",
    "                self.hr += 1\n",
    "                self.henergy = self.senergy[self.day][self.hr]\n",
    "                self.fcast = self.fforecast[self.day]\n",
    "            else:\n",
    "                if(self.daycounter < self.NO_OF_DAYS -1):\n",
    "                    end_of_day = True\n",
    "                    self.daycounter += 1\n",
    "                    self.hr = 0\n",
    "                    daytype = random.choice(np.arange(0,self.NO_OF_DAYTYPE)) #choose random daytype\n",
    "                    self.day = np.random.choice(self.sorted_days[daytype]) #choose random day from that daytype\n",
    "                    self.henergy = self.senergy[self.day][self.hr] \n",
    "                    self.fcast = self.fforecast[self.day]\n",
    "                else: \n",
    "                    end_of_day = True\n",
    "                    end_of_year = True\n",
    "                    self.daycounter = 0\n",
    "        \n",
    "        \n",
    "        return [self.henergy, self.fcast, end_of_day, end_of_year]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CAPM (object):\n",
    "    def __init__(self,location='tokyo', year=2010, shuffle=False, trainmode=False):\n",
    "\n",
    "        #all energy values i.e. BMIN, BMAX, BOPT, HMAX are in mWhr. Assuming one timestep is one hour\n",
    "        \n",
    "        self.BMIN = 0.0                #Minimum battery level that is tolerated. Maybe non-zero also\n",
    "        self.BMAX = 9250.0            #Max Battery Level. May not necessarily be equal to total batter capacity [3.6V x 2500mAh]\n",
    "        self.BOPT = 0.5 * self.BMAX    #Optimal Battery Level. Assuming 50% of battery is the optimum\n",
    "        \n",
    "        self.HMIN = 0      #Minimum energy that can be harvested by the solar panel.\n",
    "        self.HMAX = None   #Maximum energy that can be harvested by the solar panel. [500mW]\n",
    "        \n",
    "        self.DMAX = 500      #Maximum energy that can be consumed by the node in one time step. [~ 3.6V x 135mA]\n",
    "        self.N_ACTIONS = 10  #No. of different duty cycles possible\n",
    "        self.DMIN = self.DMAX/self.N_ACTIONS #Minimum energy that can be consumed by the node in one time step. [~ 3.6V x 15mA]\n",
    "        \n",
    "        self.binit = None     #battery at the beginning of day\n",
    "        self.btrack = []      #track the mean battery level for each day\n",
    "        self.atrack = []      #track the duty cycles for each day\n",
    "        self.batt = None      #battery variable\n",
    "        self.enp = None       #enp at end of hr\n",
    "        self.henergy = None   #harvested energy variable\n",
    "        self.fcast = None     #forecast variable\n",
    "        \n",
    "        self.MUBATT = 0.6\n",
    "        self.SDBATT = 0.02\n",
    "        \n",
    "        self.MUHENERGY = 0.5\n",
    "        self.SDHENERGY = 0.2\n",
    "        \n",
    "        self.MUENP = 0\n",
    "        self.SDENP = 0.02\n",
    "        \n",
    "        self.location = location\n",
    "        self.year = year\n",
    "        self.shuffle = shuffle\n",
    "        self.trainmode = trainmode\n",
    "        self.eno = None#ENO(self.location, self.year, shuffle=shuffle, day_balance=trainmode) #if trainmode is enable, then days are automatically balanced according to daytype i.e. day_balance= True\n",
    "        \n",
    "        self.day_violation_flag = False\n",
    "        self.violation_flag = False\n",
    "        self.violation_counter = 0\n",
    "\n",
    "        self.NO_OF_DAYTYPE = 10 #no. of daytypes\n",
    " \n",
    "    def reset(self,day=0,batt=-1):\n",
    "        henergy, fcast, day_end, year_end = self.eno.reset(day) #reset the eno environment\n",
    "        self.violation_flag = False\n",
    "        self.violation_counter = 0\n",
    "        if(batt == -1):\n",
    "            self.batt = self.BOPT\n",
    "        else:\n",
    "            self.batt = batt\n",
    "            \n",
    "        self.batt = np.clip(self.batt, self.BMIN, self.BMAX)\n",
    "        self.binit = self.batt\n",
    "        self.btrack = np.append(self.btrack, self.batt) #track battery levels\n",
    "\n",
    "#         self.enp = self.BOPT - self.batt\n",
    "        self.enp = self.binit - self.batt #enp is calculated\n",
    "        self.henergy = np.clip(henergy, self.HMIN, self.HMAX) #clip henergy within HMIN and HMAX\n",
    "        self.fcast = fcast\n",
    "        \n",
    "        norm_batt = self.batt/self.BMAX\n",
    "        norm_enp = self.enp/(self.BMAX/2)\n",
    "        norm_henergy = self.henergy/self.HMAX\n",
    "        norm_fcast = self.fcast/(self.NO_OF_DAYTYPE-1)\n",
    "\n",
    "        c_state = [norm_batt, norm_enp, norm_henergy, norm_fcast] #continuous states\n",
    "        reward = 0\n",
    "        \n",
    "        return [c_state, reward, day_end, year_end]\n",
    "    \n",
    "    def getstate(self): #query the present state of the system\n",
    "        norm_batt = self.batt/self.BMAX - self.MUBATT\n",
    "        norm_enp = self.enp/(self.BMAX/2)\n",
    "        norm_henergy = self.henergy/self.HMAX\n",
    "        norm_fcast = self.fcast/(self.NO_OF_DAYTYPE-1)        \n",
    "        c_state = [norm_batt, norm_enp, norm_henergy] #continuous states\n",
    "\n",
    "        return c_state\n",
    "    \n",
    "#     def rewardfn(self):\n",
    "#         R_PARAM = 20000 #chosen empirically for best results\n",
    "#         mu = 0\n",
    "#         sig = 0.07*R_PARAM #knee curve starts at approx. 2000mWhr of deviation\n",
    "#         norm_reward = 3*(np.exp(-np.power((self.enp - mu)/sig, 2.)/2) / np.exp(-np.power((0 - mu)/sig, 2.)/2))-1\n",
    "\n",
    "        \n",
    "# #         if(np.abs(self.enp) <= 0.12*R_PARAM):\n",
    "# #             norm_reward = 2*(np.exp(-np.power((self.enp - mu)/sig, 2.)/2) / np.exp(-np.power((0 - mu)/sig, 2.)/2))\n",
    "# #         else:\n",
    "# #             norm_reward = -0.25 - 10*np.abs(self.enp/R_PARAM)\n",
    "#         if(self.day_violation_flag):\n",
    "#             norm_reward -= 3\n",
    "            \n",
    "#         return (norm_reward)\n",
    "        \n",
    "    \n",
    "    #reward function\n",
    "    def rewardfn(self):\n",
    "        \n",
    "        #FIRST REWARD AS A FUNCTION OF DRIFT OF BMEAN FROM BOPT i.e. in terms of BDEV = |BMEAN-BOPT|/BMAX\n",
    "        bmean = np.mean(self.btrack)\n",
    "        bdev = np.abs(self.BOPT - bmean)/self.BMAX\n",
    "        # based on the sigmoid function\n",
    "        # bdev ranges from bdev = (0,0.5) of BMAX\n",
    "        p1_sharpness = 10\n",
    "        n1_sharpness = 20\n",
    "        shift1 = 0.5\n",
    "        # r1(x) = 0.5 when x = 0.25. \n",
    "        # Therefore, shift = 0.5 to make sure that (2*x-shift) evaluates to zero at x = 0.25\n",
    "\n",
    "        if(bdev<=0.25): \n",
    "            r1 = 2*(1-(1 / (1 + np.exp(-p1_sharpness*(2*bdev-shift1)))))-1\n",
    "        else: \n",
    "            r1 = 2*(1-(1 / (1 + np.exp(-n1_sharpness*(2*bdev-shift1)))))-1\n",
    "        # r1 ranges from -1 to 1\n",
    "            \n",
    "        #SECOND REWARD AS A FUNCTION OF ENP AS LONG AS BMAX/4 <= batt <= 3*BMAX/4 i.e. bdev <= 0.25\n",
    "        if(bdev <=0.25):\n",
    "            # enp ranges from enp = (0,3) of DMAX\n",
    "            p2_sharpness = 2\n",
    "            n2_sharpness = 2\n",
    "            shift2 = 6    \n",
    "            # r1(x) = 0.5 when x = 2. \n",
    "            # Therefore, shift = 6 to make sure that (3*x-shift) evaluates to zero at x = 2\n",
    "#             print('Day energy', np.sum(self.eno.senergy[self.eno.day]))\n",
    "#             print('Node energy', np.sum(self.atrack)*self.DMAX/self.N_ACTIONS)\n",
    "#             x = np.abs(np.sum(self.eno.senergy[self.eno.day])-np.sum(self.atrack)*self.DMAX/self.N_ACTIONS )/self.DMAX\n",
    "            x = np.abs(self.enp/self.DMAX)\n",
    "            if(x<=2): \n",
    "                r2 = (1 / (1 + np.exp(p2_sharpness*(3*x-shift2))))\n",
    "            else: \n",
    "                r2 = (1 / (1 + np.exp(n2_sharpness*(3*x-shift2))))\n",
    "        else:\n",
    "            r2 = 0 # if mean battery lies outside bdev limits, then enp reward is not considered.\n",
    "        # r2 ranges from 0 to 1\n",
    "\n",
    "        #REWARD AS A FUNCTION OF BATTERY VIOLATIONS\n",
    "        if(self.day_violation_flag):\n",
    "            violation_penalty = 3\n",
    "        else:\n",
    "            violation_penalty = 0 #penalty for violating battery limits anytime during the day\n",
    "        \n",
    "#         print(\"Reward \", (r1 + r2 - violation_penalty), '\\n')\n",
    "        return (r1*(2**r2) - violation_penalty)\n",
    "    \n",
    "    def step(self, action):\n",
    "        day_end = False\n",
    "        year_end = False\n",
    "        self.violation_flag = False\n",
    "        reward = 0\n",
    "       \n",
    "        action = np.clip(action, 0, self.N_ACTIONS-1) #action values range from (0 to N_ACTIONS-1)\n",
    "        self.atrack = np.append(self.atrack, action+1) #track duty cycles\n",
    "        e_consumed = (action+1)*self.DMAX/self.N_ACTIONS   #energy consumed by the node\n",
    "        \n",
    "        self.batt += (self.henergy - e_consumed)\n",
    "        if(self.batt < 0.02*self.BMAX or self.batt > 0.98*self.BMAX ):\n",
    "            self.violation_flag = True #penalty for violating battery limits everytime it happens\n",
    "            reward = -2\n",
    "        if(self.batt < 0.02*self.BMAX):\n",
    "            reward -= 2\n",
    "            \n",
    "        if(self.violation_flag):\n",
    "            if(self.day_violation_flag == False): #penalty for violating battery limits anytime during the day - triggers once everyday\n",
    "                self.violation_counter += 1\n",
    "                self.day_violation_flag = True\n",
    "            \n",
    "        self.batt = np.clip(self.batt, self.BMIN, self.BMAX) #clip battery values within permitted level\n",
    "        self.btrack = np.append(self.btrack, self.batt) #track battery levels\n",
    "\n",
    "#         self.enp = self.BOPT - self.batt \n",
    "        self.enp = self.binit - self.atrack.sum()*self.DMAX/self.N_ACTIONS\n",
    "        \n",
    "        #proceed to the next time step\n",
    "        self.henergy, self.fcast, day_end, year_end = self.eno.step()\n",
    "        self.henergy = np.clip(self.henergy, self.HMIN, self.HMAX) #clip henergy within HMIN and HMAX\n",
    "                \n",
    "        if(day_end): #if eno object flags that the day has ended then give reward\n",
    "            reward += self.rewardfn()\n",
    "             \n",
    "            if (self.trainmode): #reset battery to optimal level if limits are exceeded when training\n",
    "#                 self.batt = np.random.uniform(self.DMAX*self.eno.TIME_STEPS/self.BMAX,0.8)*self.BMAX\n",
    "#                 if (self.violation_flag):\n",
    "                if np.random.uniform() < HELP : #occasionaly reset the battery\n",
    "                    self.batt = self.BOPT  \n",
    "            \n",
    "            self.day_violation_flag = False\n",
    "            self.binit = self.batt #this will be the new initial battery level for next day\n",
    "            self.btrack = [] #clear battery tracker\n",
    "            self.atrack = [] #clear duty cycle tracker\n",
    "            \n",
    "                    \n",
    "                \n",
    "        norm_batt = self.batt/self.BMAX\n",
    "        norm_enp = self.enp/(self.BMAX/2)\n",
    "        norm_henergy = self.henergy/self.HMAX\n",
    "        norm_fcast = self.fcast/(self.NO_OF_DAYTYPE-1)\n",
    "\n",
    "        c_state = [norm_batt, norm_enp, norm_henergy, norm_fcast] #continuous states\n",
    "        return [c_state, reward, day_end, year_end]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyper Parameters\n",
    "BATCH_SIZE = 32\n",
    "LR = 0.0001          # learning rate\n",
    "EPSILON = 0.9               # greedy policy\n",
    "GAMMA = 0.9                 # reward discount\n",
    "LAMBDA = 0.9                # parameter decay\n",
    "TARGET_REPLACE_ITER = 24*7*4*18    # target update frequency (every two months)\n",
    "MEMORY_CAPACITY     = 24*7*4*12*2      # store upto six month worth of memory   \n",
    "\n",
    "N_ACTIONS = 10 #no. of duty cycles (0,1,2,3,4)\n",
    "N_STATES = 4 #number of state space parameter [batt, enp, henergy, fcast]\n",
    "\n",
    "HIDDEN_LAYER = 50\n",
    "NO_OF_ITERATIONS = 50\n",
    "GPU = False\n",
    "HELP = 0.05"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "\n",
    "#Class definitions for NN model and learning algorithm\n",
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "\n",
    "        self.fc1 = nn.Linear(N_STATES, HIDDEN_LAYER)\n",
    "        nn.init.kaiming_uniform_(self.fc1.weight)\n",
    "        \n",
    "        self.fc2 = nn.Linear(HIDDEN_LAYER, HIDDEN_LAYER)\n",
    "        nn.init.kaiming_uniform_(self.fc2.weight)\n",
    "\n",
    "        self.out = nn.Linear(HIDDEN_LAYER, N_ACTIONS)\n",
    "        nn.init.xavier_uniform_(self.out.weight) \n",
    "\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.fc1(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.fc2(x)\n",
    "        x = F.relu(x)\n",
    "        actions_value = self.out(x)\n",
    "        return actions_value\n",
    "    \n",
    "class DQN(object):\n",
    "    def __init__(self):\n",
    "        if(GPU): \n",
    "            device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "        else:\n",
    "            device = torch.device('cpu')\n",
    "        self.eval_net, self.target_net = Net(), Net()\n",
    "        self.eval_net.to(device)\n",
    "        self.target_net.to(device)\n",
    "        self.device = device\n",
    "#         print(\"Neural net\")\n",
    "#         print(self.eval_net)\n",
    "        self.learn_step_counter = 0                                     # for target updating\n",
    "        self.memory_counter = 0                                         # for storing memory\n",
    "        self.memory = np.zeros((MEMORY_CAPACITY, N_STATES * 2 + 2))     # initialize memory [mem: ([s], a, r, [s_]) ]\n",
    "        self.optimizer = torch.optim.Adam(self.eval_net.parameters(), lr=LR, weight_decay=1e-3)\n",
    "        self.loss_func = nn.MSELoss()\n",
    "        self.nettoggle = False\n",
    "\n",
    "    def choose_action(self, x):\n",
    "        x = torch.unsqueeze(torch.FloatTensor(x), 0)\n",
    "        x = x.to(self.device)\n",
    "\n",
    "        # input only one sample\n",
    "        if np.random.uniform() < EPSILON:   # greedy\n",
    "            actions_value = self.eval_net.forward(x)\n",
    "            actions_value = actions_value.to(torch.device(\"cpu\"))\n",
    "            action = torch.max(actions_value, 1)[1].data.numpy()\n",
    "            action = action[0] # return the argmax index\n",
    "        else:   # random\n",
    "            action = np.random.randint(0, N_ACTIONS)\n",
    "            action = action\n",
    "        return action\n",
    "    \n",
    "    def choose_greedy_action(self, x):\n",
    "        x = torch.unsqueeze(torch.FloatTensor(x), 0)\n",
    "        x = x.to(self.device)\n",
    "\n",
    "        # input only one sample\n",
    "        if True:   # greedy\n",
    "            actions_value = self.eval_net.forward(x)\n",
    "            actions_value = actions_value.to(torch.device(\"cpu\"))\n",
    "            action = torch.max(actions_value, 1)[1].data.numpy()\n",
    "            action = action[0] # return the argmax index\n",
    "        return action\n",
    "\n",
    "    def store_transition(self, s, a, r, s_):\n",
    "        transition = np.hstack((s, [a, r], s_))\n",
    "        # replace the old memory with new memory\n",
    "        index = self.memory_counter % MEMORY_CAPACITY\n",
    "        self.memory[index, :] = transition\n",
    "        self.memory_counter += 1\n",
    "    \n",
    "    def store_day_transition(self, transition_rec):\n",
    "        data = transition_rec\n",
    "        index = self.memory_counter % MEMORY_CAPACITY\n",
    "        self.memory= np.insert(self.memory, index, data,0)\n",
    "        self.memory_counter += transition_rec.shape[0]\n",
    "\n",
    "    def learn(self):\n",
    "        # target parameter update\n",
    "        if self.learn_step_counter % TARGET_REPLACE_ITER == 0:\n",
    "            self.target_net.load_state_dict(self.eval_net.state_dict())\n",
    "            self.nettoggle = not self.nettoggle\n",
    "        self.learn_step_counter += 1\n",
    "\n",
    "        # sample batch transitions\n",
    "        sample_index = np.random.choice(MEMORY_CAPACITY, BATCH_SIZE)\n",
    "        b_memory = self.memory[sample_index, :]\n",
    "        b_s = torch.FloatTensor(b_memory[:, :N_STATES])\n",
    "        b_a = torch.LongTensor(b_memory[:, N_STATES:N_STATES+1].astype(int))\n",
    "        b_r = torch.FloatTensor(b_memory[:, N_STATES+1:N_STATES+2])\n",
    "        b_s_ = torch.FloatTensor(b_memory[:, -N_STATES:])\n",
    "        \n",
    "        b_s = b_s.to(self.device)\n",
    "        b_a = b_a.to(self.device)\n",
    "        b_r = b_r.to(self.device)\n",
    "        b_s_ = b_s_.to(self.device)\n",
    "\n",
    "        # q_eval w.r.t the action in experience\n",
    "        q_eval = self.eval_net(b_s).gather(1, b_a)  # shape (batch, 1)\n",
    "        q_next = self.target_net(b_s_).detach()     # detach from graph, don't backpropagate\n",
    "        q_target = b_r + GAMMA * q_next.max(1)[0].view(BATCH_SIZE, 1)   # shape (batch, 1)\n",
    "        loss = self.loss_func(q_eval, q_target)\n",
    "\n",
    "        self.optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        self.optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def stdize(s):\n",
    "    MU_BATT = 0.5\n",
    "    SD_BATT = 0.15\n",
    "    \n",
    "    MU_ENP = 0\n",
    "    SD_ENP = 0.15\n",
    "    \n",
    "    MU_HENERGY = 0.35\n",
    "    SD_HENERGY = 0.25\n",
    "    \n",
    "    MU_FCAST = 0.42\n",
    "    SD_FCAST = 0.27\n",
    "    \n",
    "    norm_batt, norm_enp, norm_henergy, norm_fcast = s\n",
    "    \n",
    "    std_batt = (norm_batt - MU_BATT)/SD_BATT\n",
    "    std_enp = (norm_enp - MU_ENP)/SD_ENP\n",
    "    std_henergy = (norm_henergy - MU_HENERGY)/SD_HENERGY\n",
    "    std_fcast = (norm_fcast - MU_FCAST)/SD_FCAST\n",
    "\n",
    "\n",
    "    return [std_batt, std_enp, std_henergy, std_fcast]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "TRAINING IN PROGRESS\n",
      "\n",
      "Device:  cpu\n",
      "\n",
      "Iteration 0:  TOKYO, 2003 \n",
      "Average Reward \t\t= -2.980\n",
      "Violation Counter \t= 264\n",
      "\n",
      "Iteration 1:  TOKYO, 2007 \n",
      "Average Reward \t\t= -2.887\n",
      "Violation Counter \t= 253\n",
      "\n",
      "Iteration 2:  TOKYO, 2001 \n",
      "Average Reward \t\t= -3.081\n",
      "Violation Counter \t= 214\n",
      "\n",
      "Iteration 3:  TOKYO, 2002 \n",
      "Average Reward \t\t= -3.912\n",
      "Violation Counter \t= 275\n",
      "\n",
      "Iteration 4:  TOKYO, 2000 \n",
      "Average Reward \t\t= -2.887\n",
      "Violation Counter \t= 207\n",
      "\n",
      "Iteration 5:  TOKYO, 2004 \n",
      "Average Reward \t\t= -1.990\n",
      "Violation Counter \t= 151\n",
      "\n",
      "Iteration 6:  TOKYO, 2003 \n",
      "Average Reward \t\t= -1.915\n",
      "Violation Counter \t= 137\n",
      "\n",
      "Iteration 7:  TOKYO, 2006 \n",
      "Average Reward \t\t= -1.951\n",
      "Violation Counter \t= 131\n",
      "\n",
      "Iteration 8:  TOKYO, 2003 \n",
      "Average Reward \t\t= -0.864\n",
      "Violation Counter \t= 72\n",
      "\n",
      "Iteration 9:  TOKYO, 2006 \n",
      "Average Reward \t\t= -0.873\n",
      "Violation Counter \t= 80\n",
      "\n",
      "Iteration 10:  TOKYO, 2008 \n",
      "Average Reward \t\t= 0.271\n",
      "Violation Counter \t= 33\n",
      "\n",
      "Iteration 11:  TOKYO, 2007 \n",
      "Average Reward \t\t= 0.319\n",
      "Violation Counter \t= 29\n",
      "\n",
      "Iteration 12:  TOKYO, 2000 \n",
      "Average Reward \t\t= 0.103\n",
      "Violation Counter \t= 39\n",
      "\n",
      "Iteration 13:  TOKYO, 2002 \n",
      "Average Reward \t\t= 0.088\n",
      "Violation Counter \t= 43\n",
      "\n",
      "Iteration 14:  TOKYO, 2002 \n",
      "Average Reward \t\t= 0.086\n",
      "Violation Counter \t= 46\n",
      "\n",
      "Iteration 15:  TOKYO, 2001 \n",
      "Average Reward \t\t= 0.604\n",
      "Violation Counter \t= 23\n",
      "\n",
      "Iteration 16:  TOKYO, 2003 \n",
      "Average Reward \t\t= 0.505\n",
      "Violation Counter \t= 29\n",
      "\n",
      "Iteration 17:  TOKYO, 2000 \n",
      "Average Reward \t\t= 0.476\n",
      "Violation Counter \t= 29\n",
      "\n",
      "Iteration 18:  TOKYO, 2003 \n",
      "Average Reward \t\t= 0.488\n",
      "Violation Counter \t= 29\n",
      "\n",
      "Iteration 19:  TOKYO, 2009 \n",
      "Average Reward \t\t= 0.455\n",
      "Violation Counter \t= 38\n",
      "\n",
      "Iteration 20:  TOKYO, 2004 \n",
      "Average Reward \t\t= 0.758\n",
      "Violation Counter \t= 27\n",
      "\n",
      "Iteration 21:  TOKYO, 2000 \n",
      "Average Reward \t\t= 0.591\n",
      "Violation Counter \t= 30\n",
      "\n",
      "Iteration 22:  TOKYO, 2003 \n",
      "Average Reward \t\t= 0.650\n",
      "Violation Counter \t= 27\n",
      "\n",
      "Iteration 23:  TOKYO, 2002 \n",
      "Average Reward \t\t= 0.579\n",
      "Violation Counter \t= 30\n",
      "\n",
      "Iteration 24:  TOKYO, 2005 \n",
      "Average Reward \t\t= 1.084\n",
      "Violation Counter \t= 17\n",
      "\n",
      "Iteration 25:  TOKYO, 2005 \n",
      "Average Reward \t\t= 1.119\n",
      "Violation Counter \t= 16\n",
      "\n",
      "Iteration 26:  TOKYO, 2005 \n",
      "Average Reward \t\t= 1.164\n",
      "Violation Counter \t= 13\n",
      "\n",
      "Iteration 27:  TOKYO, 2008 \n",
      "Average Reward \t\t= 0.979\n",
      "Violation Counter \t= 17\n",
      "\n",
      "Iteration 28:  TOKYO, 2009 \n",
      "Average Reward \t\t= 0.917\n",
      "Violation Counter \t= 17\n",
      "\n",
      "Iteration 29:  TOKYO, 2003 \n",
      "Average Reward \t\t= 1.006\n",
      "Violation Counter \t= 15\n",
      "\n",
      "Iteration 30:  TOKYO, 2009 \n",
      "Average Reward \t\t= 0.853\n",
      "Violation Counter \t= 25\n",
      "\n",
      "Iteration 31:  TOKYO, 2005 \n",
      "Average Reward \t\t= 1.110\n",
      "Violation Counter \t= 12\n",
      "\n",
      "Iteration 32:  TOKYO, 2006 \n",
      "Average Reward \t\t= 0.670\n",
      "Violation Counter \t= 29\n",
      "\n",
      "Iteration 33:  TOKYO, 2008 \n",
      "Average Reward \t\t= 1.118\n",
      "Violation Counter \t= 11\n",
      "\n",
      "Iteration 34:  TOKYO, 2007 \n",
      "Average Reward \t\t= 1.110\n",
      "Violation Counter \t= 7\n",
      "\n",
      "Iteration 35:  TOKYO, 2002 \n",
      "Average Reward \t\t= 0.766\n",
      "Violation Counter \t= 22\n",
      "\n",
      "Iteration 36:  TOKYO, 2007 \n",
      "Average Reward \t\t= 1.090\n",
      "Violation Counter \t= 8\n",
      "\n",
      "Iteration 37:  TOKYO, 2000 \n",
      "Average Reward \t\t= 1.069\n",
      "Violation Counter \t= 10\n",
      "\n",
      "Iteration 38:  TOKYO, 2007 \n",
      "Average Reward \t\t= 0.742\n",
      "Violation Counter \t= 25\n",
      "\n",
      "Iteration 39:  TOKYO, 2009 \n",
      "Average Reward \t\t= 1.026\n",
      "Violation Counter \t= 14\n",
      "\n",
      "Iteration 40:  TOKYO, 2005 \n",
      "Average Reward \t\t= 1.274\n",
      "Violation Counter \t= 8\n",
      "\n",
      "Iteration 41:  TOKYO, 2008 \n",
      "Average Reward \t\t= 1.143\n",
      "Violation Counter \t= 9\n",
      "\n",
      "Iteration 42:  TOKYO, 2003 \n",
      "Average Reward \t\t= 1.077\n",
      "Violation Counter \t= 10\n",
      "\n",
      "Iteration 43:  TOKYO, 2005 \n",
      "Average Reward \t\t= 1.192\n",
      "Violation Counter \t= 13\n",
      "\n",
      "Iteration 44:  TOKYO, 2008 \n",
      "Average Reward \t\t= 1.047\n",
      "Violation Counter \t= 14\n",
      "\n",
      "Iteration 45:  TOKYO, 2000 \n",
      "Average Reward \t\t= 1.074\n",
      "Violation Counter \t= 8\n",
      "\n",
      "Iteration 46:  TOKYO, 2001 \n",
      "Average Reward \t\t= 1.175\n",
      "Violation Counter \t= 10\n",
      "\n",
      "Iteration 47:  TOKYO, 2007 \n",
      "Average Reward \t\t= 1.075\n",
      "Violation Counter \t= 10\n",
      "\n",
      "Iteration 48:  TOKYO, 2006 \n",
      "Average Reward \t\t= 0.845\n",
      "Violation Counter \t= 17\n",
      "\n",
      "Iteration 49:  TOKYO, 2009 \n",
      "Average Reward \t\t= 0.994\n",
      "Violation Counter \t= 14\n"
     ]
    }
   ],
   "source": [
    "#TRAIN \n",
    "dqn = DQN()\n",
    "# for recording weights\n",
    "oldfc1 = dqn.eval_net.fc1.weight.data.cpu().numpy().flatten()\n",
    "old2fc1 = oldfc1\n",
    "\n",
    "oldfc2 = dqn.eval_net.fc2.weight.data.cpu().numpy().flatten()\n",
    "old2fc2 = oldfc2\n",
    "\n",
    "# oldfc3 = dqn.eval_net.fc3.weight.data.cpu().numpy().flatten()\n",
    "# old2fc3 = oldfc3\n",
    "\n",
    "oldout = dqn.eval_net.out.weight.data.cpu().numpy().flatten()\n",
    "old2out = oldout\n",
    "########################################\n",
    "\n",
    "best_iteration = -1\n",
    "best_avg_reward = -1000 #initialize best average reward to very low value\n",
    "reset_counter = 0 #count number of times the battery had to be reset\n",
    "change_hr = 0\n",
    "# PFILENAME = ''.join(random.choice(string.ascii_uppercase + string.digits) for _ in range(8)) #create random filename\n",
    "# BFILENAME = \"best\"+PFILENAME + \".pt\" #this file stores the best model\n",
    "# TFILENAME = \"terminal\"+PFILENAME + \".pt\" #this file stores the last model\n",
    "\n",
    "avg_reward_rec = [] #record the yearly average rewards over the entire duration of training\n",
    "violation_rec = []\n",
    "print('\\nTRAINING IN PROGRESS\\n')\n",
    "print('Device: ', dqn.device)\n",
    "\n",
    "for iteration in range(NO_OF_ITERATIONS):\n",
    "    LOCATION = 'tokyo'#random.choice(['tokyo','wakkanai','minamidaito'])\n",
    "    YEAR = random.choice(np.arange(2000,2010))\n",
    "    capm = CAPM(LOCATION,YEAR,shuffle=False, trainmode=False) #instantiate the CAPM class\n",
    "    capm.eno = ENO(LOCATION,YEAR, shuffle=False, day_balance=False) #instantiate the environment inside the CAPM class\n",
    "    capm.HMAX = capm.eno.SMAX #maximum power output of solar cell is set in CAPM object using the value in ENO object\n",
    "#     clear_output()\n",
    "    print('\\nIteration {}:  {}, {} '.format(iteration, LOCATION.upper(), YEAR))\n",
    "\n",
    "    s, r, day_end, year_end = capm.reset()\n",
    "    yr_record = np.empty(4)\n",
    "\n",
    "    record = np.empty(4) #record for battery, henergy, reward and action\n",
    "    transition_rec = np.zeros((capm.eno.TIME_STEPS, N_STATES * 2 + 2)) #record all the transition in one day\n",
    "\n",
    "    while True:\n",
    "        a = dqn.choose_action(stdize(s))\n",
    "\n",
    "        # present state = [batt, enp, henergy]\n",
    "        record = np.vstack((record, [s[0],s[2],r, a])) # record battery, henergy, reward and action for troubleshooting\n",
    "        yr_record = np.vstack((yr_record, [s[0],s[2],r, a]))\n",
    "\n",
    "        # take action\n",
    "        s_, r, day_end, year_end = capm.step(a)\n",
    "        \n",
    "        temp_transitions = np.hstack((stdize(s), [a, r], stdize(s_)))\n",
    "        transition_rec[capm.eno.hr-1,:] = temp_transitions\n",
    "\n",
    "        if (day_end):\n",
    "            transition_rec[:,5] += r #broadcast reward to all states\n",
    "            decay_factor = [i for i in (LAMBDA**n for n in reversed(range(0, capm.eno.TIME_STEPS)))]\n",
    "            transition_rec[:,5] = transition_rec[:,5] * decay_factor #decay reward proportionately\n",
    "            dqn.store_day_transition(transition_rec)\n",
    "\n",
    "        if dqn.memory_counter > MEMORY_CAPACITY:\n",
    "            dqn.learn()\n",
    "\n",
    "        if dqn.nettoggle:\n",
    "            change_hr = capm.eno.day*24+capm.eno.hr #to mark when the DQN is updated.\n",
    "            dqn.nettoggle = not dqn.nettoggle\n",
    "\n",
    "        if (year_end):\n",
    "            break\n",
    "\n",
    "        # transition to new state\n",
    "        s = s_\n",
    "\n",
    "    record = np.delete(record, 0, 0) #remove the first row which is garbage\n",
    "    reward_rec = record[:,2] #extract reward information from the record array\n",
    "    reward_rec = reward_rec[::24] #only consider terminal rewards\n",
    "    print(\"Average Reward \\t\\t= {:.3f}\".format(np.mean(reward_rec)))\n",
    "    print(\"Violation Counter \\t= {}\".format(capm.violation_counter))\n",
    "\n",
    "#     if(best_avg_reward < np.mean(reward_rec)):\n",
    "#         best_avg_reward = np.mean(reward_rec)\n",
    "    \n",
    "#     if(best_avg_reward > 1.5 or iteration > 20):\n",
    "#         EPSILON = 0.9\n",
    "#         LR = 0.01\n",
    "        \n",
    "#     if (capm.violation_counter < 5):\n",
    "#         reset_flag = False\n",
    "#         EPSILON = 0.95\n",
    "#         LR = 0.001\n",
    "        \n",
    "\n",
    "#     # Check if reward beats the High Score and possible save it    \n",
    "#     if (iteration > 19): #save the best models only after 20 iterations\n",
    "#         print(\"Best Score \\t = {:8.3f} @ Iteration No. {}\".format(best_avg_reward, best_iteration))\n",
    "#         if(best_avg_reward < np.mean(reward_rec)):\n",
    "#             best_iteration = iteration\n",
    "#             best_avg_reward = np.mean(reward_rec)\n",
    "#             print(\"Saving Model\")\n",
    "#             torch.save(dqn.eval_net.state_dict(), BFILENAME)\n",
    "#     else:\n",
    "#         print(\"\\r\")\n",
    "\n",
    "    # Log the average reward in avg_reward_rec\n",
    "    avg_reward_rec = np.append(avg_reward_rec, np.mean(reward_rec))\n",
    "    violation_rec = np.append(violation_rec, capm.violation_counter)\n",
    "\n",
    "    \n",
    "###########################################################################################\n",
    "# #   PLOT battery levels, hourly rewards and the weights\n",
    "#     yr_record = np.delete(yr_record, 0, 0) #remove the first row which is garbage\n",
    "# #     NO_OF_DAYS = capm.eno.NO_OF_DAYS\n",
    "#     hourly_yr_reward_rec = yr_record[:,2]\n",
    "#     yr_reward_rec = hourly_yr_reward_rec[::24]\n",
    "\n",
    "    \n",
    "#     fig = plt.figure(figsize=(24,3))\n",
    "#     TIME_STEPS = capm.eno.TIME_STEPS\n",
    "#     NO_OF_DAYS = capm.eno.NO_OF_DAYS\n",
    "#     DAY_SPACING = 15\n",
    "#     TICK_SPACING = TIME_STEPS*DAY_SPACING\n",
    "#     #plot battery\n",
    "#     ax = fig.add_subplot(111)\n",
    "#     ax.plot(np.arange(0,TIME_STEPS*NO_OF_DAYS),yr_record[:,0],'r')\n",
    "#     ax.set_ylim([0,1])\n",
    "#     ax.axvline(x=change_hr)\n",
    "#     ax.xaxis.set_major_locator(ticker.MultipleLocator(TICK_SPACING))\n",
    "# #     labels = [item for item in ax.get_xticklabels()]\n",
    "# #     print(labels)\n",
    "# #     labels [15:-1] = np.arange(0,NO_OF_DAYS,DAY_SPACING) #the first label is reserved to negative values\n",
    "# #     ax.set_xticklabels(labels)\n",
    "#     #plot hourly reward\n",
    "#     ax0 = ax.twinx()\n",
    "#     ax0.plot(hourly_yr_reward_rec, color='m')\n",
    "#     ax0.set_ylim(-7,3)\n",
    "#     plt.show()\n",
    "\n",
    "\n",
    "#     fig = plt.figure(figsize=(18,3))\n",
    "#     ax1 = fig.add_subplot(131)\n",
    "#     newfc1 = dqn.eval_net.fc1.weight.data.cpu().numpy().flatten()\n",
    "#     ax1.plot(old2fc1,color='b', alpha=0.4)\n",
    "#     ax1.plot(oldfc1,color='b',alpha = 0.7)\n",
    "#     ax1.plot(newfc1,color='b')\n",
    "#     old2fc1 = oldfc1\n",
    "#     oldfc1 = newfc1\n",
    "    \n",
    "#     ax2 = fig.add_subplot(132)\n",
    "#     newfc2 = dqn.eval_net.fc2.weight.data.cpu().numpy().flatten()\n",
    "#     ax2.plot(old2fc2,color='y', alpha=0.4)\n",
    "#     ax2.plot(oldfc2,color='y',alpha = 0.7)\n",
    "#     ax2.plot(newfc2,color='y')\n",
    "#     old2fc2 = oldfc2\n",
    "#     oldfc2 = newfc2\n",
    "    \n",
    "# #     ax3 = fig.add_subplot(143)\n",
    "# #     newfc3 = dqn.eval_net.fc3.weight.data.cpu().numpy().flatten()\n",
    "# #     ax3.plot(old2fc3,color='y', alpha=0.4)\n",
    "# #     ax3.plot(oldfc3,color='y',alpha = 0.7)\n",
    "# #     ax3.plot(newfc3,color='y')\n",
    "# #     old2fc3 = oldfc3\n",
    "# #     oldfc3 = newfc3\n",
    "    \n",
    "#     axO = fig.add_subplot(133)\n",
    "#     newout = dqn.eval_net.out.weight.data.cpu().numpy().flatten()\n",
    "#     axO.plot(old2out,color='g', alpha=0.4)\n",
    "#     axO.plot(oldout,color='g',alpha=0.7)\n",
    "#     axO.plot(newout,color='g')\n",
    "#     old2out = oldout\n",
    "#     oldout = newout\n",
    "    \n",
    "#     fig.tight_layout()\n",
    "#     plt.show()\n",
    "\n",
    "\n",
    "    # End of training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 648x216 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig = plt.figure(figsize=(9,3))\n",
    "# Plot the average reward log\n",
    "\n",
    "ax1 = fig.add_subplot(111)\n",
    "ax1.set_ylabel(\"Reward\")\n",
    "# ax1.set_ylim([-3,3]);\n",
    "ax1.plot(avg_reward_rec,'b')\n",
    "ax1.tick_params(axis='y', colors='b')\n",
    "\n",
    "#Plot the violation record log\n",
    "ax2 = ax1.twinx()\n",
    "ax2.set_ylabel(\"Violations\",color = 'r')\n",
    "ax2.plot(violation_rec,'r')\n",
    "for xpt in np.argwhere(violation_rec<1):\n",
    "    ax2.axvline(x=xpt,color='g')\n",
    "ax2.set_ylim([0,50]);\n",
    "ax2.tick_params(axis='y', colors='r')\n",
    "\n",
    "fig.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device:  cpu\n",
      "runtime: 0:20:08.826763\n"
     ]
    }
   ],
   "source": [
    "print('Device: ', dqn.device)\n",
    "print('runtime: {}'.format(datetime.now() - tic))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPSILON =  0.98\n",
      "LR      =  5e-05\n",
      "\n",
      "LAST PHASE ITERATION #0:  TOKYO, 2007 \n",
      "Average Reward \t\t= 1.331\n",
      "Violation Counter \t= 1\n",
      "***MEASURING PERFORMANCE OF THE MODEL***\n",
      "\tBEST AVERAGE ANNUAL AVERAGE REWARD = -1000.000\n",
      "\tBEST TOTAL VIOLATIONS              = 1000\n",
      "\n",
      "\tAverage Annual Average Reward      = 1.227\n",
      "\tTotal Violations                   = 28.0\n",
      "****************************************\n",
      "\n",
      "LAST PHASE ITERATION #1:  TOKYO, 2008 \n",
      "Average Reward \t\t= 1.369\n",
      "Violation Counter \t= 0\n",
      "***MEASURING PERFORMANCE OF THE MODEL***\n",
      "\tBEST AVERAGE ANNUAL AVERAGE REWARD = 1.227\n",
      "\tBEST TOTAL VIOLATIONS              = 28.0\n",
      "\n",
      "\tAverage Annual Average Reward      = 1.321\n",
      "\tTotal Violations                   = 0.0\n",
      "****************************************\n",
      "\n",
      "LAST PHASE ITERATION #2:  TOKYO, 2001 \n",
      "Average Reward \t\t= 1.406\n",
      "Violation Counter \t= 1\n",
      "***MEASURING PERFORMANCE OF THE MODEL***\n",
      "\tBEST AVERAGE ANNUAL AVERAGE REWARD = 1.321\n",
      "\tBEST TOTAL VIOLATIONS              = 0.0\n",
      "\n",
      "\tAverage Annual Average Reward      = 1.425\n",
      "\tTotal Violations                   = 0.0\n",
      "****************************************\n",
      "\n",
      "LAST PHASE ITERATION #3:  TOKYO, 2001 \n",
      "Average Reward \t\t= 1.379\n",
      "Violation Counter \t= 0\n",
      "***MEASURING PERFORMANCE OF THE MODEL***\n",
      "\tBEST AVERAGE ANNUAL AVERAGE REWARD = 1.425\n",
      "\tBEST TOTAL VIOLATIONS              = 0.0\n",
      "\n",
      "\tAverage Annual Average Reward      = 1.364\n",
      "\tTotal Violations                   = 0.0\n",
      "****************************************\n",
      "\n",
      "LAST PHASE ITERATION #4:  TOKYO, 2002 \n",
      "Average Reward \t\t= 1.008\n",
      "Violation Counter \t= 9\n",
      "***MEASURING PERFORMANCE OF THE MODEL***\n",
      "\tBEST AVERAGE ANNUAL AVERAGE REWARD = 1.425\n",
      "\tBEST TOTAL VIOLATIONS              = 0.0\n",
      "\n",
      "\tAverage Annual Average Reward      = 1.251\n",
      "\tTotal Violations                   = 6.0\n",
      "****************************************\n",
      "\n",
      "LAST PHASE ITERATION #5:  TOKYO, 2004 \n",
      "Average Reward \t\t= 1.180\n",
      "Violation Counter \t= 4\n",
      "***MEASURING PERFORMANCE OF THE MODEL***\n",
      "\tBEST AVERAGE ANNUAL AVERAGE REWARD = 1.425\n",
      "\tBEST TOTAL VIOLATIONS              = 0.0\n",
      "\n",
      "\tAverage Annual Average Reward      = 1.292\n",
      "\tTotal Violations                   = 0.0\n",
      "****************************************\n",
      "\n",
      "LAST PHASE ITERATION #6:  TOKYO, 2007 \n",
      "Average Reward \t\t= 1.278\n",
      "Violation Counter \t= 1\n",
      "***MEASURING PERFORMANCE OF THE MODEL***\n",
      "\tBEST AVERAGE ANNUAL AVERAGE REWARD = 1.425\n",
      "\tBEST TOTAL VIOLATIONS              = 0.0\n",
      "\n",
      "\tAverage Annual Average Reward      = 1.296\n",
      "\tTotal Violations                   = 16.0\n",
      "****************************************\n",
      "\n",
      "LAST PHASE ITERATION #7:  TOKYO, 2008 \n",
      "Average Reward \t\t= 1.295\n",
      "Violation Counter \t= 1\n",
      "***MEASURING PERFORMANCE OF THE MODEL***\n",
      "\tBEST AVERAGE ANNUAL AVERAGE REWARD = 1.425\n",
      "\tBEST TOTAL VIOLATIONS              = 0.0\n",
      "\n",
      "\tAverage Annual Average Reward      = 1.209\n",
      "\tTotal Violations                   = 18.0\n",
      "****************************************\n",
      "\n",
      "LAST PHASE ITERATION #8:  TOKYO, 2006 \n",
      "Average Reward \t\t= 1.187\n",
      "Violation Counter \t= 4\n",
      "***MEASURING PERFORMANCE OF THE MODEL***\n",
      "\tBEST AVERAGE ANNUAL AVERAGE REWARD = 1.425\n",
      "\tBEST TOTAL VIOLATIONS              = 0.0\n",
      "\n",
      "\tAverage Annual Average Reward      = 1.272\n",
      "\tTotal Violations                   = 19.0\n",
      "****************************************\n",
      "\n",
      "LAST PHASE ITERATION #9:  TOKYO, 2006 \n",
      "Average Reward \t\t= 1.193\n",
      "Violation Counter \t= 4\n",
      "***MEASURING PERFORMANCE OF THE MODEL***\n",
      "\tBEST AVERAGE ANNUAL AVERAGE REWARD = 1.425\n",
      "\tBEST TOTAL VIOLATIONS              = 0.0\n",
      "\n",
      "\tAverage Annual Average Reward      = 1.110\n",
      "\tTotal Violations                   = 21.0\n",
      "****************************************\n"
     ]
    }
   ],
   "source": [
    "#END OF TRAINING PHASE - CHOOSING THE BEST MODEL INSTANCE\n",
    "#INCREASE GREEDY RATE\n",
    "#VALIDATE AFTER EVERY ITERATION\n",
    "\n",
    "# Use this model and its output as base standards for the last phase of training\n",
    "best_avg_avg_reward = -1000\n",
    "best_net_avg_reward = dqn.eval_net\n",
    "best_avg_v_counter = 1000\n",
    "best_net_v_counter = dqn.eval_net\n",
    "\n",
    "\n",
    "NO_OF_LAST_PHASE_ITERATIONS = 10\n",
    "EPSILON = 0.98\n",
    "LR = 0.00005\n",
    "\n",
    "print(\"EPSILON = \", EPSILON)\n",
    "print(\"LR      = \", LR)\n",
    "\n",
    "for iteration in range(NO_OF_LAST_PHASE_ITERATIONS):\n",
    "    LOCATION = 'tokyo'#random.choice(['tokyo','wakkanai','minamidaito'])\n",
    "    YEAR = random.choice(np.arange(2000,2010))\n",
    "    capm = CAPM(LOCATION,YEAR,shuffle=False, trainmode=False) #instantiate the CAPM class\n",
    "    capm.eno = ENO(LOCATION,YEAR, shuffle=False, day_balance=False) #instantiate the environment inside the CAPM class\n",
    "    capm.HMAX = capm.eno.SMAX #maximum power output of solar cell is set in CAPM object using the value in ENO object\n",
    "\n",
    "    print('\\nLAST PHASE ITERATION #{}:  {}, {} '.format(iteration, LOCATION.upper(), YEAR))\n",
    "    \n",
    "    \n",
    "    my_avg_reward = -1000\n",
    "    my_v_counter = 1000\n",
    "    \n",
    "    s, r, day_end, year_end = capm.reset()\n",
    "    yr_record = np.empty(4)\n",
    "\n",
    "    record = np.empty(4) #record for battery, henergy, reward and action\n",
    "    transition_rec = np.zeros((capm.eno.TIME_STEPS, N_STATES * 2 + 2)) #record all the transition in one day\n",
    "\n",
    "    while True:\n",
    "        a = dqn.choose_action(stdize(s))\n",
    "\n",
    "        # present state = [batt, enp, henergy]\n",
    "        record = np.vstack((record, [s[0],s[2],r, a])) # record battery, henergy, reward and action for troubleshooting\n",
    "        yr_record = np.vstack((yr_record, [s[0],s[2],r, a]))\n",
    "\n",
    "        # take action\n",
    "        s_, r, day_end, year_end = capm.step(a)\n",
    "        \n",
    "        temp_transitions = np.hstack((stdize(s), [a, r], stdize(s_)))\n",
    "        transition_rec[capm.eno.hr-1,:] = temp_transitions\n",
    "\n",
    "        if (day_end):\n",
    "            transition_rec[:,5] += r #broadcast reward to all states\n",
    "            decay_factor = [i for i in (LAMBDA**n for n in reversed(range(0, capm.eno.TIME_STEPS)))]\n",
    "            transition_rec[:,5] = transition_rec[:,5] * decay_factor #decay reward proportionately\n",
    "            dqn.store_day_transition(transition_rec)\n",
    "\n",
    "        if dqn.memory_counter > MEMORY_CAPACITY:\n",
    "            dqn.learn()\n",
    "\n",
    "        if dqn.nettoggle:\n",
    "            change_hr = capm.eno.day*24+capm.eno.hr #to mark when the DQN is updated.\n",
    "            dqn.nettoggle = not dqn.nettoggle\n",
    "\n",
    "        if (year_end):\n",
    "            break\n",
    "\n",
    "        # transition to new state\n",
    "        s = s_\n",
    "\n",
    "    record = np.delete(record, 0, 0) #remove the first row which is garbage\n",
    "    reward_rec = record[:,2] #extract reward information from the record array\n",
    "    reward_rec = reward_rec[::24] #only consider terminal rewards\n",
    "    print(\"Average Reward \\t\\t= {:.3f}\".format(np.mean(reward_rec)))\n",
    "    print(\"Violation Counter \\t= {}\".format(capm.violation_counter))\n",
    "    \n",
    "    \n",
    "    print(\"***MEASURING PERFORMANCE OF THE MODEL***\")\n",
    "    print(\"\\tBEST AVERAGE ANNUAL AVERAGE REWARD = {:.3f}\".format(best_avg_avg_reward))\n",
    "    print(\"\\tBEST TOTAL VIOLATIONS              = {}\".format(best_avg_v_counter))\n",
    "    LOCATION = 'tokyo'\n",
    "    results = np.empty(3)\n",
    "    for YEAR in np.arange(2010,2015):\n",
    "        capm = CAPM(LOCATION,YEAR,shuffle=False, trainmode=False) #instantiate the CAPM class\n",
    "        capm.eno = ENO(LOCATION,YEAR, shuffle=False, day_balance=False) #instantiate the environment inside the CAPM class\n",
    "        capm.HMAX = capm.eno.SMAX #maximum power output of solar cell is set in CAPM object using the value in ENO object\n",
    "\n",
    "        s, r, day_end, year_end = capm.reset()\n",
    "        yr_test_record = np.empty(4)\n",
    "\n",
    "        while True:\n",
    "            a = dqn.choose_greedy_action(stdize(s))\n",
    "            #state = [batt, enp, henergy, fcast]\n",
    "            yr_test_record = np.vstack((yr_test_record, [s[0],s[2],r, a])) #record battery, henergy, reward and action\n",
    "            # take action\n",
    "            s_, r, day_end, year_end = capm.step(a)\n",
    "            if year_end:\n",
    "                break\n",
    "            s = s_\n",
    "\n",
    "        yr_test_record = np.delete(yr_test_record, 0, 0) #remove the first row which is garbage\n",
    "        yr_test_reward_rec = yr_test_record[:,2]\n",
    "        yr_test_reward_rec = yr_test_reward_rec[::24] #annual average reward\n",
    "        results = np.vstack((results, [int(YEAR), np.mean(yr_test_reward_rec), int(capm.violation_counter)]))\n",
    "    results = np.delete(results,0,0)\n",
    "    my_avg_reward = np.mean(results[:,1]) #the average of annual average rewards\n",
    "    my_v_counter = np.sum(results[:,-1]) #total sum of violations\n",
    "    print(\"\\n\\tAverage Annual Average Reward      = {:.3f}\".format(my_avg_reward))\n",
    "    print(\"\\tTotal Violations                   = {}\".format(my_v_counter))\n",
    "\n",
    "    if (my_avg_reward > best_avg_avg_reward):\n",
    "            best_avg_avg_reward = my_avg_reward\n",
    "            best_net_avg_reward = dqn.eval_net\n",
    "\n",
    "    if (my_v_counter < best_avg_v_counter):\n",
    "        best_avg_v_counter = my_v_counter\n",
    "        best_net_v_counter = dqn.eval_net\n",
    "    elif (my_v_counter == best_avg_v_counter):\n",
    "        if (my_avg_reward > best_avg_avg_reward):\n",
    "            best_avg_v_counter = my_v_counter\n",
    "            best_net_v_counter = dqn.eval_net\n",
    "    print(\"****************************************\")\n",
    "\n",
    "    \n",
    "\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TESTING BEST MODEL BASED ON AVERAGE REWARD METRIC\n",
      "YEAR\t\tAVG_RWD\t\tVIOLATIONS\n",
      "2015 \t\t 1.1 \t\t 3\n",
      "2016 \t\t 1.12 \t\t 4\n",
      "2017 \t\t 1.07 \t\t 10\n",
      "2018 \t\t 1.13 \t\t 2\n"
     ]
    }
   ],
   "source": [
    "#TESTING BEST MODEL BASED ON AVERAGE REWARD METRIC\n",
    "dqn = DQN()\n",
    "dqn.eval_net = best_net_avg_reward\n",
    "LOCATION = 'tokyo'\n",
    "results = np.empty(3)\n",
    "for YEAR in np.arange(2015,2019):\n",
    "    capm = CAPM(LOCATION,YEAR,shuffle=False, trainmode=False) #instantiate the CAPM class\n",
    "    capm.eno = ENO(LOCATION,YEAR, shuffle=False, day_balance=False) #instantiate the environment inside the CAPM class\n",
    "    capm.HMAX = capm.eno.SMAX #maximum power output of solar cell is set in CAPM object using the value in ENO object\n",
    "\n",
    "    s, r, day_end, year_end = capm.reset()\n",
    "    yr_test_record = np.empty(4)\n",
    "\n",
    "    while True:\n",
    "        a = dqn.choose_greedy_action(stdize(s))\n",
    "\n",
    "        #state = [batt, enp, henergy, fcast]\n",
    "        yr_test_record = np.vstack((yr_test_record, [s[0],s[2],r, a])) #record battery, henergy, reward and action\n",
    "\n",
    "        # take action\n",
    "        s_, r, day_end, year_end = capm.step(a)\n",
    "\n",
    "        if year_end:\n",
    "            break\n",
    "\n",
    "        s = s_\n",
    "\n",
    "    yr_test_record = np.delete(yr_test_record, 0, 0) #remove the first row which is garbage\n",
    "\n",
    "#     #Plot the reward and battery for the entire year run\n",
    "#     title = LOCATION.upper() + ',' + str(YEAR)\n",
    "\n",
    "#     NO_OF_DAYS = capm.eno.NO_OF_DAYS\n",
    "    yr_test_reward_rec = yr_test_record[:,2]\n",
    "    yr_test_reward_rec = yr_test_reward_rec[::24]\n",
    "#     print('Average Reward for',title, '=', np.mean(yr_test_reward_rec))\n",
    "#     print('Violations for',title, '=', capm.violation_counter)\n",
    "    \n",
    "    results = np.vstack((results, [int(YEAR), np.mean(yr_test_reward_rec), int(capm.violation_counter)]))\n",
    "\n",
    "#     fig = plt.figure(figsize=(24,3))\n",
    "#     fig.suptitle(title, fontsize=15)\n",
    "\n",
    "#     #     ax1 = fig.add_subplot(211)\n",
    "#     #     ax1.plot(yr_test_reward_rec)\n",
    "#     #     ax1.set_title(\"\\n\\nYear Run Reward\")\n",
    "#     #     ax1.set_ylim([-3,1])\n",
    "\n",
    "#     ax2 = fig.add_subplot(111)\n",
    "#     ax2.plot(yr_test_record[:,0],'r')\n",
    "#     ax2.set_title(\"\\n\\nYear Run Battery\")\n",
    "#     ax2.set_ylim([0,1])\n",
    "#     plt.sca(ax2)\n",
    "#     plt.xticks(np.arange(0, NO_OF_DAYS*24, 50*24),np.arange(0,NO_OF_DAYS,50))\n",
    "\n",
    "#     fig.tight_layout()\n",
    "#     plt.show()\n",
    "\n",
    "results = np.delete(results,0,0)\n",
    "\n",
    "print(\"TESTING BEST MODEL BASED ON AVERAGE REWARD METRIC\")\n",
    "print('YEAR\\t\\tAVG_RWD\\t\\tVIOLATIONS')\n",
    "for x in np.arange(0,results.shape[0]):\n",
    "    print('{} \\t\\t {} \\t\\t {}'.format(int(results[x,0]), np.around(results[x,1],2), int(results[x,-1])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TESTING BASED ON VIOLATION COUNTER METRIC\n",
      "YEAR\t\tAVG_RWD\t\tVIOLATIONS\n",
      "2015 \t\t 1.1 \t\t 3\n",
      "2016 \t\t 1.12 \t\t 4\n",
      "2017 \t\t 1.07 \t\t 10\n",
      "2018 \t\t 1.13 \t\t 2\n"
     ]
    }
   ],
   "source": [
    "#TESTING BASED ON VIOLATION COUNTER METRIC\n",
    "dqn = DQN()\n",
    "dqn.eval_net = best_net_v_counter\n",
    "\n",
    "LOCATION = 'tokyo'\n",
    "results = np.empty(3)\n",
    "for YEAR in np.arange(2015,2019):\n",
    "    capm = CAPM(LOCATION,YEAR,shuffle=False, trainmode=False) #instantiate the CAPM class\n",
    "    capm.eno = ENO(LOCATION,YEAR, shuffle=False, day_balance=False) #instantiate the environment inside the CAPM class\n",
    "    capm.HMAX = capm.eno.SMAX #maximum power output of solar cell is set in CAPM object using the value in ENO object\n",
    "\n",
    "    s, r, day_end, year_end = capm.reset()\n",
    "    yr_test_record = np.empty(4)\n",
    "\n",
    "    while True:\n",
    "        a = dqn.choose_greedy_action(stdize(s))\n",
    "\n",
    "        #state = [batt, enp, henergy, fcast]\n",
    "        yr_test_record = np.vstack((yr_test_record, [s[0],s[2],r, a])) #record battery, henergy, reward and action\n",
    "\n",
    "        # take action\n",
    "        s_, r, day_end, year_end = capm.step(a)\n",
    "\n",
    "        if year_end:\n",
    "            break\n",
    "\n",
    "        s = s_\n",
    "\n",
    "    yr_test_record = np.delete(yr_test_record, 0, 0) #remove the first row which is garbage\n",
    "\n",
    "#     #Plot the reward and battery for the entire year run\n",
    "#     title = LOCATION.upper() + ',' + str(YEAR)\n",
    "\n",
    "#     NO_OF_DAYS = capm.eno.NO_OF_DAYS\n",
    "    yr_test_reward_rec = yr_test_record[:,2]\n",
    "    yr_test_reward_rec = yr_test_reward_rec[::24]\n",
    "#     print('Average Reward for',title, '=', np.mean(yr_test_reward_rec))\n",
    "#     print('Violations for',title, '=', capm.violation_counter)\n",
    "    \n",
    "    results = np.vstack((results, [int(YEAR), np.mean(yr_test_reward_rec), int(capm.violation_counter)]))\n",
    "\n",
    "#     fig = plt.figure(figsize=(24,3))\n",
    "#     fig.suptitle(title, fontsize=15)\n",
    "\n",
    "#     #     ax1 = fig.add_subplot(211)\n",
    "#     #     ax1.plot(yr_test_reward_rec)\n",
    "#     #     ax1.set_title(\"\\n\\nYear Run Reward\")\n",
    "#     #     ax1.set_ylim([-3,1])\n",
    "\n",
    "#     ax2 = fig.add_subplot(111)\n",
    "#     ax2.plot(yr_test_record[:,0],'r')\n",
    "#     ax2.set_title(\"\\n\\nYear Run Battery\")\n",
    "#     ax2.set_ylim([0,1])\n",
    "#     plt.sca(ax2)\n",
    "#     plt.xticks(np.arange(0, NO_OF_DAYS*24, 50*24),np.arange(0,NO_OF_DAYS,50))\n",
    "\n",
    "#     fig.tight_layout()\n",
    "#     plt.show()\n",
    "\n",
    "results = np.delete(results,0,0)\n",
    "\n",
    "print(\"TESTING BASED ON VIOLATION COUNTER METRIC\")\n",
    "print('YEAR\\t\\tAVG_RWD\\t\\tVIOLATIONS')\n",
    "for x in np.arange(0,results.shape[0]):\n",
    "    print('{} \\t\\t {} \\t\\t {}'.format(int(results[x,0]), np.around(results[x,1],2), int(results[x,-1])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #Plot the reward and battery for the entire year run on a day by day basis\n",
    "# title = LOCATION.upper() + ',' + str(YEAR)\n",
    "# TIME_AXIS = np.arange(0,capm.eno.TIME_STEPS)\n",
    "# for DAY in range(0,10):#capm.eno.NO_OF_DAYS):\n",
    "#     START = DAY*24\n",
    "#     END = START+24\n",
    "\n",
    "#     daytitle = title + ' - DAY ' + str(DAY)\n",
    "#     fig = plt.figure(figsize=(16,4))\n",
    "#     st = fig.suptitle(daytitle)\n",
    "\n",
    "#     ax2 = fig.add_subplot(121)\n",
    "#     ax2.plot(yr_test_record[START:END,1],'g')\n",
    "#     ax2.set_title(\"HARVESTED ENERGY\")\n",
    "#     plt.xlabel(\"Hour\")\n",
    "#     ax2.set_ylim([0,1])\n",
    "\n",
    "#     #plot battery for year run\n",
    "#     ax1 = fig.add_subplot(122)\n",
    "#     ax1.plot(TIME_AXIS,yr_test_record[START:END,0],'r') \n",
    "# #     ax1.plot(TIME_AXIS, np.ones(capm.eno.TIME_STEPS)*capm.BOPT/capm.BMAX,'r--')\n",
    "#     ax1.plot(TIME_AXIS, np.ones(capm.eno.TIME_STEPS)*capm.BOPT/capm.BMAX,'r--')\n",
    "#     ax1.text(0.1, 0.2, \"BINIT = %.2f\\n\" %(yr_test_record[START,0]),fontsize=11, ha='left')\n",
    "#     ax1.text(0.1, 0.4, \"TENP = %.2f\\n\" %(capm.BOPT/capm.BMAX-yr_test_record[END,0]),fontsize=11, ha='left')\n",
    "#     ax1.text(0.1, 0.3, \"BMEAN = %.2f\\n\" %(np.mean(yr_test_record[START:END,0])),fontsize=11, ha='left')\n",
    "\n",
    "\n",
    "\n",
    "#     ax1.set_title(\"YEAR RUN TEST\")\n",
    "#     if END < (capm.eno.NO_OF_DAYS*capm.eno.TIME_STEPS):\n",
    "#         ax1.text(0.1, 0, \"REWARD = %.2f\\n\" %(yr_test_record[END,2]),fontsize=13, ha='left')\n",
    "#     plt.xlabel(\"Hour\")\n",
    "#     ax1.set_ylabel('Battery', color='r',fontsize=12)\n",
    "#     ax1.set_ylim([0,1])\n",
    "\n",
    "#     #plot actions for year run\n",
    "#     ax1a = ax1.twinx()\n",
    "#     ax1a.plot(yr_test_record[START:END,3])\n",
    "#     ax1a.set_ylim([0,N_ACTIONS])\n",
    "#     ax1a.set_ylabel('Duty Cycle', color='b',fontsize=12)\n",
    "\n",
    "#     fig.tight_layout()\n",
    "#     st.set_y(0.95)\n",
    "#     fig.subplots_adjust(top=0.75)\n",
    "#     plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
