{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "from datetime import datetime\n",
    "tic = datetime.now()\n",
    "\n",
    "import os\n",
    "from os.path import dirname, abspath, join\n",
    "from os import getcwd\n",
    "import sys\n",
    "\n",
    "# THIS_DIR = getcwd()\n",
    "# CLASS_DIR = abspath(join(THIS_DIR, 'dsnclasses'))  #abspath(join(THIS_DIR, '../../..', 'dsnclasses'))\n",
    "# sys.path.append(CLASS_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import string\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.ticker as ticker\n",
    "import torch\n",
    "from IPython.display import clear_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 161\n",
    "random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "torch.cuda.manual_seed(seed)\n",
    "np.random.seed(seed)\n",
    "os.environ['PYTHONHASHSEED'] = str(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ENO(object):\n",
    "    \n",
    "    #no. of forecast types is 6 ranging from 0 to 5\n",
    "  \n",
    "    def __init__(self, location='tokyo', year=2010, shuffle=False, day_balance=False):\n",
    "        self.location = location\n",
    "        self.year = year\n",
    "        self.day = None\n",
    "        self.hr = None\n",
    "        \n",
    "        self.shuffle = shuffle\n",
    "        self.day_balance = day_balance\n",
    "\n",
    "        self.TIME_STEPS = None #no. of time steps in one episode\n",
    "        self.NO_OF_DAYS = None #no. of days in one year\n",
    "        \n",
    "        self.NO_OF_DAYTYPE = 10 #no. of daytypes\n",
    "        self.daycounter = 0 #to count number of days that have been passed\n",
    "        \n",
    "        self.sradiation = None #matrix with GSR for the entire year\n",
    "        self.senergy = None #matrix with harvested energy data for the entire year\n",
    "        self.fforecast = None #array with forecast values for each day\n",
    "        \n",
    "\n",
    "        self.henergy = None #harvested energy variable\n",
    "        self.fcast = None #forecast variable\n",
    "        self.sorted_days = [] #days sorted according to day type\n",
    "        \n",
    "        self.SMAX = 1000 # 1 Watt Solar Panel\n",
    "\n",
    "    \n",
    "    #function to get the solar data for the given location and year and prep it\n",
    "    def get_data(self):\n",
    "        #solar_data/CSV files contain the values of GSR (Global Solar Radiation in MegaJoules per meters squared per hour)\n",
    "        #weather_data/CSV files contain the weather summary from 06:00 to 18:00 and 18:00 to 06:00+1\n",
    "        location = self.location\n",
    "        year = self.year\n",
    "\n",
    "        THIS_DIR = getcwd()\n",
    "        SDATA_DIR = abspath(join(THIS_DIR, 'solar_data'))  #abspath(join(THIS_DIR, '../../..', 'data'))\n",
    "        \n",
    "        sfile = SDATA_DIR + '/' + location +'/' + str(year) + '.csv'\n",
    "        \n",
    "        #skiprows=4 to remove unnecessary title texts\n",
    "        #usecols=4 to read only the Global Solar Radiation (GSR) values\n",
    "        solar_radiation = pd.read_csv(sfile, skiprows=4, encoding='shift_jisx0213', usecols=[4])\n",
    "      \n",
    "        #convert dataframe to numpy array\n",
    "        solar_radiation = solar_radiation.values\n",
    "\n",
    "        #convert missing data in CSV files to zero\n",
    "        solar_radiation[np.isnan(solar_radiation)] = 0\n",
    "\n",
    "        #reshape solar_radiation into no_of_daysx24 array\n",
    "        solar_radiation = solar_radiation.reshape(-1,24)\n",
    "\n",
    "        if(self.shuffle): #if class instatiation calls for shuffling the day order. Required when learning\n",
    "            np.random.shuffle(solar_radiation) \n",
    "        self.sradiation = solar_radiation\n",
    "        \n",
    "        #GSR values (in MJ/sq.mts per hour) need to be expressed in mW\n",
    "        # Conversion is accomplished by \n",
    "        # solar_energy = GSR(in MJ/m2/hr) * 1e6 * size of solar cell * efficiency of solar cell /(60x60) *1000 (to express in mW)\n",
    "        # the factor of 2 in the end is assuming two solar cells\n",
    "        self.senergy = 2*self.sradiation * 1e6 * (55e-3 * 70e-3) * 0.15 * 1000/(60*60)\n",
    "\n",
    "        return 0\n",
    "    \n",
    "    #function to map total day radiation into type of day ranging from 0 to 5\n",
    "    #the classification into day types is quite arbitrary. There is no solid logic behind this type of classification.\n",
    "    \n",
    "    def get_day_state(self,tot_day_radiation):\n",
    "        bin_edges = np.array([0, 3.5, 6.5, 9.0, 12.5, 15.5, 18.5, 22.0, 25, 28])\n",
    "        for k in np.arange(1,bin_edges.size):\n",
    "            if (bin_edges[k-1] < tot_day_radiation <= bin_edges[k]):\n",
    "                day_state = k -1\n",
    "            else:\n",
    "                day_state = bin_edges.size - 1\n",
    "        return int(day_state)\n",
    "    \n",
    "    def get_forecast(self):\n",
    "        #create a perfect forecaster.\n",
    "        tot_day_radiation = np.sum(self.sradiation, axis=1) #contains total solar radiation for each day\n",
    "        get_day_state = np.vectorize(self.get_day_state)\n",
    "        self.fforecast = get_day_state(tot_day_radiation)\n",
    "        \n",
    "        #sort days depending on the type of day and shuffle them; maybe required when learning\n",
    "        for fcast in range(0,6):\n",
    "            fcast_days = ([i for i,x in enumerate(self.fforecast) if x == fcast])\n",
    "            np.random.shuffle(fcast_days)\n",
    "            self.sorted_days.append(fcast_days)\n",
    "        return 0\n",
    "    \n",
    "    def reset(self,day=0): #it is possible to reset to the beginning of a certain day\n",
    "        \n",
    "        self.get_data() #first get data for the given year\n",
    "        self.get_forecast() #calculate the forecast\n",
    "        \n",
    "        self.TIME_STEPS = self.senergy.shape[1]\n",
    "        self.NO_OF_DAYS = self.senergy.shape[0]\n",
    "        \n",
    "        self.day = day\n",
    "        self.hr = 0\n",
    "        \n",
    "        self.henergy = self.senergy[self.day][self.hr]\n",
    "        self.fcast = self.fforecast[self.day]\n",
    "        \n",
    "        end_of_day = False\n",
    "        end_of_year = False\n",
    "        return [self.henergy, self.fcast, end_of_day, end_of_year]\n",
    "\n",
    "    \n",
    "    def step(self):\n",
    "        end_of_day = False\n",
    "        end_of_year = False\n",
    "        if not(self.day_balance): #if daytype balance is not required\n",
    "            if(self.hr < self.TIME_STEPS - 1):\n",
    "                self.hr += 1\n",
    "                self.henergy = self.senergy[self.day][self.hr] \n",
    "                self.fcast = self.fforecast[self.day]\n",
    "            else:\n",
    "                if(self.day < self.NO_OF_DAYS -1):\n",
    "                    end_of_day = True\n",
    "                    self.hr = 0\n",
    "                    self.day += 1\n",
    "                    self.henergy = self.senergy[self.day][self.hr] \n",
    "                    self.fcast = self.fforecast[self.day]\n",
    "                else:\n",
    "                    end_of_day = True\n",
    "                    end_of_year = True\n",
    "                    \n",
    "        else: #when training, we want all daytypes to be equally represented for robust policy\n",
    "              #obviously, the days are going to be in random order\n",
    "            if(self.hr < self.TIME_STEPS - 1):\n",
    "                self.hr += 1\n",
    "                self.henergy = self.senergy[self.day][self.hr]\n",
    "                self.fcast = self.fforecast[self.day]\n",
    "            else:\n",
    "                if(self.daycounter < self.NO_OF_DAYS -1):\n",
    "                    end_of_day = True\n",
    "                    self.daycounter += 1\n",
    "                    self.hr = 0\n",
    "                    daytype = random.choice(np.arange(0,self.NO_OF_DAYTYPE)) #choose random daytype\n",
    "                    self.day = np.random.choice(self.sorted_days[daytype]) #choose random day from that daytype\n",
    "                    self.henergy = self.senergy[self.day][self.hr] \n",
    "                    self.fcast = self.fforecast[self.day]\n",
    "                else: \n",
    "                    end_of_day = True\n",
    "                    end_of_year = True\n",
    "                    self.daycounter = 0\n",
    "        \n",
    "        \n",
    "        return [self.henergy, self.fcast, end_of_day, end_of_year]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CAPM (object):\n",
    "    def __init__(self,location='tokyo', year=2010, shuffle=False, trainmode=False):\n",
    "\n",
    "        #all energy values i.e. BMIN, BMAX, BOPT, HMAX are in mWhr. Assuming one timestep is one hour\n",
    "        \n",
    "        self.BMIN = 0.0                #Minimum battery level that is tolerated. Maybe non-zero also\n",
    "        self.BMAX = 9250.0            #Max Battery Level. May not necessarily be equal to total batter capacity [3.6V x 2500mAh]\n",
    "        self.BOPT = 0.5 * self.BMAX    #Optimal Battery Level. Assuming 50% of battery is the optimum\n",
    "        \n",
    "        self.HMIN = 0      #Minimum energy that can be harvested by the solar panel.\n",
    "        self.HMAX = None   #Maximum energy that can be harvested by the solar panel. [500mW]\n",
    "        \n",
    "        self.DMAX = 500      #Maximum energy that can be consumed by the node in one time step. [~ 3.6V x 135mA]\n",
    "        self.N_ACTIONS = 10  #No. of different duty cycles possible\n",
    "        self.DMIN = self.DMAX/self.N_ACTIONS #Minimum energy that can be consumed by the node in one time step. [~ 3.6V x 15mA]\n",
    "        \n",
    "        self.binit = None     #battery at the beginning of day\n",
    "        self.btrack = []      #track the mean battery level for each day\n",
    "        self.atrack = []      #track the duty cycles for each day\n",
    "        self.batt = None      #battery variable\n",
    "        self.enp = None       #enp at end of hr\n",
    "        self.henergy = None   #harvested energy variable\n",
    "        self.fcast = None     #forecast variable\n",
    "        \n",
    "        self.MUBATT = 0.6\n",
    "        self.SDBATT = 0.02\n",
    "        \n",
    "        self.MUHENERGY = 0.5\n",
    "        self.SDHENERGY = 0.2\n",
    "        \n",
    "        self.MUENP = 0\n",
    "        self.SDENP = 0.02\n",
    "        \n",
    "        self.location = location\n",
    "        self.year = year\n",
    "        self.shuffle = shuffle\n",
    "        self.trainmode = trainmode\n",
    "        self.eno = None#ENO(self.location, self.year, shuffle=shuffle, day_balance=trainmode) #if trainmode is enable, then days are automatically balanced according to daytype i.e. day_balance= True\n",
    "        \n",
    "        self.day_violation_flag = False\n",
    "        self.violation_flag = False\n",
    "        self.violation_counter = 0\n",
    "\n",
    "        self.NO_OF_DAYTYPE = 10 #no. of daytypes\n",
    " \n",
    "    def reset(self,day=0,batt=-1):\n",
    "        henergy, fcast, day_end, year_end = self.eno.reset(day) #reset the eno environment\n",
    "        self.violation_flag = False\n",
    "        self.violation_counter = 0\n",
    "        if(batt == -1):\n",
    "            self.batt = self.BOPT\n",
    "        else:\n",
    "            self.batt = batt\n",
    "            \n",
    "        self.batt = np.clip(self.batt, self.BMIN, self.BMAX)\n",
    "        self.binit = self.batt\n",
    "        self.btrack = np.append(self.btrack, self.batt) #track battery levels\n",
    "\n",
    "#         self.enp = self.BOPT - self.batt\n",
    "        self.enp = self.binit - self.batt #enp is calculated\n",
    "        self.henergy = np.clip(henergy, self.HMIN, self.HMAX) #clip henergy within HMIN and HMAX\n",
    "        self.fcast = fcast\n",
    "        \n",
    "        norm_batt = self.batt/self.BMAX\n",
    "        norm_enp = self.enp/(self.BMAX/2)\n",
    "        norm_henergy = self.henergy/self.HMAX\n",
    "        norm_fcast = self.fcast/(self.NO_OF_DAYTYPE-1)\n",
    "\n",
    "        c_state = [norm_batt, norm_enp, norm_henergy, norm_fcast] #continuous states\n",
    "        reward = 0\n",
    "        \n",
    "        return [c_state, reward, day_end, year_end]\n",
    "    \n",
    "    def getstate(self): #query the present state of the system\n",
    "        norm_batt = self.batt/self.BMAX - self.MUBATT\n",
    "        norm_enp = self.enp/(self.BMAX/2)\n",
    "        norm_henergy = self.henergy/self.HMAX\n",
    "        norm_fcast = self.fcast/(self.NO_OF_DAYTYPE-1)        \n",
    "        c_state = [norm_batt, norm_enp, norm_henergy] #continuous states\n",
    "\n",
    "        return c_state\n",
    "    \n",
    "#     def rewardfn(self):\n",
    "#         R_PARAM = 20000 #chosen empirically for best results\n",
    "#         mu = 0\n",
    "#         sig = 0.07*R_PARAM #knee curve starts at approx. 2000mWhr of deviation\n",
    "#         norm_reward = 3*(np.exp(-np.power((self.enp - mu)/sig, 2.)/2) / np.exp(-np.power((0 - mu)/sig, 2.)/2))-1\n",
    "\n",
    "        \n",
    "# #         if(np.abs(self.enp) <= 0.12*R_PARAM):\n",
    "# #             norm_reward = 2*(np.exp(-np.power((self.enp - mu)/sig, 2.)/2) / np.exp(-np.power((0 - mu)/sig, 2.)/2))\n",
    "# #         else:\n",
    "# #             norm_reward = -0.25 - 10*np.abs(self.enp/R_PARAM)\n",
    "#         if(self.day_violation_flag):\n",
    "#             norm_reward -= 3\n",
    "            \n",
    "#         return (norm_reward)\n",
    "        \n",
    "    \n",
    "    #reward function\n",
    "    def rewardfn(self):\n",
    "        \n",
    "        #FIRST REWARD AS A FUNCTION OF DRIFT OF BMEAN FROM BOPT i.e. in terms of BDEV = |BMEAN-BOPT|/BMAX\n",
    "        bmean = np.mean(self.btrack)\n",
    "        bdev = np.abs(self.BOPT - bmean)/self.BMAX\n",
    "        # based on the sigmoid function\n",
    "        # bdev ranges from bdev = (0,0.5) of BMAX\n",
    "        p1_sharpness = 10\n",
    "        n1_sharpness = 20\n",
    "        shift1 = 0.5\n",
    "        # r1(x) = 0.5 when x = 0.25. \n",
    "        # Therefore, shift = 0.5 to make sure that (2*x-shift) evaluates to zero at x = 0.25\n",
    "\n",
    "        if(bdev<=0.25): \n",
    "            r1 = 2*(1-(1 / (1 + np.exp(-p1_sharpness*(2*bdev-shift1)))))-1\n",
    "        else: \n",
    "            r1 = 2*(1-(1 / (1 + np.exp(-n1_sharpness*(2*bdev-shift1)))))-1\n",
    "        # r1 ranges from -1 to 1\n",
    "            \n",
    "        #SECOND REWARD AS A FUNCTION OF ENP AS LONG AS BMAX/4 <= batt <= 3*BMAX/4 i.e. bdev <= 0.25\n",
    "        if(bdev <=0.25):\n",
    "            # enp ranges from enp = (0,3) of DMAX\n",
    "            p2_sharpness = 2\n",
    "            n2_sharpness = 2\n",
    "            shift2 = 6    \n",
    "            # r1(x) = 0.5 when x = 2. \n",
    "            # Therefore, shift = 6 to make sure that (3*x-shift) evaluates to zero at x = 2\n",
    "#             print('Day energy', np.sum(self.eno.senergy[self.eno.day]))\n",
    "#             print('Node energy', np.sum(self.atrack)*self.DMAX/self.N_ACTIONS)\n",
    "#             x = np.abs(np.sum(self.eno.senergy[self.eno.day])-np.sum(self.atrack)*self.DMAX/self.N_ACTIONS )/self.DMAX\n",
    "            x = np.abs(self.enp/self.DMAX)\n",
    "            if(x<=2): \n",
    "                r2 = (1 / (1 + np.exp(p2_sharpness*(3*x-shift2))))\n",
    "            else: \n",
    "                r2 = (1 / (1 + np.exp(n2_sharpness*(3*x-shift2))))\n",
    "        else:\n",
    "            r2 = 0 # if mean battery lies outside bdev limits, then enp reward is not considered.\n",
    "        # r2 ranges from 0 to 1\n",
    "\n",
    "        #REWARD AS A FUNCTION OF BATTERY VIOLATIONS\n",
    "        if(self.day_violation_flag):\n",
    "            violation_penalty = 3\n",
    "        else:\n",
    "            violation_penalty = 0 #penalty for violating battery limits anytime during the day\n",
    "        \n",
    "#         print(\"Reward \", (r1 + r2 - violation_penalty), '\\n')\n",
    "        return (r1*(2**r2) - violation_penalty)\n",
    "    \n",
    "    def step(self, action):\n",
    "        day_end = False\n",
    "        year_end = False\n",
    "        self.violation_flag = False\n",
    "        reward = 0\n",
    "       \n",
    "        action = np.clip(action, 0, self.N_ACTIONS-1) #action values range from (0 to N_ACTIONS-1)\n",
    "        self.atrack = np.append(self.atrack, action+1) #track duty cycles\n",
    "        e_consumed = (action+1)*self.DMAX/self.N_ACTIONS   #energy consumed by the node\n",
    "        \n",
    "        self.batt += (self.henergy - e_consumed)\n",
    "        if(self.batt < 0.02*self.BMAX or self.batt > 0.98*self.BMAX ):\n",
    "            self.violation_flag = True #penalty for violating battery limits everytime it happens\n",
    "            reward = -2\n",
    "        if(self.batt < 0.02*self.BMAX):\n",
    "            reward -= 2\n",
    "            \n",
    "        if(self.violation_flag):\n",
    "            if(self.day_violation_flag == False): #penalty for violating battery limits anytime during the day - triggers once everyday\n",
    "                self.violation_counter += 1\n",
    "                self.day_violation_flag = True\n",
    "            \n",
    "        self.batt = np.clip(self.batt, self.BMIN, self.BMAX) #clip battery values within permitted level\n",
    "        self.btrack = np.append(self.btrack, self.batt) #track battery levels\n",
    "\n",
    "#         self.enp = self.BOPT - self.batt \n",
    "        self.enp = self.binit - self.atrack.sum()*self.DMAX/self.N_ACTIONS\n",
    "        \n",
    "        #proceed to the next time step\n",
    "        self.henergy, self.fcast, day_end, year_end = self.eno.step()\n",
    "        self.henergy = np.clip(self.henergy, self.HMIN, self.HMAX) #clip henergy within HMIN and HMAX\n",
    "                \n",
    "        if(day_end): #if eno object flags that the day has ended then give reward\n",
    "            reward += self.rewardfn()\n",
    "             \n",
    "            if (self.trainmode): #reset battery to optimal level if limits are exceeded when training\n",
    "#                 self.batt = np.random.uniform(self.DMAX*self.eno.TIME_STEPS/self.BMAX,0.8)*self.BMAX\n",
    "#                 if (self.violation_flag):\n",
    "                if np.random.uniform() < HELP : #occasionaly reset the battery\n",
    "                    self.batt = self.BOPT  \n",
    "            \n",
    "            self.day_violation_flag = False\n",
    "            self.binit = self.batt #this will be the new initial battery level for next day\n",
    "            self.btrack = [] #clear battery tracker\n",
    "            self.atrack = [] #clear duty cycle tracker\n",
    "            \n",
    "                    \n",
    "                \n",
    "        norm_batt = self.batt/self.BMAX\n",
    "        norm_enp = self.enp/(self.BMAX/2)\n",
    "        norm_henergy = self.henergy/self.HMAX\n",
    "        norm_fcast = self.fcast/(self.NO_OF_DAYTYPE-1)\n",
    "\n",
    "        c_state = [norm_batt, norm_enp, norm_henergy, norm_fcast] #continuous states\n",
    "        return [c_state, reward, day_end, year_end]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyper Parameters\n",
    "BATCH_SIZE = 32\n",
    "LR = 0.0001          # learning rate\n",
    "EPSILON = 0.9               # greedy policy\n",
    "GAMMA = 0.9                 # reward discount\n",
    "LAMBDA = 0.9                # parameter decay\n",
    "TARGET_REPLACE_ITER = 24*7*4*18    # target update frequency (every two months)\n",
    "MEMORY_CAPACITY     = 24*7*4*12*2      # store upto six month worth of memory   \n",
    "\n",
    "N_ACTIONS = 10 #no. of duty cycles (0,1,2,3,4)\n",
    "N_STATES = 4 #number of state space parameter [batt, enp, henergy, fcast]\n",
    "\n",
    "HIDDEN_LAYER = 20\n",
    "NO_OF_ITERATIONS = 50\n",
    "GPU = False\n",
    "HELP = 0.05"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "\n",
    "#Class definitions for NN model and learning algorithm\n",
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "\n",
    "        self.fc1 = nn.Linear(N_STATES, HIDDEN_LAYER)\n",
    "        nn.init.kaiming_uniform_(self.fc1.weight)\n",
    "        \n",
    "        self.fc2 = nn.Linear(HIDDEN_LAYER, HIDDEN_LAYER)\n",
    "        nn.init.kaiming_uniform_(self.fc2.weight)\n",
    "\n",
    "        self.out = nn.Linear(HIDDEN_LAYER, N_ACTIONS)\n",
    "        nn.init.xavier_uniform_(self.out.weight) \n",
    "\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.fc1(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.fc2(x)\n",
    "        x = F.relu(x)\n",
    "        actions_value = self.out(x)\n",
    "        return actions_value\n",
    "    \n",
    "class DQN(object):\n",
    "    def __init__(self):\n",
    "        if(GPU): \n",
    "            device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "        else:\n",
    "            device = torch.device('cpu')\n",
    "        self.eval_net, self.target_net = Net(), Net()\n",
    "        self.eval_net.to(device)\n",
    "        self.target_net.to(device)\n",
    "        self.device = device\n",
    "#         print(\"Neural net\")\n",
    "#         print(self.eval_net)\n",
    "        self.learn_step_counter = 0                                     # for target updating\n",
    "        self.memory_counter = 0                                         # for storing memory\n",
    "        self.memory = np.zeros((MEMORY_CAPACITY, N_STATES * 2 + 2))     # initialize memory [mem: ([s], a, r, [s_]) ]\n",
    "        self.optimizer = torch.optim.Adam(self.eval_net.parameters(), lr=LR, weight_decay=1e-3)\n",
    "        self.loss_func = nn.MSELoss()\n",
    "        self.nettoggle = False\n",
    "\n",
    "    def choose_action(self, x):\n",
    "        x = torch.unsqueeze(torch.FloatTensor(x), 0)\n",
    "        x = x.to(self.device)\n",
    "\n",
    "        # input only one sample\n",
    "        if np.random.uniform() < EPSILON:   # greedy\n",
    "            actions_value = self.eval_net.forward(x)\n",
    "            actions_value = actions_value.to(torch.device(\"cpu\"))\n",
    "            action = torch.max(actions_value, 1)[1].data.numpy()\n",
    "            action = action[0] # return the argmax index\n",
    "        else:   # random\n",
    "            action = np.random.randint(0, N_ACTIONS)\n",
    "            action = action\n",
    "        return action\n",
    "    \n",
    "    def choose_greedy_action(self, x):\n",
    "        x = torch.unsqueeze(torch.FloatTensor(x), 0)\n",
    "        x = x.to(self.device)\n",
    "\n",
    "        # input only one sample\n",
    "        if True:   # greedy\n",
    "            actions_value = self.eval_net.forward(x)\n",
    "            actions_value = actions_value.to(torch.device(\"cpu\"))\n",
    "            action = torch.max(actions_value, 1)[1].data.numpy()\n",
    "            action = action[0] # return the argmax index\n",
    "        return action\n",
    "\n",
    "    def store_transition(self, s, a, r, s_):\n",
    "        transition = np.hstack((s, [a, r], s_))\n",
    "        # replace the old memory with new memory\n",
    "        index = self.memory_counter % MEMORY_CAPACITY\n",
    "        self.memory[index, :] = transition\n",
    "        self.memory_counter += 1\n",
    "    \n",
    "    def store_day_transition(self, transition_rec):\n",
    "        data = transition_rec\n",
    "        index = self.memory_counter % MEMORY_CAPACITY\n",
    "        self.memory= np.insert(self.memory, index, data,0)\n",
    "        self.memory_counter += transition_rec.shape[0]\n",
    "\n",
    "    def learn(self):\n",
    "        # target parameter update\n",
    "        if self.learn_step_counter % TARGET_REPLACE_ITER == 0:\n",
    "            self.target_net.load_state_dict(self.eval_net.state_dict())\n",
    "            self.nettoggle = not self.nettoggle\n",
    "        self.learn_step_counter += 1\n",
    "\n",
    "        # sample batch transitions\n",
    "        sample_index = np.random.choice(MEMORY_CAPACITY, BATCH_SIZE)\n",
    "        b_memory = self.memory[sample_index, :]\n",
    "        b_s = torch.FloatTensor(b_memory[:, :N_STATES])\n",
    "        b_a = torch.LongTensor(b_memory[:, N_STATES:N_STATES+1].astype(int))\n",
    "        b_r = torch.FloatTensor(b_memory[:, N_STATES+1:N_STATES+2])\n",
    "        b_s_ = torch.FloatTensor(b_memory[:, -N_STATES:])\n",
    "        \n",
    "        b_s = b_s.to(self.device)\n",
    "        b_a = b_a.to(self.device)\n",
    "        b_r = b_r.to(self.device)\n",
    "        b_s_ = b_s_.to(self.device)\n",
    "\n",
    "        # q_eval w.r.t the action in experience\n",
    "        q_eval = self.eval_net(b_s).gather(1, b_a)  # shape (batch, 1)\n",
    "        q_next = self.target_net(b_s_).detach()     # detach from graph, don't backpropagate\n",
    "        q_target = b_r + GAMMA * q_next.max(1)[0].view(BATCH_SIZE, 1)   # shape (batch, 1)\n",
    "        loss = self.loss_func(q_eval, q_target)\n",
    "\n",
    "        self.optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        self.optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def stdize(s):\n",
    "    MU_BATT = 0.5\n",
    "    SD_BATT = 0.15\n",
    "    \n",
    "    MU_ENP = 0\n",
    "    SD_ENP = 0.15\n",
    "    \n",
    "    MU_HENERGY = 0.35\n",
    "    SD_HENERGY = 0.25\n",
    "    \n",
    "    MU_FCAST = 0.42\n",
    "    SD_FCAST = 0.27\n",
    "    \n",
    "    norm_batt, norm_enp, norm_henergy, norm_fcast = s\n",
    "    \n",
    "    std_batt = (norm_batt - MU_BATT)/SD_BATT\n",
    "    std_enp = (norm_enp - MU_ENP)/SD_ENP\n",
    "    std_henergy = (norm_henergy - MU_HENERGY)/SD_HENERGY\n",
    "    std_fcast = (norm_fcast - MU_FCAST)/SD_FCAST\n",
    "\n",
    "\n",
    "    return [std_batt, std_enp, std_henergy, std_fcast]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "TRAINING IN PROGRESS\n",
      "\n",
      "Device:  cpu\n",
      "\n",
      "Iteration 0:  TOKYO, 2008 \n",
      "Average Reward \t\t= -5.116\n",
      "Violation Counter \t= 316\n",
      "\n",
      "Iteration 1:  TOKYO, 2003 \n",
      "Average Reward \t\t= -7.403\n",
      "Violation Counter \t= 362\n",
      "\n",
      "Iteration 2:  TOKYO, 2008 \n",
      "Average Reward \t\t= -6.815\n",
      "Violation Counter \t= 356\n",
      "\n",
      "Iteration 3:  TOKYO, 2002 \n",
      "Average Reward \t\t= -5.701\n",
      "Violation Counter \t= 306\n",
      "\n",
      "Iteration 4:  TOKYO, 2003 \n",
      "Average Reward \t\t= -4.666\n",
      "Violation Counter \t= 252\n",
      "\n",
      "Iteration 5:  TOKYO, 2009 \n",
      "Average Reward \t\t= -2.308\n",
      "Violation Counter \t= 160\n",
      "\n",
      "Iteration 6:  TOKYO, 2007 \n",
      "Average Reward \t\t= -1.198\n",
      "Violation Counter \t= 107\n",
      "\n",
      "Iteration 7:  TOKYO, 2000 \n",
      "Average Reward \t\t= -0.100\n",
      "Violation Counter \t= 54\n",
      "\n",
      "Iteration 8:  TOKYO, 2007 \n",
      "Average Reward \t\t= -0.083\n",
      "Violation Counter \t= 55\n",
      "\n",
      "Iteration 9:  TOKYO, 2007 \n",
      "Average Reward \t\t= 0.150\n",
      "Violation Counter \t= 50\n",
      "\n",
      "Iteration 10:  TOKYO, 2000 \n",
      "Average Reward \t\t= -0.038\n",
      "Violation Counter \t= 56\n",
      "\n",
      "Iteration 11:  TOKYO, 2007 \n",
      "Average Reward \t\t= 0.093\n",
      "Violation Counter \t= 52\n",
      "\n",
      "Iteration 12:  TOKYO, 2000 \n",
      "Average Reward \t\t= 0.078\n",
      "Violation Counter \t= 48\n",
      "\n",
      "Iteration 13:  TOKYO, 2007 \n",
      "Average Reward \t\t= 0.173\n",
      "Violation Counter \t= 47\n",
      "\n",
      "Iteration 14:  TOKYO, 2009 \n",
      "Average Reward \t\t= -0.065\n",
      "Violation Counter \t= 61\n",
      "\n",
      "Iteration 15:  TOKYO, 2001 \n",
      "Average Reward \t\t= -0.001\n",
      "Violation Counter \t= 58\n",
      "\n",
      "Iteration 16:  TOKYO, 2009 \n",
      "Average Reward \t\t= 0.008\n",
      "Violation Counter \t= 56\n",
      "\n",
      "Iteration 17:  TOKYO, 2007 \n",
      "Average Reward \t\t= 0.138\n",
      "Violation Counter \t= 50\n",
      "\n",
      "Iteration 18:  TOKYO, 2000 \n",
      "Average Reward \t\t= 0.200\n",
      "Violation Counter \t= 54\n",
      "\n",
      "Iteration 19:  TOKYO, 2000 \n",
      "Average Reward \t\t= 0.088\n",
      "Violation Counter \t= 59\n",
      "\n",
      "Iteration 20:  TOKYO, 2004 \n",
      "Average Reward \t\t= 0.054\n",
      "Violation Counter \t= 64\n",
      "\n",
      "Iteration 21:  TOKYO, 2008 \n",
      "Average Reward \t\t= 0.426\n",
      "Violation Counter \t= 39\n",
      "\n",
      "Iteration 22:  TOKYO, 2004 \n",
      "Average Reward \t\t= 0.239\n",
      "Violation Counter \t= 50\n",
      "\n",
      "Iteration 23:  TOKYO, 2006 \n",
      "Average Reward \t\t= -0.378\n",
      "Violation Counter \t= 74\n",
      "\n",
      "Iteration 24:  TOKYO, 2007 \n",
      "Average Reward \t\t= 0.417\n",
      "Violation Counter \t= 43\n",
      "\n",
      "Iteration 25:  TOKYO, 2003 \n",
      "Average Reward \t\t= 0.065\n",
      "Violation Counter \t= 56\n",
      "\n",
      "Iteration 26:  TOKYO, 2005 \n",
      "Average Reward \t\t= 0.667\n",
      "Violation Counter \t= 32\n",
      "\n",
      "Iteration 27:  TOKYO, 2007 \n",
      "Average Reward \t\t= 0.470\n",
      "Violation Counter \t= 39\n",
      "\n",
      "Iteration 28:  TOKYO, 2008 \n",
      "Average Reward \t\t= 0.370\n",
      "Violation Counter \t= 44\n",
      "\n",
      "Iteration 29:  TOKYO, 2004 \n",
      "Average Reward \t\t= 0.564\n",
      "Violation Counter \t= 35\n",
      "\n",
      "Iteration 30:  TOKYO, 2006 \n",
      "Average Reward \t\t= -0.396\n",
      "Violation Counter \t= 76\n",
      "\n",
      "Iteration 31:  TOKYO, 2002 \n",
      "Average Reward \t\t= -0.284\n",
      "Violation Counter \t= 77\n",
      "\n",
      "Iteration 32:  TOKYO, 2007 \n",
      "Average Reward \t\t= 0.529\n",
      "Violation Counter \t= 39\n",
      "\n",
      "Iteration 33:  TOKYO, 2000 \n",
      "Average Reward \t\t= 0.519\n",
      "Violation Counter \t= 37\n",
      "\n",
      "Iteration 34:  TOKYO, 2007 \n",
      "Average Reward \t\t= 0.629\n",
      "Violation Counter \t= 32\n",
      "\n",
      "Iteration 35:  TOKYO, 2002 \n",
      "Average Reward \t\t= 0.444\n",
      "Violation Counter \t= 41\n",
      "\n",
      "Iteration 36:  TOKYO, 2003 \n",
      "Average Reward \t\t= 0.550\n",
      "Violation Counter \t= 35\n",
      "\n",
      "Iteration 37:  TOKYO, 2006 \n",
      "Average Reward \t\t= 0.354\n",
      "Violation Counter \t= 48\n",
      "\n",
      "Iteration 38:  TOKYO, 2004 \n",
      "Average Reward \t\t= 0.806\n",
      "Violation Counter \t= 24\n",
      "\n",
      "Iteration 39:  TOKYO, 2008 \n",
      "Average Reward \t\t= 0.724\n",
      "Violation Counter \t= 29\n",
      "\n",
      "Iteration 40:  TOKYO, 2006 \n",
      "Average Reward \t\t= 0.387\n",
      "Violation Counter \t= 42\n",
      "\n",
      "Iteration 41:  TOKYO, 2000 \n",
      "Average Reward \t\t= 0.624\n",
      "Violation Counter \t= 36\n",
      "\n",
      "Iteration 42:  TOKYO, 2000 \n",
      "Average Reward \t\t= 0.728\n",
      "Violation Counter \t= 27\n",
      "\n",
      "Iteration 43:  TOKYO, 2004 \n",
      "Average Reward \t\t= 0.775\n",
      "Violation Counter \t= 26\n",
      "\n",
      "Iteration 44:  TOKYO, 2008 \n",
      "Average Reward \t\t= 0.744\n",
      "Violation Counter \t= 29\n",
      "\n",
      "Iteration 45:  TOKYO, 2007 \n",
      "Average Reward \t\t= 0.697\n",
      "Violation Counter \t= 30\n",
      "\n",
      "Iteration 46:  TOKYO, 2009 \n",
      "Average Reward \t\t= 0.705\n",
      "Violation Counter \t= 28\n",
      "\n",
      "Iteration 47:  TOKYO, 2003 \n",
      "Average Reward \t\t= 0.613\n",
      "Violation Counter \t= 31\n",
      "\n",
      "Iteration 48:  TOKYO, 2007 \n",
      "Average Reward \t\t= 0.855\n",
      "Violation Counter \t= 20\n",
      "\n",
      "Iteration 49:  TOKYO, 2009 \n",
      "Average Reward \t\t= 0.858\n",
      "Violation Counter \t= 22\n"
     ]
    }
   ],
   "source": [
    "#TRAIN \n",
    "dqn = DQN()\n",
    "# for recording weights\n",
    "oldfc1 = dqn.eval_net.fc1.weight.data.cpu().numpy().flatten()\n",
    "old2fc1 = oldfc1\n",
    "\n",
    "oldfc2 = dqn.eval_net.fc2.weight.data.cpu().numpy().flatten()\n",
    "old2fc2 = oldfc2\n",
    "\n",
    "# oldfc3 = dqn.eval_net.fc3.weight.data.cpu().numpy().flatten()\n",
    "# old2fc3 = oldfc3\n",
    "\n",
    "oldout = dqn.eval_net.out.weight.data.cpu().numpy().flatten()\n",
    "old2out = oldout\n",
    "########################################\n",
    "\n",
    "best_iteration = -1\n",
    "best_avg_reward = -1000 #initialize best average reward to very low value\n",
    "reset_counter = 0 #count number of times the battery had to be reset\n",
    "change_hr = 0\n",
    "# PFILENAME = ''.join(random.choice(string.ascii_uppercase + string.digits) for _ in range(8)) #create random filename\n",
    "# BFILENAME = \"best\"+PFILENAME + \".pt\" #this file stores the best model\n",
    "# TFILENAME = \"terminal\"+PFILENAME + \".pt\" #this file stores the last model\n",
    "\n",
    "avg_reward_rec = [] #record the yearly average rewards over the entire duration of training\n",
    "violation_rec = []\n",
    "print('\\nTRAINING IN PROGRESS\\n')\n",
    "print('Device: ', dqn.device)\n",
    "\n",
    "for iteration in range(NO_OF_ITERATIONS):\n",
    "    LOCATION = 'tokyo'#random.choice(['tokyo','wakkanai','minamidaito'])\n",
    "    YEAR = random.choice(np.arange(2000,2010))\n",
    "    capm = CAPM(LOCATION,YEAR,shuffle=False, trainmode=False) #instantiate the CAPM class\n",
    "    capm.eno = ENO(LOCATION,YEAR, shuffle=False, day_balance=False) #instantiate the environment inside the CAPM class\n",
    "    capm.HMAX = capm.eno.SMAX #maximum power output of solar cell is set in CAPM object using the value in ENO object\n",
    "#     clear_output()\n",
    "    print('\\nIteration {}:  {}, {} '.format(iteration, LOCATION.upper(), YEAR))\n",
    "\n",
    "    s, r, day_end, year_end = capm.reset()\n",
    "    yr_record = np.empty(4)\n",
    "\n",
    "    record = np.empty(4) #record for battery, henergy, reward and action\n",
    "    transition_rec = np.zeros((capm.eno.TIME_STEPS, N_STATES * 2 + 2)) #record all the transition in one day\n",
    "\n",
    "    while True:\n",
    "        a = dqn.choose_action(stdize(s))\n",
    "\n",
    "        # present state = [batt, enp, henergy]\n",
    "        record = np.vstack((record, [s[0],s[2],r, a])) # record battery, henergy, reward and action for troubleshooting\n",
    "        yr_record = np.vstack((yr_record, [s[0],s[2],r, a]))\n",
    "\n",
    "        # take action\n",
    "        s_, r, day_end, year_end = capm.step(a)\n",
    "        \n",
    "        temp_transitions = np.hstack((stdize(s), [a, r], stdize(s_)))\n",
    "        transition_rec[capm.eno.hr-1,:] = temp_transitions\n",
    "\n",
    "        if (day_end):\n",
    "            transition_rec[:,5] += r #broadcast reward to all states\n",
    "            decay_factor = [i for i in (LAMBDA**n for n in reversed(range(0, capm.eno.TIME_STEPS)))]\n",
    "            transition_rec[:,5] = transition_rec[:,5] * decay_factor #decay reward proportionately\n",
    "            dqn.store_day_transition(transition_rec)\n",
    "\n",
    "        if dqn.memory_counter > MEMORY_CAPACITY:\n",
    "            dqn.learn()\n",
    "\n",
    "        if dqn.nettoggle:\n",
    "            change_hr = capm.eno.day*24+capm.eno.hr #to mark when the DQN is updated.\n",
    "            dqn.nettoggle = not dqn.nettoggle\n",
    "\n",
    "        if (year_end):\n",
    "            break\n",
    "\n",
    "        # transition to new state\n",
    "        s = s_\n",
    "\n",
    "    record = np.delete(record, 0, 0) #remove the first row which is garbage\n",
    "    reward_rec = record[:,2] #extract reward information from the record array\n",
    "    reward_rec = reward_rec[::24] #only consider terminal rewards\n",
    "    print(\"Average Reward \\t\\t= {:.3f}\".format(np.mean(reward_rec)))\n",
    "    print(\"Violation Counter \\t= {}\".format(capm.violation_counter))\n",
    "\n",
    "#     if(best_avg_reward < np.mean(reward_rec)):\n",
    "#         best_avg_reward = np.mean(reward_rec)\n",
    "    \n",
    "#     if(best_avg_reward > 1.5 or iteration > 20):\n",
    "#         EPSILON = 0.9\n",
    "#         LR = 0.01\n",
    "        \n",
    "#     if (capm.violation_counter < 5):\n",
    "#         reset_flag = False\n",
    "#         EPSILON = 0.95\n",
    "#         LR = 0.001\n",
    "        \n",
    "\n",
    "#     # Check if reward beats the High Score and possible save it    \n",
    "#     if (iteration > 19): #save the best models only after 20 iterations\n",
    "#         print(\"Best Score \\t = {:8.3f} @ Iteration No. {}\".format(best_avg_reward, best_iteration))\n",
    "#         if(best_avg_reward < np.mean(reward_rec)):\n",
    "#             best_iteration = iteration\n",
    "#             best_avg_reward = np.mean(reward_rec)\n",
    "#             print(\"Saving Model\")\n",
    "#             torch.save(dqn.eval_net.state_dict(), BFILENAME)\n",
    "#     else:\n",
    "#         print(\"\\r\")\n",
    "\n",
    "    # Log the average reward in avg_reward_rec\n",
    "    avg_reward_rec = np.append(avg_reward_rec, np.mean(reward_rec))\n",
    "    violation_rec = np.append(violation_rec, capm.violation_counter)\n",
    "\n",
    "    \n",
    "###########################################################################################\n",
    "# #   PLOT battery levels, hourly rewards and the weights\n",
    "#     yr_record = np.delete(yr_record, 0, 0) #remove the first row which is garbage\n",
    "# #     NO_OF_DAYS = capm.eno.NO_OF_DAYS\n",
    "#     hourly_yr_reward_rec = yr_record[:,2]\n",
    "#     yr_reward_rec = hourly_yr_reward_rec[::24]\n",
    "\n",
    "    \n",
    "#     fig = plt.figure(figsize=(24,3))\n",
    "#     TIME_STEPS = capm.eno.TIME_STEPS\n",
    "#     NO_OF_DAYS = capm.eno.NO_OF_DAYS\n",
    "#     DAY_SPACING = 15\n",
    "#     TICK_SPACING = TIME_STEPS*DAY_SPACING\n",
    "#     #plot battery\n",
    "#     ax = fig.add_subplot(111)\n",
    "#     ax.plot(np.arange(0,TIME_STEPS*NO_OF_DAYS),yr_record[:,0],'r')\n",
    "#     ax.set_ylim([0,1])\n",
    "#     ax.axvline(x=change_hr)\n",
    "#     ax.xaxis.set_major_locator(ticker.MultipleLocator(TICK_SPACING))\n",
    "# #     labels = [item for item in ax.get_xticklabels()]\n",
    "# #     print(labels)\n",
    "# #     labels [15:-1] = np.arange(0,NO_OF_DAYS,DAY_SPACING) #the first label is reserved to negative values\n",
    "# #     ax.set_xticklabels(labels)\n",
    "#     #plot hourly reward\n",
    "#     ax0 = ax.twinx()\n",
    "#     ax0.plot(hourly_yr_reward_rec, color='m')\n",
    "#     ax0.set_ylim(-7,3)\n",
    "#     plt.show()\n",
    "\n",
    "\n",
    "#     fig = plt.figure(figsize=(18,3))\n",
    "#     ax1 = fig.add_subplot(131)\n",
    "#     newfc1 = dqn.eval_net.fc1.weight.data.cpu().numpy().flatten()\n",
    "#     ax1.plot(old2fc1,color='b', alpha=0.4)\n",
    "#     ax1.plot(oldfc1,color='b',alpha = 0.7)\n",
    "#     ax1.plot(newfc1,color='b')\n",
    "#     old2fc1 = oldfc1\n",
    "#     oldfc1 = newfc1\n",
    "    \n",
    "#     ax2 = fig.add_subplot(132)\n",
    "#     newfc2 = dqn.eval_net.fc2.weight.data.cpu().numpy().flatten()\n",
    "#     ax2.plot(old2fc2,color='y', alpha=0.4)\n",
    "#     ax2.plot(oldfc2,color='y',alpha = 0.7)\n",
    "#     ax2.plot(newfc2,color='y')\n",
    "#     old2fc2 = oldfc2\n",
    "#     oldfc2 = newfc2\n",
    "    \n",
    "# #     ax3 = fig.add_subplot(143)\n",
    "# #     newfc3 = dqn.eval_net.fc3.weight.data.cpu().numpy().flatten()\n",
    "# #     ax3.plot(old2fc3,color='y', alpha=0.4)\n",
    "# #     ax3.plot(oldfc3,color='y',alpha = 0.7)\n",
    "# #     ax3.plot(newfc3,color='y')\n",
    "# #     old2fc3 = oldfc3\n",
    "# #     oldfc3 = newfc3\n",
    "    \n",
    "#     axO = fig.add_subplot(133)\n",
    "#     newout = dqn.eval_net.out.weight.data.cpu().numpy().flatten()\n",
    "#     axO.plot(old2out,color='g', alpha=0.4)\n",
    "#     axO.plot(oldout,color='g',alpha=0.7)\n",
    "#     axO.plot(newout,color='g')\n",
    "#     old2out = oldout\n",
    "#     oldout = newout\n",
    "    \n",
    "#     fig.tight_layout()\n",
    "#     plt.show()\n",
    "\n",
    "\n",
    "    # End of training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 648x216 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig = plt.figure(figsize=(9,3))\n",
    "# Plot the average reward log\n",
    "\n",
    "ax1 = fig.add_subplot(111)\n",
    "ax1.set_ylabel(\"Reward\")\n",
    "# ax1.set_ylim([-3,3]);\n",
    "ax1.plot(avg_reward_rec,'b')\n",
    "ax1.tick_params(axis='y', colors='b')\n",
    "\n",
    "#Plot the violation record log\n",
    "ax2 = ax1.twinx()\n",
    "ax2.set_ylabel(\"Violations\",color = 'r')\n",
    "ax2.plot(violation_rec,'r')\n",
    "for xpt in np.argwhere(violation_rec<1):\n",
    "    ax2.axvline(x=xpt,color='g')\n",
    "ax2.set_ylim([0,50]);\n",
    "ax2.tick_params(axis='y', colors='r')\n",
    "\n",
    "fig.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device:  cpu\n",
      "runtime: 0:12:31.680661\n"
     ]
    }
   ],
   "source": [
    "print('Device: ', dqn.device)\n",
    "print('runtime: {}'.format(datetime.now() - tic))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPSILON =  0.98\n",
      "LR      =  5e-05\n",
      "\n",
      "LAST PHASE ITERATION #0:  TOKYO, 2000 \n",
      "Average Reward \t\t= 1.149\n",
      "Violation Counter \t= 8\n",
      "***MEASURING PERFORMANCE OF THE MODEL***\n",
      "\tBEST AVERAGE ANNUAL AVERAGE REWARD = -1000.000\n",
      "\tBEST TOTAL VIOLATIONS              = 1000\n",
      "\n",
      "\tAverage Annual Average Reward      = 1.310\n",
      "\tTotal Violations                   = 16.0\n",
      "****************************************\n",
      "\n",
      "LAST PHASE ITERATION #1:  TOKYO, 2004 \n",
      "Average Reward \t\t= 1.243\n",
      "Violation Counter \t= 7\n",
      "***MEASURING PERFORMANCE OF THE MODEL***\n",
      "\tBEST AVERAGE ANNUAL AVERAGE REWARD = 1.310\n",
      "\tBEST TOTAL VIOLATIONS              = 16.0\n",
      "\n",
      "\tAverage Annual Average Reward      = 1.382\n",
      "\tTotal Violations                   = 9.0\n",
      "****************************************\n",
      "\n",
      "LAST PHASE ITERATION #2:  TOKYO, 2008 \n",
      "Average Reward \t\t= 1.365\n",
      "Violation Counter \t= 0\n",
      "***MEASURING PERFORMANCE OF THE MODEL***\n",
      "\tBEST AVERAGE ANNUAL AVERAGE REWARD = 1.382\n",
      "\tBEST TOTAL VIOLATIONS              = 9.0\n",
      "\n",
      "\tAverage Annual Average Reward      = 1.321\n",
      "\tTotal Violations                   = 20.0\n",
      "****************************************\n",
      "\n",
      "LAST PHASE ITERATION #3:  TOKYO, 2001 \n",
      "Average Reward \t\t= 1.409\n",
      "Violation Counter \t= 0\n",
      "***MEASURING PERFORMANCE OF THE MODEL***\n",
      "\tBEST AVERAGE ANNUAL AVERAGE REWARD = 1.382\n",
      "\tBEST TOTAL VIOLATIONS              = 9.0\n",
      "\n",
      "\tAverage Annual Average Reward      = 1.333\n",
      "\tTotal Violations                   = 17.0\n",
      "****************************************\n",
      "\n",
      "LAST PHASE ITERATION #4:  TOKYO, 2008 \n",
      "Average Reward \t\t= 1.362\n",
      "Violation Counter \t= 3\n",
      "***MEASURING PERFORMANCE OF THE MODEL***\n",
      "\tBEST AVERAGE ANNUAL AVERAGE REWARD = 1.382\n",
      "\tBEST TOTAL VIOLATIONS              = 9.0\n",
      "\n",
      "\tAverage Annual Average Reward      = 1.343\n",
      "\tTotal Violations                   = 12.0\n",
      "****************************************\n",
      "\n",
      "LAST PHASE ITERATION #5:  TOKYO, 2005 \n",
      "Average Reward \t\t= 1.424\n",
      "Violation Counter \t= 4\n",
      "***MEASURING PERFORMANCE OF THE MODEL***\n",
      "\tBEST AVERAGE ANNUAL AVERAGE REWARD = 1.382\n",
      "\tBEST TOTAL VIOLATIONS              = 9.0\n",
      "\n",
      "\tAverage Annual Average Reward      = 1.293\n",
      "\tTotal Violations                   = 25.0\n",
      "****************************************\n",
      "\n",
      "LAST PHASE ITERATION #6:  TOKYO, 2003 \n",
      "Average Reward \t\t= 1.178\n",
      "Violation Counter \t= 10\n",
      "***MEASURING PERFORMANCE OF THE MODEL***\n",
      "\tBEST AVERAGE ANNUAL AVERAGE REWARD = 1.382\n",
      "\tBEST TOTAL VIOLATIONS              = 9.0\n",
      "\n",
      "\tAverage Annual Average Reward      = 1.270\n",
      "\tTotal Violations                   = 29.0\n",
      "****************************************\n",
      "\n",
      "LAST PHASE ITERATION #7:  TOKYO, 2008 \n",
      "Average Reward \t\t= 1.263\n",
      "Violation Counter \t= 7\n",
      "***MEASURING PERFORMANCE OF THE MODEL***\n",
      "\tBEST AVERAGE ANNUAL AVERAGE REWARD = 1.382\n",
      "\tBEST TOTAL VIOLATIONS              = 9.0\n",
      "\n",
      "\tAverage Annual Average Reward      = 1.309\n",
      "\tTotal Violations                   = 18.0\n",
      "****************************************\n",
      "\n",
      "LAST PHASE ITERATION #8:  TOKYO, 2005 \n",
      "Average Reward \t\t= 1.420\n",
      "Violation Counter \t= 3\n",
      "***MEASURING PERFORMANCE OF THE MODEL***\n",
      "\tBEST AVERAGE ANNUAL AVERAGE REWARD = 1.382\n",
      "\tBEST TOTAL VIOLATIONS              = 9.0\n",
      "\n",
      "\tAverage Annual Average Reward      = 0.676\n",
      "\tTotal Violations                   = 152.0\n",
      "****************************************\n",
      "\n",
      "LAST PHASE ITERATION #9:  TOKYO, 2003 \n",
      "Average Reward \t\t= 0.931\n",
      "Violation Counter \t= 22\n",
      "***MEASURING PERFORMANCE OF THE MODEL***\n",
      "\tBEST AVERAGE ANNUAL AVERAGE REWARD = 1.382\n",
      "\tBEST TOTAL VIOLATIONS              = 9.0\n",
      "\n",
      "\tAverage Annual Average Reward      = 1.287\n",
      "\tTotal Violations                   = 29.0\n",
      "****************************************\n"
     ]
    }
   ],
   "source": [
    "#END OF TRAINING PHASE - CHOOSING THE BEST MODEL INSTANCE\n",
    "#INCREASE GREEDY RATE\n",
    "#VALIDATE AFTER EVERY ITERATION\n",
    "\n",
    "# Use this model and its output as base standards for the last phase of training\n",
    "best_avg_avg_reward = -1000\n",
    "best_net_avg_reward = dqn.eval_net\n",
    "best_avg_v_counter = 1000\n",
    "best_net_v_counter = dqn.eval_net\n",
    "\n",
    "\n",
    "NO_OF_LAST_PHASE_ITERATIONS = 10\n",
    "EPSILON = 0.98\n",
    "LR = 0.00005\n",
    "\n",
    "print(\"EPSILON = \", EPSILON)\n",
    "print(\"LR      = \", LR)\n",
    "\n",
    "for iteration in range(NO_OF_LAST_PHASE_ITERATIONS):\n",
    "    LOCATION = 'tokyo'#random.choice(['tokyo','wakkanai','minamidaito'])\n",
    "    YEAR = random.choice(np.arange(2000,2010))\n",
    "    capm = CAPM(LOCATION,YEAR,shuffle=False, trainmode=False) #instantiate the CAPM class\n",
    "    capm.eno = ENO(LOCATION,YEAR, shuffle=False, day_balance=False) #instantiate the environment inside the CAPM class\n",
    "    capm.HMAX = capm.eno.SMAX #maximum power output of solar cell is set in CAPM object using the value in ENO object\n",
    "\n",
    "    print('\\nLAST PHASE ITERATION #{}:  {}, {} '.format(iteration, LOCATION.upper(), YEAR))\n",
    "    \n",
    "    \n",
    "    my_avg_reward = -1000\n",
    "    my_v_counter = 1000\n",
    "    \n",
    "    s, r, day_end, year_end = capm.reset()\n",
    "    yr_record = np.empty(4)\n",
    "\n",
    "    record = np.empty(4) #record for battery, henergy, reward and action\n",
    "    transition_rec = np.zeros((capm.eno.TIME_STEPS, N_STATES * 2 + 2)) #record all the transition in one day\n",
    "\n",
    "    while True:\n",
    "        a = dqn.choose_action(stdize(s))\n",
    "\n",
    "        # present state = [batt, enp, henergy]\n",
    "        record = np.vstack((record, [s[0],s[2],r, a])) # record battery, henergy, reward and action for troubleshooting\n",
    "        yr_record = np.vstack((yr_record, [s[0],s[2],r, a]))\n",
    "\n",
    "        # take action\n",
    "        s_, r, day_end, year_end = capm.step(a)\n",
    "        \n",
    "        temp_transitions = np.hstack((stdize(s), [a, r], stdize(s_)))\n",
    "        transition_rec[capm.eno.hr-1,:] = temp_transitions\n",
    "\n",
    "        if (day_end):\n",
    "            transition_rec[:,5] += r #broadcast reward to all states\n",
    "            decay_factor = [i for i in (LAMBDA**n for n in reversed(range(0, capm.eno.TIME_STEPS)))]\n",
    "            transition_rec[:,5] = transition_rec[:,5] * decay_factor #decay reward proportionately\n",
    "            dqn.store_day_transition(transition_rec)\n",
    "\n",
    "        if dqn.memory_counter > MEMORY_CAPACITY:\n",
    "            dqn.learn()\n",
    "\n",
    "        if dqn.nettoggle:\n",
    "            change_hr = capm.eno.day*24+capm.eno.hr #to mark when the DQN is updated.\n",
    "            dqn.nettoggle = not dqn.nettoggle\n",
    "\n",
    "        if (year_end):\n",
    "            break\n",
    "\n",
    "        # transition to new state\n",
    "        s = s_\n",
    "\n",
    "    record = np.delete(record, 0, 0) #remove the first row which is garbage\n",
    "    reward_rec = record[:,2] #extract reward information from the record array\n",
    "    reward_rec = reward_rec[::24] #only consider terminal rewards\n",
    "    print(\"Average Reward \\t\\t= {:.3f}\".format(np.mean(reward_rec)))\n",
    "    print(\"Violation Counter \\t= {}\".format(capm.violation_counter))\n",
    "    \n",
    "    \n",
    "    print(\"***MEASURING PERFORMANCE OF THE MODEL***\")\n",
    "    print(\"\\tBEST AVERAGE ANNUAL AVERAGE REWARD = {:.3f}\".format(best_avg_avg_reward))\n",
    "    print(\"\\tBEST TOTAL VIOLATIONS              = {}\".format(best_avg_v_counter))\n",
    "    LOCATION = 'tokyo'\n",
    "    results = np.empty(3)\n",
    "    for YEAR in np.arange(2010,2015):\n",
    "        capm = CAPM(LOCATION,YEAR,shuffle=False, trainmode=False) #instantiate the CAPM class\n",
    "        capm.eno = ENO(LOCATION,YEAR, shuffle=False, day_balance=False) #instantiate the environment inside the CAPM class\n",
    "        capm.HMAX = capm.eno.SMAX #maximum power output of solar cell is set in CAPM object using the value in ENO object\n",
    "\n",
    "        s, r, day_end, year_end = capm.reset()\n",
    "        yr_test_record = np.empty(4)\n",
    "        EPSILON = 1\n",
    "\n",
    "        while True:\n",
    "            a = dqn.choose_greedy_action(stdize(s))\n",
    "            #state = [batt, enp, henergy, fcast]\n",
    "            yr_test_record = np.vstack((yr_test_record, [s[0],s[2],r, a])) #record battery, henergy, reward and action\n",
    "            # take action\n",
    "            s_, r, day_end, year_end = capm.step(a)\n",
    "            if year_end:\n",
    "                break\n",
    "            s = s_\n",
    "\n",
    "        yr_test_record = np.delete(yr_test_record, 0, 0) #remove the first row which is garbage\n",
    "        yr_test_reward_rec = yr_test_record[:,2]\n",
    "        yr_test_reward_rec = yr_test_reward_rec[::24] #annual average reward\n",
    "        results = np.vstack((results, [int(YEAR), np.mean(yr_test_reward_rec), int(capm.violation_counter)]))\n",
    "    results = np.delete(results,0,0)\n",
    "    my_avg_reward = np.mean(results[:,1]) #the average of annual average rewards\n",
    "    my_v_counter = np.sum(results[:,-1]) #total sum of violations\n",
    "    print(\"\\n\\tAverage Annual Average Reward      = {:.3f}\".format(my_avg_reward))\n",
    "    print(\"\\tTotal Violations                   = {}\".format(my_v_counter))\n",
    "\n",
    "    if (my_avg_reward > best_avg_avg_reward):\n",
    "            best_avg_avg_reward = my_avg_reward\n",
    "            best_net_avg_reward = dqn.eval_net\n",
    "\n",
    "    if (my_v_counter < best_avg_v_counter):\n",
    "        best_avg_v_counter = my_v_counter\n",
    "        best_net_v_counter = dqn.eval_net\n",
    "    elif (my_v_counter == best_avg_v_counter):\n",
    "        if (my_avg_reward > best_avg_avg_reward):\n",
    "            best_avg_v_counter = my_v_counter\n",
    "            best_net_v_counter = dqn.eval_net\n",
    "    print(\"****************************************\")\n",
    "\n",
    "    \n",
    "\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TESTING BEST MODEL BASED ON AVERAGE REWARD METRIC\n",
      "YEAR\t\tAVG_RWD\t\tVIOLATIONS\n",
      "2015 \t\t 1.25 \t\t 4\n",
      "2016 \t\t 1.33 \t\t 3\n",
      "2017 \t\t 1.33 \t\t 7\n",
      "2018 \t\t 1.39 \t\t 0\n"
     ]
    }
   ],
   "source": [
    "#TESTING BEST MODEL BASED ON AVERAGE REWARD METRIC\n",
    "dqn = DQN()\n",
    "dqn.eval_net = best_net_avg_reward\n",
    "LOCATION = 'tokyo'\n",
    "results = np.empty(3)\n",
    "for YEAR in np.arange(2015,2019):\n",
    "    capm = CAPM(LOCATION,YEAR,shuffle=False, trainmode=False) #instantiate the CAPM class\n",
    "    capm.eno = ENO(LOCATION,YEAR, shuffle=False, day_balance=False) #instantiate the environment inside the CAPM class\n",
    "    capm.HMAX = capm.eno.SMAX #maximum power output of solar cell is set in CAPM object using the value in ENO object\n",
    "\n",
    "    s, r, day_end, year_end = capm.reset()\n",
    "    yr_test_record = np.empty(4)\n",
    "    EPSILON = 1\n",
    "\n",
    "    while True:\n",
    "        a = dqn.choose_greedy_action(stdize(s))\n",
    "\n",
    "        #state = [batt, enp, henergy, fcast]\n",
    "        yr_test_record = np.vstack((yr_test_record, [s[0],s[2],r, a])) #record battery, henergy, reward and action\n",
    "\n",
    "        # take action\n",
    "        s_, r, day_end, year_end = capm.step(a)\n",
    "\n",
    "        if year_end:\n",
    "            break\n",
    "\n",
    "        s = s_\n",
    "\n",
    "    yr_test_record = np.delete(yr_test_record, 0, 0) #remove the first row which is garbage\n",
    "\n",
    "#     #Plot the reward and battery for the entire year run\n",
    "#     title = LOCATION.upper() + ',' + str(YEAR)\n",
    "\n",
    "#     NO_OF_DAYS = capm.eno.NO_OF_DAYS\n",
    "    yr_test_reward_rec = yr_test_record[:,2]\n",
    "    yr_test_reward_rec = yr_test_reward_rec[::24]\n",
    "#     print('Average Reward for',title, '=', np.mean(yr_test_reward_rec))\n",
    "#     print('Violations for',title, '=', capm.violation_counter)\n",
    "    \n",
    "    results = np.vstack((results, [int(YEAR), np.mean(yr_test_reward_rec), int(capm.violation_counter)]))\n",
    "\n",
    "#     fig = plt.figure(figsize=(24,3))\n",
    "#     fig.suptitle(title, fontsize=15)\n",
    "\n",
    "#     #     ax1 = fig.add_subplot(211)\n",
    "#     #     ax1.plot(yr_test_reward_rec)\n",
    "#     #     ax1.set_title(\"\\n\\nYear Run Reward\")\n",
    "#     #     ax1.set_ylim([-3,1])\n",
    "\n",
    "#     ax2 = fig.add_subplot(111)\n",
    "#     ax2.plot(yr_test_record[:,0],'r')\n",
    "#     ax2.set_title(\"\\n\\nYear Run Battery\")\n",
    "#     ax2.set_ylim([0,1])\n",
    "#     plt.sca(ax2)\n",
    "#     plt.xticks(np.arange(0, NO_OF_DAYS*24, 50*24),np.arange(0,NO_OF_DAYS,50))\n",
    "\n",
    "#     fig.tight_layout()\n",
    "#     plt.show()\n",
    "\n",
    "results = np.delete(results,0,0)\n",
    "\n",
    "print(\"TESTING BEST MODEL BASED ON AVERAGE REWARD METRIC\")\n",
    "print('YEAR\\t\\tAVG_RWD\\t\\tVIOLATIONS')\n",
    "for x in np.arange(0,results.shape[0]):\n",
    "    print('{} \\t\\t {} \\t\\t {}'.format(int(results[x,0]), np.around(results[x,1],2), int(results[x,-1])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TESTING BASED ON VIOLATION COUNTER METRIC\n",
      "YEAR\t\tAVG_RWD\t\tVIOLATIONS\n",
      "2015 \t\t 1.25 \t\t 4\n",
      "2016 \t\t 1.33 \t\t 3\n",
      "2017 \t\t 1.33 \t\t 7\n",
      "2018 \t\t 1.39 \t\t 0\n"
     ]
    }
   ],
   "source": [
    "#TESTING BASED ON VIOLATION COUNTER METRIC\n",
    "dqn = DQN()\n",
    "dqn.eval_net = best_net_v_counter\n",
    "\n",
    "LOCATION = 'tokyo'\n",
    "results = np.empty(3)\n",
    "for YEAR in np.arange(2015,2019):\n",
    "    capm = CAPM(LOCATION,YEAR,shuffle=False, trainmode=False) #instantiate the CAPM class\n",
    "    capm.eno = ENO(LOCATION,YEAR, shuffle=False, day_balance=False) #instantiate the environment inside the CAPM class\n",
    "    capm.HMAX = capm.eno.SMAX #maximum power output of solar cell is set in CAPM object using the value in ENO object\n",
    "\n",
    "    s, r, day_end, year_end = capm.reset()\n",
    "    yr_test_record = np.empty(4)\n",
    "    EPSILON = 1\n",
    "\n",
    "    while True:\n",
    "        a = dqn.choose_greedy_action(stdize(s))\n",
    "\n",
    "        #state = [batt, enp, henergy, fcast]\n",
    "        yr_test_record = np.vstack((yr_test_record, [s[0],s[2],r, a])) #record battery, henergy, reward and action\n",
    "\n",
    "        # take action\n",
    "        s_, r, day_end, year_end = capm.step(a)\n",
    "\n",
    "        if year_end:\n",
    "            break\n",
    "\n",
    "        s = s_\n",
    "\n",
    "    yr_test_record = np.delete(yr_test_record, 0, 0) #remove the first row which is garbage\n",
    "\n",
    "#     #Plot the reward and battery for the entire year run\n",
    "#     title = LOCATION.upper() + ',' + str(YEAR)\n",
    "\n",
    "#     NO_OF_DAYS = capm.eno.NO_OF_DAYS\n",
    "    yr_test_reward_rec = yr_test_record[:,2]\n",
    "    yr_test_reward_rec = yr_test_reward_rec[::24]\n",
    "#     print('Average Reward for',title, '=', np.mean(yr_test_reward_rec))\n",
    "#     print('Violations for',title, '=', capm.violation_counter)\n",
    "    \n",
    "    results = np.vstack((results, [int(YEAR), np.mean(yr_test_reward_rec), int(capm.violation_counter)]))\n",
    "\n",
    "#     fig = plt.figure(figsize=(24,3))\n",
    "#     fig.suptitle(title, fontsize=15)\n",
    "\n",
    "#     #     ax1 = fig.add_subplot(211)\n",
    "#     #     ax1.plot(yr_test_reward_rec)\n",
    "#     #     ax1.set_title(\"\\n\\nYear Run Reward\")\n",
    "#     #     ax1.set_ylim([-3,1])\n",
    "\n",
    "#     ax2 = fig.add_subplot(111)\n",
    "#     ax2.plot(yr_test_record[:,0],'r')\n",
    "#     ax2.set_title(\"\\n\\nYear Run Battery\")\n",
    "#     ax2.set_ylim([0,1])\n",
    "#     plt.sca(ax2)\n",
    "#     plt.xticks(np.arange(0, NO_OF_DAYS*24, 50*24),np.arange(0,NO_OF_DAYS,50))\n",
    "\n",
    "#     fig.tight_layout()\n",
    "#     plt.show()\n",
    "\n",
    "results = np.delete(results,0,0)\n",
    "\n",
    "print(\"TESTING BASED ON VIOLATION COUNTER METRIC\")\n",
    "print('YEAR\\t\\tAVG_RWD\\t\\tVIOLATIONS')\n",
    "for x in np.arange(0,results.shape[0]):\n",
    "    print('{} \\t\\t {} \\t\\t {}'.format(int(results[x,0]), np.around(results[x,1],2), int(results[x,-1])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #Plot the reward and battery for the entire year run on a day by day basis\n",
    "# title = LOCATION.upper() + ',' + str(YEAR)\n",
    "# TIME_AXIS = np.arange(0,capm.eno.TIME_STEPS)\n",
    "# for DAY in range(0,10):#capm.eno.NO_OF_DAYS):\n",
    "#     START = DAY*24\n",
    "#     END = START+24\n",
    "\n",
    "#     daytitle = title + ' - DAY ' + str(DAY)\n",
    "#     fig = plt.figure(figsize=(16,4))\n",
    "#     st = fig.suptitle(daytitle)\n",
    "\n",
    "#     ax2 = fig.add_subplot(121)\n",
    "#     ax2.plot(yr_test_record[START:END,1],'g')\n",
    "#     ax2.set_title(\"HARVESTED ENERGY\")\n",
    "#     plt.xlabel(\"Hour\")\n",
    "#     ax2.set_ylim([0,1])\n",
    "\n",
    "#     #plot battery for year run\n",
    "#     ax1 = fig.add_subplot(122)\n",
    "#     ax1.plot(TIME_AXIS,yr_test_record[START:END,0],'r') \n",
    "# #     ax1.plot(TIME_AXIS, np.ones(capm.eno.TIME_STEPS)*capm.BOPT/capm.BMAX,'r--')\n",
    "#     ax1.plot(TIME_AXIS, np.ones(capm.eno.TIME_STEPS)*capm.BOPT/capm.BMAX,'r--')\n",
    "#     ax1.text(0.1, 0.2, \"BINIT = %.2f\\n\" %(yr_test_record[START,0]),fontsize=11, ha='left')\n",
    "#     ax1.text(0.1, 0.4, \"TENP = %.2f\\n\" %(capm.BOPT/capm.BMAX-yr_test_record[END,0]),fontsize=11, ha='left')\n",
    "#     ax1.text(0.1, 0.3, \"BMEAN = %.2f\\n\" %(np.mean(yr_test_record[START:END,0])),fontsize=11, ha='left')\n",
    "\n",
    "\n",
    "\n",
    "#     ax1.set_title(\"YEAR RUN TEST\")\n",
    "#     if END < (capm.eno.NO_OF_DAYS*capm.eno.TIME_STEPS):\n",
    "#         ax1.text(0.1, 0, \"REWARD = %.2f\\n\" %(yr_test_record[END,2]),fontsize=13, ha='left')\n",
    "#     plt.xlabel(\"Hour\")\n",
    "#     ax1.set_ylabel('Battery', color='r',fontsize=12)\n",
    "#     ax1.set_ylim([0,1])\n",
    "\n",
    "#     #plot actions for year run\n",
    "#     ax1a = ax1.twinx()\n",
    "#     ax1a.plot(yr_test_record[START:END,3])\n",
    "#     ax1a.set_ylim([0,N_ACTIONS])\n",
    "#     ax1a.set_ylabel('Duty Cycle', color='b',fontsize=12)\n",
    "\n",
    "#     fig.tight_layout()\n",
    "#     st.set_y(0.95)\n",
    "#     fig.subplots_adjust(top=0.75)\n",
    "#     plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
