**************************************************************
C(RELU)
|
|--C1(Huber Loss)--X
|              
|--C2(Leaky RELU)--X
|
|--C3(Xa-Ka-Ka)--X   
|              
|--C4(Ka-Ka-Xa)->C4A(LR=0.0001)->C4B->0C->0C1


HYPOTHESIS:
Increase layer width to see if it improves performance.

MODEL:

0C1 : INPUT->FC1->RELU->FC2->RELU->OUT
LOSS : MSE

LEARNING:   INIT_WEIGHT     = FC1 : KAIMING
                              FC2 : KAIMING
                              OUT : XAVIER
            WIDTH           = 50
            DEPTH           = 2
            WEIGHT_DECAY    = 1E-3
            LR              = 1E-4
            UPDATE_FREQ     = 18 MONTHS
            MEMORY          = 24 MONTHS
            ITERATION       = 50
            BATCH_SIZE      = 32
            EPSILON         = 0.9              
            GAMMA           = 0.9                
            LAMBDA          = 0.9
            
TRAINING:   TOKYO[2000-2009]
            BATTERY_RESET    = 0 %
            REWARD_FUNC      = TYPE2
                               violation_penalty = 3
                               battery full = -2
                               battery dead = -4
                               R = r1*(2**r2) - violation_penalty
            REWARD_BROADCAST = TRUE
            
            LAST TRAINING ITERATIONS = 10 [TOKYO, 2000-2009]
            EPSILON                  = 0.98
            LR                       = 1E-5
            
TESTING:    TOKYO[2015-2018]
            GREEDY POLICY
   

RESULTS:
# Some non-robust behavior

SEED 0
TESTING BEST MODEL BASED ON AVERAGE REWARD METRIC
YEAR		AVG_RWD		VIOLATIONS
2015 		 -0.26 		 66
2016 		 1.35 		 1
2017 		 -0.52 		 77
2018 		 1.21 		 7

TESTING BASED ON VIOLATION COUNTER METRIC
YEAR		AVG_RWD		VIOLATIONS
2015 		 -0.26 		 66
2016 		 1.35 		 1
2017 		 -0.52 		 77
2018 		 1.21 		 7


SEED 1
TESTING BEST MODEL BASED ON AVERAGE REWARD METRIC
YEAR		AVG_RWD		VIOLATIONS
2015 		 1.34 		 0
2016 		 1.39 		 0
2017 		 1.42 		 0
2018 		 1.36 		 0
TESTING BASED ON VIOLATION COUNTER METRIC
YEAR		AVG_RWD		VIOLATIONS
2015 		 1.34 		 0
2016 		 1.39 		 0
2017 		 1.42 		 0
2018 		 1.36 		 0


SEED 2
TESTING BEST MODEL BASED ON AVERAGE REWARD METRIC
YEAR		AVG_RWD		VIOLATIONS
2015 		 1.3 		 0
2016 		 1.32 		 0
2017 		 1.38 		 0
2018 		 1.33 		 0
TESTING BASED ON VIOLATION COUNTER METRIC
YEAR		AVG_RWD		VIOLATIONS
2015 		 1.3 		 0
2016 		 1.32 		 0
2017 		 1.38 		 0
2018 		 1.33 		 0


SEED 3
TESTING BEST MODEL BASED ON AVERAGE REWARD METRIC
YEAR		AVG_RWD		VIOLATIONS
2015 		 1.4 		 0
2016 		 1.44 		 0
2017 		 -0.45 		 76
2018 		 1.45 		 0
TESTING BASED ON VIOLATION COUNTER METRIC
YEAR		AVG_RWD		VIOLATIONS
2015 		 1.4 		 0
2016 		 1.44 		 0
2017 		 -0.45 		 76
2018 		 1.45 		 0


CONCLUSIONS:
Better behavior is shown when Layer width is increased.

DISCUSSION:
The cause of non-robustness should be identifies.
Maybe due to lack of experiences when battery dies.
More analysis required
**************************************************************
**************************************************************
C(RELU)
|
|--C1(Huber Loss)--X
|              
|--C2(Leaky RELU)--X
|
|--C3(Xa-Ka-Ka)--X   
|              
|--C4(Ka-Ka-Xa)->C4A(LR=0.0001)->C4B->0C


HYPOTHESIS:
Proper validation is required.

After 50 iterations, get the best model in the next 10 iterations depending on average annual average reward/total violation counter.

MODEL:

0C : INPUT->FC1->RELU->FC2->RELU->OUT
LOSS : MSE

LEARNING:   INIT_WEIGHT     = FC1 : KAIMING
                              FC2 : KAIMING
                              OUT : XAVIER
            WIDTH           = 20
            DEPTH           = 2
            WEIGHT_DECAY    = 1E-3
            LR              = 1E-4
            UPDATE_FREQ     = 18 MONTHS
            MEMORY          = 24 MONTHS
            ITERATION       = 50
            BATCH_SIZE      = 32
            EPSILON         = 0.9              
            GAMMA           = 0.9                
            LAMBDA          = 0.9
            
TRAINING:   TOKYO[2000-2009]
            BATTERY_RESET    = 0 %
            REWARD_FUNC      = TYPE2
                               violation_penalty = 3
                               battery full = -2
                               battery dead = -4
                               R = r1*(2**r2) - violation_penalty
            REWARD_BROADCAST = TRUE
            
            LAST TRAINING ITERATIONS = 10 [TOKYO, 2000-2009]
            EPSILON                  = 0.98
            LR                       = 1E-5
            
TESTING:    TOKYO[2015-2018]
            GREEDY POLICY
   

RESULTS:
# Some non-robust behavior

SEED 0
TESTING BEST MODEL BASED ON AVERAGE REWARD METRIC
YEAR		AVG_RWD		VIOLATIONS
2015 		 1.25 		 4
2016 		 1.33 		 3
2017 		 1.33 		 7
2018 		 1.39 		 0
TESTING BASED ON VIOLATION COUNTER METRIC
YEAR		AVG_RWD		VIOLATIONS
2015 		 1.25 		 4
2016 		 1.33 		 3
2017 		 1.33 		 7
2018 		 1.39 		 0


SEED 1
TESTING BEST MODEL BASED ON AVERAGE REWARD METRIC
YEAR		AVG_RWD		VIOLATIONS
2015 		 1.33 		 3
2016 		 -1.12 		 102
2017 		 1.43 		 1
2018 		 1.32 		 4
TESTING BASED ON VIOLATION COUNTER METRIC
YEAR		AVG_RWD		VIOLATIONS
2015 		 1.33 		 3
2016 		 -1.12 		 102
2017 		 1.43 		 1
2018 		 1.32 		 4


SEED 2
TESTING BEST MODEL BASED ON AVERAGE REWARD METRIC
YEAR		AVG_RWD		VIOLATIONS
2015 		 1.21 		 14
2016 		 1.29 		 8
2017 		 1.3 		 13
2018 		 1.23 		 13
TESTING BASED ON VIOLATION COUNTER METRIC
YEAR		AVG_RWD		VIOLATIONS
2015 		 1.21 		 14
2016 		 1.29 		 8
2017 		 1.3 		 13
2018 		 1.23 		 13

SEED 3
TESTING BEST MODEL BASED ON AVERAGE REWARD METRIC
YEAR		AVG_RWD		VIOLATIONS
2015 		 1.19 		 2
2016 		 1.25 		 2
2017 		 1.24 		 8
2018 		 1.25 		 3
TESTING BASED ON VIOLATION COUNTER METRIC
YEAR		AVG_RWD		VIOLATIONS
2015 		 1.19 		 2
2016 		 1.25 		 2
2017 		 1.24 		 8
2018 		 1.25 		 3



CONCLUSIONS:
The cause of non-robust behaviro is unknown.

DISCUSSION:
More analysis required
**************************************************************
**************************************************************
C(RELU)
|
|--C1(Huber Loss)--X
|              
|--C2(Leaky RELU)--X
|
|--C3(Xa-Ka-Ka)--X   
|              
|--C4(Ka-Ka-Xa)->C4A(LR=0.0001)->C4B


HYPOTHESIS:
Since LR=0.0001 had some slow learning behavior, check how learning rate to 0.0005 turns out.

MODEL:

C4B : INPUT->FC1->RELU->FC2->RELU->OUT
LOSS : MSE

LEARNING:   INIT_WEIGHT     = FC1 : KAIMING
                              FC2 : KAIMING
                              OUT : XAVIER
            WIDTH           = 20
            DEPTH           = 2
            WEIGHT_DECAY    = 1E-3
            LR              = 0.0005
            UPDATE_FREQ     = 18 MONTHS
            MEMORY          = 24 MONTHS
            ITERATION       = 50
            BATCH_SIZE      = 32
            EPSILON         = 0.9              
            GAMMA           = 0.9                
            LAMBDA          = 0.9
            
TRAINING:   TOKYO[2000-2010]
            BATTERY_RESET    = 0 %
            REWARD_FUNC      = TYPE2
                               violation_penalty = 3
                               battery full = -2
                               battery dead = -4
                               R = r1*(2**r2) - violation_penalty
            REWARD_BROADCAST = TRUE
            
TESTING:    TOKYO[2011-2018]
            GREEDY POLICY
   

RESULTS:
# As good as if not better than C4A.
SEED 0
YEAR		AVG_RWD		VIOLATIONS
2011 		 1.2 		 3
2012 		 1.1 		 4
2013 		 1.19 		 0
2014 		 1.07 		 5
2015 		 1.11 		 4
2016 		 1.22 		 1
2017 		 1.26 		 2
2018 		 1.13 		 3

SEED 1
YEAR		AVG_RWD		VIOLATIONS
2011 		 1.41 		 0
2012 		 1.38 		 0
2013 		 1.4 		 0
2014 		 1.32 		 0
2015 		 1.29 		 0
2016 		 1.34 		 0
2017 		 1.37 		 1
2018 		 1.36 		 0

SEED 2
YEAR		AVG_RWD		VIOLATIONS
2011 		 1.42 		 0
2012 		 1.34 		 0
2013 		 1.33 		 1
2014 		 1.24 		 0
2015 		 1.26 		 0
2016 		 1.34 		 0
2017 		 1.33 		 3
2018 		 1.29 		 0

SEED 3
YEAR		AVG_RWD		VIOLATIONS
2011 		 1.45 		 2
2012 		 1.38 		 1
2013 		 1.45 		 0
2014 		 1.29 		 3
2015 		 1.29 		 2
2016 		 1.37 		 0
2017 		 1.36 		 6
2018 		 1.38 		 0

SEED 4
YEAR		AVG_RWD		VIOLATIONS
2011 		 1.48 		 0
2012 		 1.4 		 0
2013 		 1.43 		 0
2014 		 1.34 		 1
2015 		 1.34 		 0
2016 		 1.38 		 0
2017 		 1.4 		 3
2018 		 1.38 		 0

SEED 5
YEAR		AVG_RWD		VIOLATIONS
2011 		 1.18 		 3
2012 		 1.07 		 7
2013 		 1.12 		 6
2014 		 1.02 		 6
2015 		 1.02 		 5
2016 		 1.13 		 3
2017 		 1.1 		 10
2018 		 1.08 		 5

SEED 6
YEAR		AVG_RWD		VIOLATIONS
2011 		 1.12 		 0
2012 		 1.13 		 0
2013 		 1.13 		 0
2014 		 1.09 		 0
2015 		 1.07 		 0
2016 		 1.09 		 0
2017 		 1.07 		 2
2018 		 1.09 		 0


CONCLUSIONS:
LR=0.0005 seems like a good compromise for LR.

DISCUSSION:

**************************************************************