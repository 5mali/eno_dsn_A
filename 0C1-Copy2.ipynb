{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "from datetime import datetime\n",
    "tic = datetime.now()\n",
    "\n",
    "import os\n",
    "from os.path import dirname, abspath, join\n",
    "from os import getcwd\n",
    "import sys\n",
    "\n",
    "# THIS_DIR = getcwd()\n",
    "# CLASS_DIR = abspath(join(THIS_DIR, 'dsnclasses'))  #abspath(join(THIS_DIR, '../../..', 'dsnclasses'))\n",
    "# sys.path.append(CLASS_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import string\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.ticker as ticker\n",
    "import torch\n",
    "from IPython.display import clear_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 271828\n",
    "random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "torch.cuda.manual_seed(seed)\n",
    "np.random.seed(seed)\n",
    "os.environ['PYTHONHASHSEED'] = str(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ENO(object):\n",
    "    \n",
    "    #no. of forecast types is 6 ranging from 0 to 5\n",
    "  \n",
    "    def __init__(self, location='tokyo', year=2010, shuffle=False, day_balance=False):\n",
    "        self.location = location\n",
    "        self.year = year\n",
    "        self.day = None\n",
    "        self.hr = None\n",
    "        \n",
    "        self.shuffle = shuffle\n",
    "        self.day_balance = day_balance\n",
    "\n",
    "        self.TIME_STEPS = None #no. of time steps in one episode\n",
    "        self.NO_OF_DAYS = None #no. of days in one year\n",
    "        \n",
    "        self.NO_OF_DAYTYPE = 10 #no. of daytypes\n",
    "        self.daycounter = 0 #to count number of days that have been passed\n",
    "        \n",
    "        self.sradiation = None #matrix with GSR for the entire year\n",
    "        self.senergy = None #matrix with harvested energy data for the entire year\n",
    "        self.fforecast = None #array with forecast values for each day\n",
    "        \n",
    "\n",
    "        self.henergy = None #harvested energy variable\n",
    "        self.fcast = None #forecast variable\n",
    "        self.sorted_days = [] #days sorted according to day type\n",
    "        \n",
    "        self.SMAX = 1000 # 1 Watt Solar Panel\n",
    "\n",
    "    \n",
    "    #function to get the solar data for the given location and year and prep it\n",
    "    def get_data(self):\n",
    "        #solar_data/CSV files contain the values of GSR (Global Solar Radiation in MegaJoules per meters squared per hour)\n",
    "        #weather_data/CSV files contain the weather summary from 06:00 to 18:00 and 18:00 to 06:00+1\n",
    "        location = self.location\n",
    "        year = self.year\n",
    "\n",
    "        THIS_DIR = getcwd()\n",
    "        SDATA_DIR = abspath(join(THIS_DIR, 'solar_data'))  #abspath(join(THIS_DIR, '../../..', 'data'))\n",
    "        \n",
    "        sfile = SDATA_DIR + '/' + location +'/' + str(year) + '.csv'\n",
    "        \n",
    "        #skiprows=4 to remove unnecessary title texts\n",
    "        #usecols=4 to read only the Global Solar Radiation (GSR) values\n",
    "        solar_radiation = pd.read_csv(sfile, skiprows=4, encoding='shift_jisx0213', usecols=[4])\n",
    "      \n",
    "        #convert dataframe to numpy array\n",
    "        solar_radiation = solar_radiation.values\n",
    "\n",
    "        #convert missing data in CSV files to zero\n",
    "        solar_radiation[np.isnan(solar_radiation)] = 0\n",
    "\n",
    "        #reshape solar_radiation into no_of_daysx24 array\n",
    "        solar_radiation = solar_radiation.reshape(-1,24)\n",
    "\n",
    "        if(self.shuffle): #if class instatiation calls for shuffling the day order. Required when learning\n",
    "            np.random.shuffle(solar_radiation) \n",
    "        self.sradiation = solar_radiation\n",
    "        \n",
    "        #GSR values (in MJ/sq.mts per hour) need to be expressed in mW\n",
    "        # Conversion is accomplished by \n",
    "        # solar_energy = GSR(in MJ/m2/hr) * 1e6 * size of solar cell * efficiency of solar cell /(60x60) *1000 (to express in mW)\n",
    "        # the factor of 2 in the end is assuming two solar cells\n",
    "        self.senergy = 2*self.sradiation * 1e6 * (55e-3 * 70e-3) * 0.15 * 1000/(60*60)\n",
    "\n",
    "        return 0\n",
    "    \n",
    "    #function to map total day radiation into type of day ranging from 0 to 5\n",
    "    #the classification into day types is quite arbitrary. There is no solid logic behind this type of classification.\n",
    "    \n",
    "    def get_day_state(self,tot_day_radiation):\n",
    "        bin_edges = np.array([0, 3.5, 6.5, 9.0, 12.5, 15.5, 18.5, 22.0, 25, 28])\n",
    "        for k in np.arange(1,bin_edges.size):\n",
    "            if (bin_edges[k-1] < tot_day_radiation <= bin_edges[k]):\n",
    "                day_state = k -1\n",
    "            else:\n",
    "                day_state = bin_edges.size - 1\n",
    "        return int(day_state)\n",
    "    \n",
    "    def get_forecast(self):\n",
    "        #create a perfect forecaster.\n",
    "        tot_day_radiation = np.sum(self.sradiation, axis=1) #contains total solar radiation for each day\n",
    "        get_day_state = np.vectorize(self.get_day_state)\n",
    "        self.fforecast = get_day_state(tot_day_radiation)\n",
    "        \n",
    "        #sort days depending on the type of day and shuffle them; maybe required when learning\n",
    "        for fcast in range(0,6):\n",
    "            fcast_days = ([i for i,x in enumerate(self.fforecast) if x == fcast])\n",
    "            np.random.shuffle(fcast_days)\n",
    "            self.sorted_days.append(fcast_days)\n",
    "        return 0\n",
    "    \n",
    "    def reset(self,day=0): #it is possible to reset to the beginning of a certain day\n",
    "        \n",
    "        self.get_data() #first get data for the given year\n",
    "        self.get_forecast() #calculate the forecast\n",
    "        \n",
    "        self.TIME_STEPS = self.senergy.shape[1]\n",
    "        self.NO_OF_DAYS = self.senergy.shape[0]\n",
    "        \n",
    "        self.day = day\n",
    "        self.hr = 0\n",
    "        \n",
    "        self.henergy = self.senergy[self.day][self.hr]\n",
    "        self.fcast = self.fforecast[self.day]\n",
    "        \n",
    "        end_of_day = False\n",
    "        end_of_year = False\n",
    "        return [self.henergy, self.fcast, end_of_day, end_of_year]\n",
    "\n",
    "    \n",
    "    def step(self):\n",
    "        end_of_day = False\n",
    "        end_of_year = False\n",
    "        if not(self.day_balance): #if daytype balance is not required\n",
    "            if(self.hr < self.TIME_STEPS - 1):\n",
    "                self.hr += 1\n",
    "                self.henergy = self.senergy[self.day][self.hr] \n",
    "                self.fcast = self.fforecast[self.day]\n",
    "            else:\n",
    "                if(self.day < self.NO_OF_DAYS -1):\n",
    "                    end_of_day = True\n",
    "                    self.hr = 0\n",
    "                    self.day += 1\n",
    "                    self.henergy = self.senergy[self.day][self.hr] \n",
    "                    self.fcast = self.fforecast[self.day]\n",
    "                else:\n",
    "                    end_of_day = True\n",
    "                    end_of_year = True\n",
    "                    \n",
    "        else: #when training, we want all daytypes to be equally represented for robust policy\n",
    "              #obviously, the days are going to be in random order\n",
    "            if(self.hr < self.TIME_STEPS - 1):\n",
    "                self.hr += 1\n",
    "                self.henergy = self.senergy[self.day][self.hr]\n",
    "                self.fcast = self.fforecast[self.day]\n",
    "            else:\n",
    "                if(self.daycounter < self.NO_OF_DAYS -1):\n",
    "                    end_of_day = True\n",
    "                    self.daycounter += 1\n",
    "                    self.hr = 0\n",
    "                    daytype = random.choice(np.arange(0,self.NO_OF_DAYTYPE)) #choose random daytype\n",
    "                    self.day = np.random.choice(self.sorted_days[daytype]) #choose random day from that daytype\n",
    "                    self.henergy = self.senergy[self.day][self.hr] \n",
    "                    self.fcast = self.fforecast[self.day]\n",
    "                else: \n",
    "                    end_of_day = True\n",
    "                    end_of_year = True\n",
    "                    self.daycounter = 0\n",
    "        \n",
    "        \n",
    "        return [self.henergy, self.fcast, end_of_day, end_of_year]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CAPM (object):\n",
    "    def __init__(self,location='tokyo', year=2010, shuffle=False, trainmode=False):\n",
    "\n",
    "        #all energy values i.e. BMIN, BMAX, BOPT, HMAX are in mWhr. Assuming one timestep is one hour\n",
    "        \n",
    "        self.BMIN = 0.0                #Minimum battery level that is tolerated. Maybe non-zero also\n",
    "        self.BMAX = 9250.0            #Max Battery Level. May not necessarily be equal to total batter capacity [3.6V x 2500mAh]\n",
    "        self.BOPT = 0.5 * self.BMAX    #Optimal Battery Level. Assuming 50% of battery is the optimum\n",
    "        \n",
    "        self.HMIN = 0      #Minimum energy that can be harvested by the solar panel.\n",
    "        self.HMAX = None   #Maximum energy that can be harvested by the solar panel. [500mW]\n",
    "        \n",
    "        self.DMAX = 500      #Maximum energy that can be consumed by the node in one time step. [~ 3.6V x 135mA]\n",
    "        self.N_ACTIONS = 10  #No. of different duty cycles possible\n",
    "        self.DMIN = self.DMAX/self.N_ACTIONS #Minimum energy that can be consumed by the node in one time step. [~ 3.6V x 15mA]\n",
    "        \n",
    "        self.binit = None     #battery at the beginning of day\n",
    "        self.btrack = []      #track the mean battery level for each day\n",
    "        self.atrack = []      #track the duty cycles for each day\n",
    "        self.batt = None      #battery variable\n",
    "        self.enp = None       #enp at end of hr\n",
    "        self.henergy = None   #harvested energy variable\n",
    "        self.fcast = None     #forecast variable\n",
    "        \n",
    "        self.MUBATT = 0.6\n",
    "        self.SDBATT = 0.02\n",
    "        \n",
    "        self.MUHENERGY = 0.5\n",
    "        self.SDHENERGY = 0.2\n",
    "        \n",
    "        self.MUENP = 0\n",
    "        self.SDENP = 0.02\n",
    "        \n",
    "        self.location = location\n",
    "        self.year = year\n",
    "        self.shuffle = shuffle\n",
    "        self.trainmode = trainmode\n",
    "        self.eno = None#ENO(self.location, self.year, shuffle=shuffle, day_balance=trainmode) #if trainmode is enable, then days are automatically balanced according to daytype i.e. day_balance= True\n",
    "        \n",
    "        self.day_violation_flag = False\n",
    "        self.violation_flag = False\n",
    "        self.violation_counter = 0\n",
    "\n",
    "        self.NO_OF_DAYTYPE = 10 #no. of daytypes\n",
    " \n",
    "    def reset(self,day=0,batt=-1):\n",
    "        henergy, fcast, day_end, year_end = self.eno.reset(day) #reset the eno environment\n",
    "        self.violation_flag = False\n",
    "        self.violation_counter = 0\n",
    "        if(batt == -1):\n",
    "            self.batt = self.BOPT\n",
    "        else:\n",
    "            self.batt = batt\n",
    "            \n",
    "        self.batt = np.clip(self.batt, self.BMIN, self.BMAX)\n",
    "        self.binit = self.batt\n",
    "        self.btrack = np.append(self.btrack, self.batt) #track battery levels\n",
    "\n",
    "#         self.enp = self.BOPT - self.batt\n",
    "        self.enp = self.binit - self.batt #enp is calculated\n",
    "        self.henergy = np.clip(henergy, self.HMIN, self.HMAX) #clip henergy within HMIN and HMAX\n",
    "        self.fcast = fcast\n",
    "        \n",
    "        norm_batt = self.batt/self.BMAX\n",
    "        norm_enp = self.enp/(self.BMAX/2)\n",
    "        norm_henergy = self.henergy/self.HMAX\n",
    "        norm_fcast = self.fcast/(self.NO_OF_DAYTYPE-1)\n",
    "\n",
    "        c_state = [norm_batt, norm_enp, norm_henergy, norm_fcast] #continuous states\n",
    "        reward = 0\n",
    "        \n",
    "        return [c_state, reward, day_end, year_end]\n",
    "    \n",
    "    def getstate(self): #query the present state of the system\n",
    "        norm_batt = self.batt/self.BMAX - self.MUBATT\n",
    "        norm_enp = self.enp/(self.BMAX/2)\n",
    "        norm_henergy = self.henergy/self.HMAX\n",
    "        norm_fcast = self.fcast/(self.NO_OF_DAYTYPE-1)        \n",
    "        c_state = [norm_batt, norm_enp, norm_henergy] #continuous states\n",
    "\n",
    "        return c_state\n",
    "    \n",
    "#     def rewardfn(self):\n",
    "#         R_PARAM = 20000 #chosen empirically for best results\n",
    "#         mu = 0\n",
    "#         sig = 0.07*R_PARAM #knee curve starts at approx. 2000mWhr of deviation\n",
    "#         norm_reward = 3*(np.exp(-np.power((self.enp - mu)/sig, 2.)/2) / np.exp(-np.power((0 - mu)/sig, 2.)/2))-1\n",
    "\n",
    "        \n",
    "# #         if(np.abs(self.enp) <= 0.12*R_PARAM):\n",
    "# #             norm_reward = 2*(np.exp(-np.power((self.enp - mu)/sig, 2.)/2) / np.exp(-np.power((0 - mu)/sig, 2.)/2))\n",
    "# #         else:\n",
    "# #             norm_reward = -0.25 - 10*np.abs(self.enp/R_PARAM)\n",
    "#         if(self.day_violation_flag):\n",
    "#             norm_reward -= 3\n",
    "            \n",
    "#         return (norm_reward)\n",
    "        \n",
    "    \n",
    "    #reward function\n",
    "    def rewardfn(self):\n",
    "        \n",
    "        #FIRST REWARD AS A FUNCTION OF DRIFT OF BMEAN FROM BOPT i.e. in terms of BDEV = |BMEAN-BOPT|/BMAX\n",
    "        bmean = np.mean(self.btrack)\n",
    "        bdev = np.abs(self.BOPT - bmean)/self.BMAX\n",
    "        # based on the sigmoid function\n",
    "        # bdev ranges from bdev = (0,0.5) of BMAX\n",
    "        p1_sharpness = 10\n",
    "        n1_sharpness = 20\n",
    "        shift1 = 0.5\n",
    "        # r1(x) = 0.5 when x = 0.25. \n",
    "        # Therefore, shift = 0.5 to make sure that (2*x-shift) evaluates to zero at x = 0.25\n",
    "\n",
    "        if(bdev<=0.25): \n",
    "            r1 = 2*(1-(1 / (1 + np.exp(-p1_sharpness*(2*bdev-shift1)))))-1\n",
    "        else: \n",
    "            r1 = 2*(1-(1 / (1 + np.exp(-n1_sharpness*(2*bdev-shift1)))))-1\n",
    "        # r1 ranges from -1 to 1\n",
    "            \n",
    "        #SECOND REWARD AS A FUNCTION OF ENP AS LONG AS BMAX/4 <= batt <= 3*BMAX/4 i.e. bdev <= 0.25\n",
    "        if(bdev <=0.25):\n",
    "            # enp ranges from enp = (0,3) of DMAX\n",
    "            p2_sharpness = 2\n",
    "            n2_sharpness = 2\n",
    "            shift2 = 6    \n",
    "            # r1(x) = 0.5 when x = 2. \n",
    "            # Therefore, shift = 6 to make sure that (3*x-shift) evaluates to zero at x = 2\n",
    "#             print('Day energy', np.sum(self.eno.senergy[self.eno.day]))\n",
    "#             print('Node energy', np.sum(self.atrack)*self.DMAX/self.N_ACTIONS)\n",
    "#             x = np.abs(np.sum(self.eno.senergy[self.eno.day])-np.sum(self.atrack)*self.DMAX/self.N_ACTIONS )/self.DMAX\n",
    "            x = np.abs(self.enp/self.DMAX)\n",
    "            if(x<=2): \n",
    "                r2 = (1 / (1 + np.exp(p2_sharpness*(3*x-shift2))))\n",
    "            else: \n",
    "                r2 = (1 / (1 + np.exp(n2_sharpness*(3*x-shift2))))\n",
    "        else:\n",
    "            r2 = 0 # if mean battery lies outside bdev limits, then enp reward is not considered.\n",
    "        # r2 ranges from 0 to 1\n",
    "\n",
    "        #REWARD AS A FUNCTION OF BATTERY VIOLATIONS\n",
    "        if(self.day_violation_flag):\n",
    "            violation_penalty = 3\n",
    "        else:\n",
    "            violation_penalty = 0 #penalty for violating battery limits anytime during the day\n",
    "        \n",
    "#         print(\"Reward \", (r1 + r2 - violation_penalty), '\\n')\n",
    "        return (r1*(2**r2) - violation_penalty)\n",
    "    \n",
    "    def step(self, action):\n",
    "        day_end = False\n",
    "        year_end = False\n",
    "        self.violation_flag = False\n",
    "        reward = 0\n",
    "       \n",
    "        action = np.clip(action, 0, self.N_ACTIONS-1) #action values range from (0 to N_ACTIONS-1)\n",
    "        self.atrack = np.append(self.atrack, action+1) #track duty cycles\n",
    "        e_consumed = (action+1)*self.DMAX/self.N_ACTIONS   #energy consumed by the node\n",
    "        \n",
    "        self.batt += (self.henergy - e_consumed)\n",
    "        if(self.batt < 0.02*self.BMAX or self.batt > 0.98*self.BMAX ):\n",
    "            self.violation_flag = True #penalty for violating battery limits everytime it happens\n",
    "            reward = -2\n",
    "        if(self.batt < 0.02*self.BMAX):\n",
    "            reward -= 2\n",
    "            \n",
    "        if(self.violation_flag):\n",
    "            if(self.day_violation_flag == False): #penalty for violating battery limits anytime during the day - triggers once everyday\n",
    "                self.violation_counter += 1\n",
    "                self.day_violation_flag = True\n",
    "            \n",
    "        self.batt = np.clip(self.batt, self.BMIN, self.BMAX) #clip battery values within permitted level\n",
    "        self.btrack = np.append(self.btrack, self.batt) #track battery levels\n",
    "\n",
    "#         self.enp = self.BOPT - self.batt \n",
    "        self.enp = self.binit - self.atrack.sum()*self.DMAX/self.N_ACTIONS\n",
    "        \n",
    "        #proceed to the next time step\n",
    "        self.henergy, self.fcast, day_end, year_end = self.eno.step()\n",
    "        self.henergy = np.clip(self.henergy, self.HMIN, self.HMAX) #clip henergy within HMIN and HMAX\n",
    "                \n",
    "        if(day_end): #if eno object flags that the day has ended then give reward\n",
    "            reward += self.rewardfn()\n",
    "             \n",
    "            if (self.trainmode): #reset battery to optimal level if limits are exceeded when training\n",
    "#                 self.batt = np.random.uniform(self.DMAX*self.eno.TIME_STEPS/self.BMAX,0.8)*self.BMAX\n",
    "#                 if (self.violation_flag):\n",
    "                if np.random.uniform() < HELP : #occasionaly reset the battery\n",
    "                    self.batt = self.BOPT  \n",
    "            \n",
    "            self.day_violation_flag = False\n",
    "            self.binit = self.batt #this will be the new initial battery level for next day\n",
    "            self.btrack = [] #clear battery tracker\n",
    "            self.atrack = [] #clear duty cycle tracker\n",
    "            \n",
    "                    \n",
    "                \n",
    "        norm_batt = self.batt/self.BMAX\n",
    "        norm_enp = self.enp/(self.BMAX/2)\n",
    "        norm_henergy = self.henergy/self.HMAX\n",
    "        norm_fcast = self.fcast/(self.NO_OF_DAYTYPE-1)\n",
    "\n",
    "        c_state = [norm_batt, norm_enp, norm_henergy, norm_fcast] #continuous states\n",
    "        return [c_state, reward, day_end, year_end]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyper Parameters\n",
    "BATCH_SIZE = 32\n",
    "LR = 0.0001          # learning rate\n",
    "EPSILON = 0.9               # greedy policy\n",
    "GAMMA = 0.9                 # reward discount\n",
    "LAMBDA = 0.9                # parameter decay\n",
    "TARGET_REPLACE_ITER = 24*7*4*18    # target update frequency (every two months)\n",
    "MEMORY_CAPACITY     = 24*7*4*12*2      # store upto six month worth of memory   \n",
    "\n",
    "N_ACTIONS = 10 #no. of duty cycles (0,1,2,3,4)\n",
    "N_STATES = 4 #number of state space parameter [batt, enp, henergy, fcast]\n",
    "\n",
    "HIDDEN_LAYER = 50\n",
    "NO_OF_ITERATIONS = 50\n",
    "GPU = False\n",
    "HELP = 0.05"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "\n",
    "#Class definitions for NN model and learning algorithm\n",
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "\n",
    "        self.fc1 = nn.Linear(N_STATES, HIDDEN_LAYER)\n",
    "        nn.init.kaiming_uniform_(self.fc1.weight)\n",
    "        \n",
    "        self.fc2 = nn.Linear(HIDDEN_LAYER, HIDDEN_LAYER)\n",
    "        nn.init.kaiming_uniform_(self.fc2.weight)\n",
    "\n",
    "        self.out = nn.Linear(HIDDEN_LAYER, N_ACTIONS)\n",
    "        nn.init.xavier_uniform_(self.out.weight) \n",
    "\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.fc1(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.fc2(x)\n",
    "        x = F.relu(x)\n",
    "        actions_value = self.out(x)\n",
    "        return actions_value\n",
    "    \n",
    "class DQN(object):\n",
    "    def __init__(self):\n",
    "        if(GPU): \n",
    "            device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "        else:\n",
    "            device = torch.device('cpu')\n",
    "        self.eval_net, self.target_net = Net(), Net()\n",
    "        self.eval_net.to(device)\n",
    "        self.target_net.to(device)\n",
    "        self.device = device\n",
    "#         print(\"Neural net\")\n",
    "#         print(self.eval_net)\n",
    "        self.learn_step_counter = 0                                     # for target updating\n",
    "        self.memory_counter = 0                                         # for storing memory\n",
    "        self.memory = np.zeros((MEMORY_CAPACITY, N_STATES * 2 + 2))     # initialize memory [mem: ([s], a, r, [s_]) ]\n",
    "        self.optimizer = torch.optim.Adam(self.eval_net.parameters(), lr=LR, weight_decay=1e-3)\n",
    "        self.loss_func = nn.MSELoss()\n",
    "        self.nettoggle = False\n",
    "\n",
    "    def choose_action(self, x):\n",
    "        x = torch.unsqueeze(torch.FloatTensor(x), 0)\n",
    "        x = x.to(self.device)\n",
    "\n",
    "        # input only one sample\n",
    "        if np.random.uniform() < EPSILON:   # greedy\n",
    "            actions_value = self.eval_net.forward(x)\n",
    "            actions_value = actions_value.to(torch.device(\"cpu\"))\n",
    "            action = torch.max(actions_value, 1)[1].data.numpy()\n",
    "            action = action[0] # return the argmax index\n",
    "        else:   # random\n",
    "            action = np.random.randint(0, N_ACTIONS)\n",
    "            action = action\n",
    "        return action\n",
    "    \n",
    "    def choose_greedy_action(self, x):\n",
    "        x = torch.unsqueeze(torch.FloatTensor(x), 0)\n",
    "        x = x.to(self.device)\n",
    "\n",
    "        # input only one sample\n",
    "        if True:   # greedy\n",
    "            actions_value = self.eval_net.forward(x)\n",
    "            actions_value = actions_value.to(torch.device(\"cpu\"))\n",
    "            action = torch.max(actions_value, 1)[1].data.numpy()\n",
    "            action = action[0] # return the argmax index\n",
    "        return action\n",
    "\n",
    "    def store_transition(self, s, a, r, s_):\n",
    "        transition = np.hstack((s, [a, r], s_))\n",
    "        # replace the old memory with new memory\n",
    "        index = self.memory_counter % MEMORY_CAPACITY\n",
    "        self.memory[index, :] = transition\n",
    "        self.memory_counter += 1\n",
    "    \n",
    "    def store_day_transition(self, transition_rec):\n",
    "        data = transition_rec\n",
    "        index = self.memory_counter % MEMORY_CAPACITY\n",
    "        self.memory= np.insert(self.memory, index, data,0)\n",
    "        self.memory_counter += transition_rec.shape[0]\n",
    "\n",
    "    def learn(self):\n",
    "        # target parameter update\n",
    "        if self.learn_step_counter % TARGET_REPLACE_ITER == 0:\n",
    "            self.target_net.load_state_dict(self.eval_net.state_dict())\n",
    "            self.nettoggle = not self.nettoggle\n",
    "        self.learn_step_counter += 1\n",
    "\n",
    "        # sample batch transitions\n",
    "        sample_index = np.random.choice(MEMORY_CAPACITY, BATCH_SIZE)\n",
    "        b_memory = self.memory[sample_index, :]\n",
    "        b_s = torch.FloatTensor(b_memory[:, :N_STATES])\n",
    "        b_a = torch.LongTensor(b_memory[:, N_STATES:N_STATES+1].astype(int))\n",
    "        b_r = torch.FloatTensor(b_memory[:, N_STATES+1:N_STATES+2])\n",
    "        b_s_ = torch.FloatTensor(b_memory[:, -N_STATES:])\n",
    "        \n",
    "        b_s = b_s.to(self.device)\n",
    "        b_a = b_a.to(self.device)\n",
    "        b_r = b_r.to(self.device)\n",
    "        b_s_ = b_s_.to(self.device)\n",
    "\n",
    "        # q_eval w.r.t the action in experience\n",
    "        q_eval = self.eval_net(b_s).gather(1, b_a)  # shape (batch, 1)\n",
    "        q_next = self.target_net(b_s_).detach()     # detach from graph, don't backpropagate\n",
    "        q_target = b_r + GAMMA * q_next.max(1)[0].view(BATCH_SIZE, 1)   # shape (batch, 1)\n",
    "        loss = self.loss_func(q_eval, q_target)\n",
    "\n",
    "        self.optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        self.optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def stdize(s):\n",
    "    MU_BATT = 0.5\n",
    "    SD_BATT = 0.15\n",
    "    \n",
    "    MU_ENP = 0\n",
    "    SD_ENP = 0.15\n",
    "    \n",
    "    MU_HENERGY = 0.35\n",
    "    SD_HENERGY = 0.25\n",
    "    \n",
    "    MU_FCAST = 0.42\n",
    "    SD_FCAST = 0.27\n",
    "    \n",
    "    norm_batt, norm_enp, norm_henergy, norm_fcast = s\n",
    "    \n",
    "    std_batt = (norm_batt - MU_BATT)/SD_BATT\n",
    "    std_enp = (norm_enp - MU_ENP)/SD_ENP\n",
    "    std_henergy = (norm_henergy - MU_HENERGY)/SD_HENERGY\n",
    "    std_fcast = (norm_fcast - MU_FCAST)/SD_FCAST\n",
    "\n",
    "\n",
    "    return [std_batt, std_enp, std_henergy, std_fcast]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "TRAINING IN PROGRESS\n",
      "\n",
      "Device:  cpu\n",
      "\n",
      "Iteration 0:  TOKYO, 2002 \n",
      "Average Reward \t\t= -3.367\n",
      "Violation Counter \t= 289\n",
      "\n",
      "Iteration 1:  TOKYO, 2001 \n",
      "Average Reward \t\t= -3.617\n",
      "Violation Counter \t= 297\n",
      "\n",
      "Iteration 2:  TOKYO, 2006 \n",
      "Average Reward \t\t= -3.854\n",
      "Violation Counter \t= 240\n",
      "\n",
      "Iteration 3:  TOKYO, 2000 \n",
      "Average Reward \t\t= -4.311\n",
      "Violation Counter \t= 268\n",
      "\n",
      "Iteration 4:  TOKYO, 2002 \n",
      "Average Reward \t\t= -3.608\n",
      "Violation Counter \t= 243\n",
      "\n",
      "Iteration 5:  TOKYO, 2003 \n",
      "Average Reward \t\t= -1.916\n",
      "Violation Counter \t= 143\n",
      "\n",
      "Iteration 6:  TOKYO, 2000 \n",
      "Average Reward \t\t= -0.633\n",
      "Violation Counter \t= 80\n",
      "\n",
      "Iteration 7:  TOKYO, 2002 \n",
      "Average Reward \t\t= -0.232\n",
      "Violation Counter \t= 68\n",
      "\n",
      "Iteration 8:  TOKYO, 2000 \n",
      "Average Reward \t\t= 0.233\n",
      "Violation Counter \t= 47\n",
      "\n",
      "Iteration 9:  TOKYO, 2008 \n",
      "Average Reward \t\t= 0.914\n",
      "Violation Counter \t= 20\n",
      "\n",
      "Iteration 10:  TOKYO, 2003 \n",
      "Average Reward \t\t= 0.920\n",
      "Violation Counter \t= 16\n",
      "\n",
      "Iteration 11:  TOKYO, 2004 \n",
      "Average Reward \t\t= 0.658\n",
      "Violation Counter \t= 32\n",
      "\n",
      "Iteration 12:  TOKYO, 2009 \n",
      "Average Reward \t\t= 0.972\n",
      "Violation Counter \t= 15\n",
      "\n",
      "Iteration 13:  TOKYO, 2006 \n",
      "Average Reward \t\t= 0.705\n",
      "Violation Counter \t= 29\n",
      "\n",
      "Iteration 14:  TOKYO, 2002 \n",
      "Average Reward \t\t= 0.694\n",
      "Violation Counter \t= 27\n",
      "\n",
      "Iteration 15:  TOKYO, 2009 \n",
      "Average Reward \t\t= 0.828\n",
      "Violation Counter \t= 22\n",
      "\n",
      "Iteration 16:  TOKYO, 2005 \n",
      "Average Reward \t\t= 0.940\n",
      "Violation Counter \t= 23\n",
      "\n",
      "Iteration 17:  TOKYO, 2004 \n",
      "Average Reward \t\t= 0.642\n",
      "Violation Counter \t= 31\n",
      "\n",
      "Iteration 18:  TOKYO, 2001 \n",
      "Average Reward \t\t= 0.882\n",
      "Violation Counter \t= 24\n",
      "\n",
      "Iteration 19:  TOKYO, 2007 \n",
      "Average Reward \t\t= 0.632\n",
      "Violation Counter \t= 33\n",
      "\n",
      "Iteration 20:  TOKYO, 2009 \n",
      "Average Reward \t\t= 0.555\n",
      "Violation Counter \t= 33\n",
      "\n",
      "Iteration 21:  TOKYO, 2002 \n",
      "Average Reward \t\t= 0.554\n",
      "Violation Counter \t= 33\n",
      "\n",
      "Iteration 22:  TOKYO, 2007 \n",
      "Average Reward \t\t= 0.702\n",
      "Violation Counter \t= 26\n",
      "\n",
      "Iteration 23:  TOKYO, 2005 \n",
      "Average Reward \t\t= 0.843\n",
      "Violation Counter \t= 27\n",
      "\n",
      "Iteration 24:  TOKYO, 2005 \n",
      "Average Reward \t\t= 1.010\n",
      "Violation Counter \t= 19\n",
      "\n",
      "Iteration 25:  TOKYO, 2004 \n",
      "Average Reward \t\t= 0.902\n",
      "Violation Counter \t= 21\n",
      "\n",
      "Iteration 26:  TOKYO, 2003 \n",
      "Average Reward \t\t= 0.617\n",
      "Violation Counter \t= 31\n",
      "\n",
      "Iteration 27:  TOKYO, 2002 \n",
      "Average Reward \t\t= 0.536\n",
      "Violation Counter \t= 31\n",
      "\n",
      "Iteration 28:  TOKYO, 2009 \n",
      "Average Reward \t\t= 0.830\n",
      "Violation Counter \t= 21\n",
      "\n",
      "Iteration 29:  TOKYO, 2000 \n",
      "Average Reward \t\t= 0.605\n",
      "Violation Counter \t= 28\n",
      "\n",
      "Iteration 30:  TOKYO, 2006 \n",
      "Average Reward \t\t= 0.415\n",
      "Violation Counter \t= 41\n",
      "\n",
      "Iteration 31:  TOKYO, 2008 \n",
      "Average Reward \t\t= 0.740\n",
      "Violation Counter \t= 26\n",
      "\n",
      "Iteration 32:  TOKYO, 2001 \n",
      "Average Reward \t\t= 0.894\n",
      "Violation Counter \t= 17\n",
      "\n",
      "Iteration 33:  TOKYO, 2004 \n",
      "Average Reward \t\t= 0.914\n",
      "Violation Counter \t= 16\n",
      "\n",
      "Iteration 34:  TOKYO, 2009 \n",
      "Average Reward \t\t= 0.783\n",
      "Violation Counter \t= 21\n",
      "\n",
      "Iteration 35:  TOKYO, 2009 \n",
      "Average Reward \t\t= 0.831\n",
      "Violation Counter \t= 21\n",
      "\n",
      "Iteration 36:  TOKYO, 2001 \n",
      "Average Reward \t\t= 0.949\n",
      "Violation Counter \t= 15\n",
      "\n",
      "Iteration 37:  TOKYO, 2003 \n",
      "Average Reward \t\t= 0.798\n",
      "Violation Counter \t= 22\n",
      "\n",
      "Iteration 38:  TOKYO, 2008 \n",
      "Average Reward \t\t= 0.973\n",
      "Violation Counter \t= 14\n",
      "\n",
      "Iteration 39:  TOKYO, 2009 \n",
      "Average Reward \t\t= 0.924\n",
      "Violation Counter \t= 15\n",
      "\n",
      "Iteration 40:  TOKYO, 2004 \n",
      "Average Reward \t\t= 0.776\n",
      "Violation Counter \t= 26\n",
      "\n",
      "Iteration 41:  TOKYO, 2005 \n",
      "Average Reward \t\t= 1.095\n",
      "Violation Counter \t= 11\n",
      "\n",
      "Iteration 42:  TOKYO, 2009 \n",
      "Average Reward \t\t= 0.854\n",
      "Violation Counter \t= 21\n",
      "\n",
      "Iteration 43:  TOKYO, 2003 \n",
      "Average Reward \t\t= 0.852\n",
      "Violation Counter \t= 18\n",
      "\n",
      "Iteration 44:  TOKYO, 2000 \n",
      "Average Reward \t\t= 0.910\n",
      "Violation Counter \t= 15\n",
      "\n",
      "Iteration 45:  TOKYO, 2000 \n",
      "Average Reward \t\t= 0.951\n",
      "Violation Counter \t= 11\n",
      "\n",
      "Iteration 46:  TOKYO, 2005 \n",
      "Average Reward \t\t= 1.061\n",
      "Violation Counter \t= 14\n",
      "\n",
      "Iteration 47:  TOKYO, 2003 \n",
      "Average Reward \t\t= 0.805\n",
      "Violation Counter \t= 16\n",
      "\n",
      "Iteration 48:  TOKYO, 2006 \n",
      "Average Reward \t\t= 0.469\n",
      "Violation Counter \t= 35\n",
      "\n",
      "Iteration 49:  TOKYO, 2006 \n",
      "Average Reward \t\t= 0.721\n",
      "Violation Counter \t= 25\n"
     ]
    }
   ],
   "source": [
    "#TRAIN \n",
    "dqn = DQN()\n",
    "# for recording weights\n",
    "oldfc1 = dqn.eval_net.fc1.weight.data.cpu().numpy().flatten()\n",
    "old2fc1 = oldfc1\n",
    "\n",
    "oldfc2 = dqn.eval_net.fc2.weight.data.cpu().numpy().flatten()\n",
    "old2fc2 = oldfc2\n",
    "\n",
    "# oldfc3 = dqn.eval_net.fc3.weight.data.cpu().numpy().flatten()\n",
    "# old2fc3 = oldfc3\n",
    "\n",
    "oldout = dqn.eval_net.out.weight.data.cpu().numpy().flatten()\n",
    "old2out = oldout\n",
    "########################################\n",
    "\n",
    "best_iteration = -1\n",
    "best_avg_reward = -1000 #initialize best average reward to very low value\n",
    "reset_counter = 0 #count number of times the battery had to be reset\n",
    "change_hr = 0\n",
    "# PFILENAME = ''.join(random.choice(string.ascii_uppercase + string.digits) for _ in range(8)) #create random filename\n",
    "# BFILENAME = \"best\"+PFILENAME + \".pt\" #this file stores the best model\n",
    "# TFILENAME = \"terminal\"+PFILENAME + \".pt\" #this file stores the last model\n",
    "\n",
    "avg_reward_rec = [] #record the yearly average rewards over the entire duration of training\n",
    "violation_rec = []\n",
    "print('\\nTRAINING IN PROGRESS\\n')\n",
    "print('Device: ', dqn.device)\n",
    "\n",
    "for iteration in range(NO_OF_ITERATIONS):\n",
    "    LOCATION = 'tokyo'#random.choice(['tokyo','wakkanai','minamidaito'])\n",
    "    YEAR = random.choice(np.arange(2000,2010))\n",
    "    capm = CAPM(LOCATION,YEAR,shuffle=False, trainmode=False) #instantiate the CAPM class\n",
    "    capm.eno = ENO(LOCATION,YEAR, shuffle=False, day_balance=False) #instantiate the environment inside the CAPM class\n",
    "    capm.HMAX = capm.eno.SMAX #maximum power output of solar cell is set in CAPM object using the value in ENO object\n",
    "#     clear_output()\n",
    "    print('\\nIteration {}:  {}, {} '.format(iteration, LOCATION.upper(), YEAR))\n",
    "\n",
    "    s, r, day_end, year_end = capm.reset()\n",
    "    yr_record = np.empty(4)\n",
    "\n",
    "    record = np.empty(4) #record for battery, henergy, reward and action\n",
    "    transition_rec = np.zeros((capm.eno.TIME_STEPS, N_STATES * 2 + 2)) #record all the transition in one day\n",
    "\n",
    "    while True:\n",
    "        a = dqn.choose_action(stdize(s))\n",
    "\n",
    "        # present state = [batt, enp, henergy]\n",
    "        record = np.vstack((record, [s[0],s[2],r, a])) # record battery, henergy, reward and action for troubleshooting\n",
    "        yr_record = np.vstack((yr_record, [s[0],s[2],r, a]))\n",
    "\n",
    "        # take action\n",
    "        s_, r, day_end, year_end = capm.step(a)\n",
    "        \n",
    "        temp_transitions = np.hstack((stdize(s), [a, r], stdize(s_)))\n",
    "        transition_rec[capm.eno.hr-1,:] = temp_transitions\n",
    "\n",
    "        if (day_end):\n",
    "            transition_rec[:,5] += r #broadcast reward to all states\n",
    "            decay_factor = [i for i in (LAMBDA**n for n in reversed(range(0, capm.eno.TIME_STEPS)))]\n",
    "            transition_rec[:,5] = transition_rec[:,5] * decay_factor #decay reward proportionately\n",
    "            dqn.store_day_transition(transition_rec)\n",
    "\n",
    "        if dqn.memory_counter > MEMORY_CAPACITY:\n",
    "            dqn.learn()\n",
    "\n",
    "        if dqn.nettoggle:\n",
    "            change_hr = capm.eno.day*24+capm.eno.hr #to mark when the DQN is updated.\n",
    "            dqn.nettoggle = not dqn.nettoggle\n",
    "\n",
    "        if (year_end):\n",
    "            break\n",
    "\n",
    "        # transition to new state\n",
    "        s = s_\n",
    "\n",
    "    record = np.delete(record, 0, 0) #remove the first row which is garbage\n",
    "    reward_rec = record[:,2] #extract reward information from the record array\n",
    "    reward_rec = reward_rec[::24] #only consider terminal rewards\n",
    "    print(\"Average Reward \\t\\t= {:.3f}\".format(np.mean(reward_rec)))\n",
    "    print(\"Violation Counter \\t= {}\".format(capm.violation_counter))\n",
    "\n",
    "#     if(best_avg_reward < np.mean(reward_rec)):\n",
    "#         best_avg_reward = np.mean(reward_rec)\n",
    "    \n",
    "#     if(best_avg_reward > 1.5 or iteration > 20):\n",
    "#         EPSILON = 0.9\n",
    "#         LR = 0.01\n",
    "        \n",
    "#     if (capm.violation_counter < 5):\n",
    "#         reset_flag = False\n",
    "#         EPSILON = 0.95\n",
    "#         LR = 0.001\n",
    "        \n",
    "\n",
    "#     # Check if reward beats the High Score and possible save it    \n",
    "#     if (iteration > 19): #save the best models only after 20 iterations\n",
    "#         print(\"Best Score \\t = {:8.3f} @ Iteration No. {}\".format(best_avg_reward, best_iteration))\n",
    "#         if(best_avg_reward < np.mean(reward_rec)):\n",
    "#             best_iteration = iteration\n",
    "#             best_avg_reward = np.mean(reward_rec)\n",
    "#             print(\"Saving Model\")\n",
    "#             torch.save(dqn.eval_net.state_dict(), BFILENAME)\n",
    "#     else:\n",
    "#         print(\"\\r\")\n",
    "\n",
    "    # Log the average reward in avg_reward_rec\n",
    "    avg_reward_rec = np.append(avg_reward_rec, np.mean(reward_rec))\n",
    "    violation_rec = np.append(violation_rec, capm.violation_counter)\n",
    "\n",
    "    \n",
    "###########################################################################################\n",
    "# #   PLOT battery levels, hourly rewards and the weights\n",
    "#     yr_record = np.delete(yr_record, 0, 0) #remove the first row which is garbage\n",
    "# #     NO_OF_DAYS = capm.eno.NO_OF_DAYS\n",
    "#     hourly_yr_reward_rec = yr_record[:,2]\n",
    "#     yr_reward_rec = hourly_yr_reward_rec[::24]\n",
    "\n",
    "    \n",
    "#     fig = plt.figure(figsize=(24,3))\n",
    "#     TIME_STEPS = capm.eno.TIME_STEPS\n",
    "#     NO_OF_DAYS = capm.eno.NO_OF_DAYS\n",
    "#     DAY_SPACING = 15\n",
    "#     TICK_SPACING = TIME_STEPS*DAY_SPACING\n",
    "#     #plot battery\n",
    "#     ax = fig.add_subplot(111)\n",
    "#     ax.plot(np.arange(0,TIME_STEPS*NO_OF_DAYS),yr_record[:,0],'r')\n",
    "#     ax.set_ylim([0,1])\n",
    "#     ax.axvline(x=change_hr)\n",
    "#     ax.xaxis.set_major_locator(ticker.MultipleLocator(TICK_SPACING))\n",
    "# #     labels = [item for item in ax.get_xticklabels()]\n",
    "# #     print(labels)\n",
    "# #     labels [15:-1] = np.arange(0,NO_OF_DAYS,DAY_SPACING) #the first label is reserved to negative values\n",
    "# #     ax.set_xticklabels(labels)\n",
    "#     #plot hourly reward\n",
    "#     ax0 = ax.twinx()\n",
    "#     ax0.plot(hourly_yr_reward_rec, color='m')\n",
    "#     ax0.set_ylim(-7,3)\n",
    "#     plt.show()\n",
    "\n",
    "\n",
    "#     fig = plt.figure(figsize=(18,3))\n",
    "#     ax1 = fig.add_subplot(131)\n",
    "#     newfc1 = dqn.eval_net.fc1.weight.data.cpu().numpy().flatten()\n",
    "#     ax1.plot(old2fc1,color='b', alpha=0.4)\n",
    "#     ax1.plot(oldfc1,color='b',alpha = 0.7)\n",
    "#     ax1.plot(newfc1,color='b')\n",
    "#     old2fc1 = oldfc1\n",
    "#     oldfc1 = newfc1\n",
    "    \n",
    "#     ax2 = fig.add_subplot(132)\n",
    "#     newfc2 = dqn.eval_net.fc2.weight.data.cpu().numpy().flatten()\n",
    "#     ax2.plot(old2fc2,color='y', alpha=0.4)\n",
    "#     ax2.plot(oldfc2,color='y',alpha = 0.7)\n",
    "#     ax2.plot(newfc2,color='y')\n",
    "#     old2fc2 = oldfc2\n",
    "#     oldfc2 = newfc2\n",
    "    \n",
    "# #     ax3 = fig.add_subplot(143)\n",
    "# #     newfc3 = dqn.eval_net.fc3.weight.data.cpu().numpy().flatten()\n",
    "# #     ax3.plot(old2fc3,color='y', alpha=0.4)\n",
    "# #     ax3.plot(oldfc3,color='y',alpha = 0.7)\n",
    "# #     ax3.plot(newfc3,color='y')\n",
    "# #     old2fc3 = oldfc3\n",
    "# #     oldfc3 = newfc3\n",
    "    \n",
    "#     axO = fig.add_subplot(133)\n",
    "#     newout = dqn.eval_net.out.weight.data.cpu().numpy().flatten()\n",
    "#     axO.plot(old2out,color='g', alpha=0.4)\n",
    "#     axO.plot(oldout,color='g',alpha=0.7)\n",
    "#     axO.plot(newout,color='g')\n",
    "#     old2out = oldout\n",
    "#     oldout = newout\n",
    "    \n",
    "#     fig.tight_layout()\n",
    "#     plt.show()\n",
    "\n",
    "\n",
    "    # End of training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 648x216 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig = plt.figure(figsize=(9,3))\n",
    "# Plot the average reward log\n",
    "\n",
    "ax1 = fig.add_subplot(111)\n",
    "ax1.set_ylabel(\"Reward\")\n",
    "# ax1.set_ylim([-3,3]);\n",
    "ax1.plot(avg_reward_rec,'b')\n",
    "ax1.tick_params(axis='y', colors='b')\n",
    "\n",
    "#Plot the violation record log\n",
    "ax2 = ax1.twinx()\n",
    "ax2.set_ylabel(\"Violations\",color = 'r')\n",
    "ax2.plot(violation_rec,'r')\n",
    "for xpt in np.argwhere(violation_rec<1):\n",
    "    ax2.axvline(x=xpt,color='g')\n",
    "ax2.set_ylim([0,50]);\n",
    "ax2.tick_params(axis='y', colors='r')\n",
    "\n",
    "fig.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device:  cpu\n",
      "runtime: 0:20:24.429993\n"
     ]
    }
   ],
   "source": [
    "print('Device: ', dqn.device)\n",
    "print('runtime: {}'.format(datetime.now() - tic))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPSILON =  0.98\n",
      "LR      =  5e-05\n",
      "\n",
      "LAST PHASE ITERATION #0:  TOKYO, 2001 \n",
      "Average Reward \t\t= 1.207\n",
      "Violation Counter \t= 7\n",
      "***MEASURING PERFORMANCE OF THE MODEL***\n",
      "\tBEST AVERAGE ANNUAL AVERAGE REWARD = -1000.000\n",
      "\tBEST TOTAL VIOLATIONS              = 1000\n",
      "\n",
      "\tAverage Annual Average Reward      = 1.307\n",
      "\tTotal Violations                   = 1.0\n",
      "****************************************\n",
      "\n",
      "LAST PHASE ITERATION #1:  TOKYO, 2009 \n",
      "Average Reward \t\t= 1.229\n",
      "Violation Counter \t= 5\n",
      "***MEASURING PERFORMANCE OF THE MODEL***\n",
      "\tBEST AVERAGE ANNUAL AVERAGE REWARD = 1.307\n",
      "\tBEST TOTAL VIOLATIONS              = 1.0\n",
      "\n",
      "\tAverage Annual Average Reward      = 1.328\n",
      "\tTotal Violations                   = 11.0\n",
      "****************************************\n",
      "\n",
      "LAST PHASE ITERATION #2:  TOKYO, 2005 \n",
      "Average Reward \t\t= 1.277\n",
      "Violation Counter \t= 9\n",
      "***MEASURING PERFORMANCE OF THE MODEL***\n",
      "\tBEST AVERAGE ANNUAL AVERAGE REWARD = 1.328\n",
      "\tBEST TOTAL VIOLATIONS              = 1.0\n",
      "\n",
      "\tAverage Annual Average Reward      = 1.404\n",
      "\tTotal Violations                   = 0.0\n",
      "****************************************\n",
      "\n",
      "LAST PHASE ITERATION #3:  TOKYO, 2007 \n",
      "Average Reward \t\t= 1.277\n",
      "Violation Counter \t= 2\n",
      "***MEASURING PERFORMANCE OF THE MODEL***\n",
      "\tBEST AVERAGE ANNUAL AVERAGE REWARD = 1.404\n",
      "\tBEST TOTAL VIOLATIONS              = 0.0\n",
      "\n",
      "\tAverage Annual Average Reward      = 1.346\n",
      "\tTotal Violations                   = 1.0\n",
      "****************************************\n",
      "\n",
      "LAST PHASE ITERATION #4:  TOKYO, 2005 \n",
      "Average Reward \t\t= 1.343\n",
      "Violation Counter \t= 3\n",
      "***MEASURING PERFORMANCE OF THE MODEL***\n",
      "\tBEST AVERAGE ANNUAL AVERAGE REWARD = 1.404\n",
      "\tBEST TOTAL VIOLATIONS              = 0.0\n",
      "\n",
      "\tAverage Annual Average Reward      = 1.395\n",
      "\tTotal Violations                   = 4.0\n",
      "****************************************\n",
      "\n",
      "LAST PHASE ITERATION #5:  TOKYO, 2008 \n",
      "Average Reward \t\t= 1.330\n",
      "Violation Counter \t= 4\n",
      "***MEASURING PERFORMANCE OF THE MODEL***\n",
      "\tBEST AVERAGE ANNUAL AVERAGE REWARD = 1.404\n",
      "\tBEST TOTAL VIOLATIONS              = 0.0\n",
      "\n",
      "\tAverage Annual Average Reward      = 1.393\n",
      "\tTotal Violations                   = 0.0\n",
      "****************************************\n",
      "\n",
      "LAST PHASE ITERATION #6:  TOKYO, 2004 \n",
      "Average Reward \t\t= 1.172\n",
      "Violation Counter \t= 10\n",
      "***MEASURING PERFORMANCE OF THE MODEL***\n",
      "\tBEST AVERAGE ANNUAL AVERAGE REWARD = 1.404\n",
      "\tBEST TOTAL VIOLATIONS              = 0.0\n",
      "\n",
      "\tAverage Annual Average Reward      = 1.389\n",
      "\tTotal Violations                   = 0.0\n",
      "****************************************\n",
      "\n",
      "LAST PHASE ITERATION #7:  TOKYO, 2004 \n",
      "Average Reward \t\t= 1.230\n",
      "Violation Counter \t= 6\n",
      "***MEASURING PERFORMANCE OF THE MODEL***\n",
      "\tBEST AVERAGE ANNUAL AVERAGE REWARD = 1.404\n",
      "\tBEST TOTAL VIOLATIONS              = 0.0\n",
      "\n",
      "\tAverage Annual Average Reward      = 1.251\n",
      "\tTotal Violations                   = 33.0\n",
      "****************************************\n",
      "\n",
      "LAST PHASE ITERATION #8:  TOKYO, 2001 \n",
      "Average Reward \t\t= 1.318\n",
      "Violation Counter \t= 2\n",
      "***MEASURING PERFORMANCE OF THE MODEL***\n",
      "\tBEST AVERAGE ANNUAL AVERAGE REWARD = 1.404\n",
      "\tBEST TOTAL VIOLATIONS              = 0.0\n",
      "\n",
      "\tAverage Annual Average Reward      = 1.287\n",
      "\tTotal Violations                   = 0.0\n",
      "****************************************\n",
      "\n",
      "LAST PHASE ITERATION #9:  TOKYO, 2009 \n",
      "Average Reward \t\t= 1.188\n",
      "Violation Counter \t= 7\n",
      "***MEASURING PERFORMANCE OF THE MODEL***\n",
      "\tBEST AVERAGE ANNUAL AVERAGE REWARD = 1.404\n",
      "\tBEST TOTAL VIOLATIONS              = 0.0\n",
      "\n",
      "\tAverage Annual Average Reward      = 1.371\n",
      "\tTotal Violations                   = 0.0\n",
      "****************************************\n"
     ]
    }
   ],
   "source": [
    "#END OF TRAINING PHASE - CHOOSING THE BEST MODEL INSTANCE\n",
    "#INCREASE GREEDY RATE\n",
    "#VALIDATE AFTER EVERY ITERATION\n",
    "\n",
    "# Use this model and its output as base standards for the last phase of training\n",
    "best_avg_avg_reward = -1000\n",
    "best_net_avg_reward = dqn.eval_net\n",
    "best_avg_v_counter = 1000\n",
    "best_net_v_counter = dqn.eval_net\n",
    "\n",
    "\n",
    "NO_OF_LAST_PHASE_ITERATIONS = 10\n",
    "EPSILON = 0.98\n",
    "LR = 0.00005\n",
    "\n",
    "print(\"EPSILON = \", EPSILON)\n",
    "print(\"LR      = \", LR)\n",
    "\n",
    "for iteration in range(NO_OF_LAST_PHASE_ITERATIONS):\n",
    "    LOCATION = 'tokyo'#random.choice(['tokyo','wakkanai','minamidaito'])\n",
    "    YEAR = random.choice(np.arange(2000,2010))\n",
    "    capm = CAPM(LOCATION,YEAR,shuffle=False, trainmode=False) #instantiate the CAPM class\n",
    "    capm.eno = ENO(LOCATION,YEAR, shuffle=False, day_balance=False) #instantiate the environment inside the CAPM class\n",
    "    capm.HMAX = capm.eno.SMAX #maximum power output of solar cell is set in CAPM object using the value in ENO object\n",
    "\n",
    "    print('\\nLAST PHASE ITERATION #{}:  {}, {} '.format(iteration, LOCATION.upper(), YEAR))\n",
    "    \n",
    "    \n",
    "    my_avg_reward = -1000\n",
    "    my_v_counter = 1000\n",
    "    \n",
    "    s, r, day_end, year_end = capm.reset()\n",
    "    yr_record = np.empty(4)\n",
    "\n",
    "    record = np.empty(4) #record for battery, henergy, reward and action\n",
    "    transition_rec = np.zeros((capm.eno.TIME_STEPS, N_STATES * 2 + 2)) #record all the transition in one day\n",
    "\n",
    "    while True:\n",
    "        a = dqn.choose_action(stdize(s))\n",
    "\n",
    "        # present state = [batt, enp, henergy]\n",
    "        record = np.vstack((record, [s[0],s[2],r, a])) # record battery, henergy, reward and action for troubleshooting\n",
    "        yr_record = np.vstack((yr_record, [s[0],s[2],r, a]))\n",
    "\n",
    "        # take action\n",
    "        s_, r, day_end, year_end = capm.step(a)\n",
    "        \n",
    "        temp_transitions = np.hstack((stdize(s), [a, r], stdize(s_)))\n",
    "        transition_rec[capm.eno.hr-1,:] = temp_transitions\n",
    "\n",
    "        if (day_end):\n",
    "            transition_rec[:,5] += r #broadcast reward to all states\n",
    "            decay_factor = [i for i in (LAMBDA**n for n in reversed(range(0, capm.eno.TIME_STEPS)))]\n",
    "            transition_rec[:,5] = transition_rec[:,5] * decay_factor #decay reward proportionately\n",
    "            dqn.store_day_transition(transition_rec)\n",
    "\n",
    "        if dqn.memory_counter > MEMORY_CAPACITY:\n",
    "            dqn.learn()\n",
    "\n",
    "        if dqn.nettoggle:\n",
    "            change_hr = capm.eno.day*24+capm.eno.hr #to mark when the DQN is updated.\n",
    "            dqn.nettoggle = not dqn.nettoggle\n",
    "\n",
    "        if (year_end):\n",
    "            break\n",
    "\n",
    "        # transition to new state\n",
    "        s = s_\n",
    "\n",
    "    record = np.delete(record, 0, 0) #remove the first row which is garbage\n",
    "    reward_rec = record[:,2] #extract reward information from the record array\n",
    "    reward_rec = reward_rec[::24] #only consider terminal rewards\n",
    "    print(\"Average Reward \\t\\t= {:.3f}\".format(np.mean(reward_rec)))\n",
    "    print(\"Violation Counter \\t= {}\".format(capm.violation_counter))\n",
    "    \n",
    "    \n",
    "    print(\"***MEASURING PERFORMANCE OF THE MODEL***\")\n",
    "    print(\"\\tBEST AVERAGE ANNUAL AVERAGE REWARD = {:.3f}\".format(best_avg_avg_reward))\n",
    "    print(\"\\tBEST TOTAL VIOLATIONS              = {}\".format(best_avg_v_counter))\n",
    "    LOCATION = 'tokyo'\n",
    "    results = np.empty(3)\n",
    "    for YEAR in np.arange(2010,2015):\n",
    "        capm = CAPM(LOCATION,YEAR,shuffle=False, trainmode=False) #instantiate the CAPM class\n",
    "        capm.eno = ENO(LOCATION,YEAR, shuffle=False, day_balance=False) #instantiate the environment inside the CAPM class\n",
    "        capm.HMAX = capm.eno.SMAX #maximum power output of solar cell is set in CAPM object using the value in ENO object\n",
    "\n",
    "        s, r, day_end, year_end = capm.reset()\n",
    "        yr_test_record = np.empty(4)\n",
    "\n",
    "        while True:\n",
    "            a = dqn.choose_greedy_action(stdize(s))\n",
    "            #state = [batt, enp, henergy, fcast]\n",
    "            yr_test_record = np.vstack((yr_test_record, [s[0],s[2],r, a])) #record battery, henergy, reward and action\n",
    "            # take action\n",
    "            s_, r, day_end, year_end = capm.step(a)\n",
    "            if year_end:\n",
    "                break\n",
    "            s = s_\n",
    "\n",
    "        yr_test_record = np.delete(yr_test_record, 0, 0) #remove the first row which is garbage\n",
    "        yr_test_reward_rec = yr_test_record[:,2]\n",
    "        yr_test_reward_rec = yr_test_reward_rec[::24] #annual average reward\n",
    "        results = np.vstack((results, [int(YEAR), np.mean(yr_test_reward_rec), int(capm.violation_counter)]))\n",
    "    results = np.delete(results,0,0)\n",
    "    my_avg_reward = np.mean(results[:,1]) #the average of annual average rewards\n",
    "    my_v_counter = np.sum(results[:,-1]) #total sum of violations\n",
    "    print(\"\\n\\tAverage Annual Average Reward      = {:.3f}\".format(my_avg_reward))\n",
    "    print(\"\\tTotal Violations                   = {}\".format(my_v_counter))\n",
    "\n",
    "    if (my_avg_reward > best_avg_avg_reward):\n",
    "            best_avg_avg_reward = my_avg_reward\n",
    "            best_net_avg_reward = dqn.eval_net\n",
    "\n",
    "    if (my_v_counter < best_avg_v_counter):\n",
    "        best_avg_v_counter = my_v_counter\n",
    "        best_net_v_counter = dqn.eval_net\n",
    "    elif (my_v_counter == best_avg_v_counter):\n",
    "        if (my_avg_reward > best_avg_avg_reward):\n",
    "            best_avg_v_counter = my_v_counter\n",
    "            best_net_v_counter = dqn.eval_net\n",
    "    print(\"****************************************\")\n",
    "\n",
    "    \n",
    "\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TESTING BEST MODEL BASED ON AVERAGE REWARD METRIC\n",
      "YEAR\t\tAVG_RWD\t\tVIOLATIONS\n",
      "2015 \t\t 1.31 \t\t 0\n",
      "2016 \t\t 1.36 \t\t 0\n",
      "2017 \t\t 0.55 \t\t 33\n",
      "2018 \t\t 1.35 \t\t 0\n"
     ]
    }
   ],
   "source": [
    "#TESTING BEST MODEL BASED ON AVERAGE REWARD METRIC\n",
    "dqn = DQN()\n",
    "dqn.eval_net = best_net_avg_reward\n",
    "LOCATION = 'tokyo'\n",
    "results = np.empty(3)\n",
    "for YEAR in np.arange(2015,2019):\n",
    "    capm = CAPM(LOCATION,YEAR,shuffle=False, trainmode=False) #instantiate the CAPM class\n",
    "    capm.eno = ENO(LOCATION,YEAR, shuffle=False, day_balance=False) #instantiate the environment inside the CAPM class\n",
    "    capm.HMAX = capm.eno.SMAX #maximum power output of solar cell is set in CAPM object using the value in ENO object\n",
    "\n",
    "    s, r, day_end, year_end = capm.reset()\n",
    "    yr_test_record = np.empty(4)\n",
    "\n",
    "    while True:\n",
    "        a = dqn.choose_greedy_action(stdize(s))\n",
    "\n",
    "        #state = [batt, enp, henergy, fcast]\n",
    "        yr_test_record = np.vstack((yr_test_record, [s[0],s[2],r, a])) #record battery, henergy, reward and action\n",
    "\n",
    "        # take action\n",
    "        s_, r, day_end, year_end = capm.step(a)\n",
    "\n",
    "        if year_end:\n",
    "            break\n",
    "\n",
    "        s = s_\n",
    "\n",
    "    yr_test_record = np.delete(yr_test_record, 0, 0) #remove the first row which is garbage\n",
    "\n",
    "#     #Plot the reward and battery for the entire year run\n",
    "#     title = LOCATION.upper() + ',' + str(YEAR)\n",
    "\n",
    "#     NO_OF_DAYS = capm.eno.NO_OF_DAYS\n",
    "    yr_test_reward_rec = yr_test_record[:,2]\n",
    "    yr_test_reward_rec = yr_test_reward_rec[::24]\n",
    "#     print('Average Reward for',title, '=', np.mean(yr_test_reward_rec))\n",
    "#     print('Violations for',title, '=', capm.violation_counter)\n",
    "    \n",
    "    results = np.vstack((results, [int(YEAR), np.mean(yr_test_reward_rec), int(capm.violation_counter)]))\n",
    "\n",
    "#     fig = plt.figure(figsize=(24,3))\n",
    "#     fig.suptitle(title, fontsize=15)\n",
    "\n",
    "#     #     ax1 = fig.add_subplot(211)\n",
    "#     #     ax1.plot(yr_test_reward_rec)\n",
    "#     #     ax1.set_title(\"\\n\\nYear Run Reward\")\n",
    "#     #     ax1.set_ylim([-3,1])\n",
    "\n",
    "#     ax2 = fig.add_subplot(111)\n",
    "#     ax2.plot(yr_test_record[:,0],'r')\n",
    "#     ax2.set_title(\"\\n\\nYear Run Battery\")\n",
    "#     ax2.set_ylim([0,1])\n",
    "#     plt.sca(ax2)\n",
    "#     plt.xticks(np.arange(0, NO_OF_DAYS*24, 50*24),np.arange(0,NO_OF_DAYS,50))\n",
    "\n",
    "#     fig.tight_layout()\n",
    "#     plt.show()\n",
    "\n",
    "results = np.delete(results,0,0)\n",
    "\n",
    "print(\"TESTING BEST MODEL BASED ON AVERAGE REWARD METRIC\")\n",
    "print('YEAR\\t\\tAVG_RWD\\t\\tVIOLATIONS')\n",
    "for x in np.arange(0,results.shape[0]):\n",
    "    print('{} \\t\\t {} \\t\\t {}'.format(int(results[x,0]), np.around(results[x,1],2), int(results[x,-1])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TESTING BASED ON VIOLATION COUNTER METRIC\n",
      "YEAR\t\tAVG_RWD\t\tVIOLATIONS\n",
      "2015 \t\t 1.31 \t\t 0\n",
      "2016 \t\t 1.36 \t\t 0\n",
      "2017 \t\t 0.55 \t\t 33\n",
      "2018 \t\t 1.35 \t\t 0\n"
     ]
    }
   ],
   "source": [
    "#TESTING BASED ON VIOLATION COUNTER METRIC\n",
    "dqn = DQN()\n",
    "dqn.eval_net = best_net_v_counter\n",
    "\n",
    "LOCATION = 'tokyo'\n",
    "results = np.empty(3)\n",
    "for YEAR in np.arange(2015,2019):\n",
    "    capm = CAPM(LOCATION,YEAR,shuffle=False, trainmode=False) #instantiate the CAPM class\n",
    "    capm.eno = ENO(LOCATION,YEAR, shuffle=False, day_balance=False) #instantiate the environment inside the CAPM class\n",
    "    capm.HMAX = capm.eno.SMAX #maximum power output of solar cell is set in CAPM object using the value in ENO object\n",
    "\n",
    "    s, r, day_end, year_end = capm.reset()\n",
    "    yr_test_record = np.empty(4)\n",
    "\n",
    "    while True:\n",
    "        a = dqn.choose_greedy_action(stdize(s))\n",
    "\n",
    "        #state = [batt, enp, henergy, fcast]\n",
    "        yr_test_record = np.vstack((yr_test_record, [s[0],s[2],r, a])) #record battery, henergy, reward and action\n",
    "\n",
    "        # take action\n",
    "        s_, r, day_end, year_end = capm.step(a)\n",
    "\n",
    "        if year_end:\n",
    "            break\n",
    "\n",
    "        s = s_\n",
    "\n",
    "    yr_test_record = np.delete(yr_test_record, 0, 0) #remove the first row which is garbage\n",
    "\n",
    "#     #Plot the reward and battery for the entire year run\n",
    "#     title = LOCATION.upper() + ',' + str(YEAR)\n",
    "\n",
    "#     NO_OF_DAYS = capm.eno.NO_OF_DAYS\n",
    "    yr_test_reward_rec = yr_test_record[:,2]\n",
    "    yr_test_reward_rec = yr_test_reward_rec[::24]\n",
    "#     print('Average Reward for',title, '=', np.mean(yr_test_reward_rec))\n",
    "#     print('Violations for',title, '=', capm.violation_counter)\n",
    "    \n",
    "    results = np.vstack((results, [int(YEAR), np.mean(yr_test_reward_rec), int(capm.violation_counter)]))\n",
    "\n",
    "#     fig = plt.figure(figsize=(24,3))\n",
    "#     fig.suptitle(title, fontsize=15)\n",
    "\n",
    "#     #     ax1 = fig.add_subplot(211)\n",
    "#     #     ax1.plot(yr_test_reward_rec)\n",
    "#     #     ax1.set_title(\"\\n\\nYear Run Reward\")\n",
    "#     #     ax1.set_ylim([-3,1])\n",
    "\n",
    "#     ax2 = fig.add_subplot(111)\n",
    "#     ax2.plot(yr_test_record[:,0],'r')\n",
    "#     ax2.set_title(\"\\n\\nYear Run Battery\")\n",
    "#     ax2.set_ylim([0,1])\n",
    "#     plt.sca(ax2)\n",
    "#     plt.xticks(np.arange(0, NO_OF_DAYS*24, 50*24),np.arange(0,NO_OF_DAYS,50))\n",
    "\n",
    "#     fig.tight_layout()\n",
    "#     plt.show()\n",
    "\n",
    "results = np.delete(results,0,0)\n",
    "\n",
    "print(\"TESTING BASED ON VIOLATION COUNTER METRIC\")\n",
    "print('YEAR\\t\\tAVG_RWD\\t\\tVIOLATIONS')\n",
    "for x in np.arange(0,results.shape[0]):\n",
    "    print('{} \\t\\t {} \\t\\t {}'.format(int(results[x,0]), np.around(results[x,1],2), int(results[x,-1])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #Plot the reward and battery for the entire year run on a day by day basis\n",
    "# title = LOCATION.upper() + ',' + str(YEAR)\n",
    "# TIME_AXIS = np.arange(0,capm.eno.TIME_STEPS)\n",
    "# for DAY in range(0,10):#capm.eno.NO_OF_DAYS):\n",
    "#     START = DAY*24\n",
    "#     END = START+24\n",
    "\n",
    "#     daytitle = title + ' - DAY ' + str(DAY)\n",
    "#     fig = plt.figure(figsize=(16,4))\n",
    "#     st = fig.suptitle(daytitle)\n",
    "\n",
    "#     ax2 = fig.add_subplot(121)\n",
    "#     ax2.plot(yr_test_record[START:END,1],'g')\n",
    "#     ax2.set_title(\"HARVESTED ENERGY\")\n",
    "#     plt.xlabel(\"Hour\")\n",
    "#     ax2.set_ylim([0,1])\n",
    "\n",
    "#     #plot battery for year run\n",
    "#     ax1 = fig.add_subplot(122)\n",
    "#     ax1.plot(TIME_AXIS,yr_test_record[START:END,0],'r') \n",
    "# #     ax1.plot(TIME_AXIS, np.ones(capm.eno.TIME_STEPS)*capm.BOPT/capm.BMAX,'r--')\n",
    "#     ax1.plot(TIME_AXIS, np.ones(capm.eno.TIME_STEPS)*capm.BOPT/capm.BMAX,'r--')\n",
    "#     ax1.text(0.1, 0.2, \"BINIT = %.2f\\n\" %(yr_test_record[START,0]),fontsize=11, ha='left')\n",
    "#     ax1.text(0.1, 0.4, \"TENP = %.2f\\n\" %(capm.BOPT/capm.BMAX-yr_test_record[END,0]),fontsize=11, ha='left')\n",
    "#     ax1.text(0.1, 0.3, \"BMEAN = %.2f\\n\" %(np.mean(yr_test_record[START:END,0])),fontsize=11, ha='left')\n",
    "\n",
    "\n",
    "\n",
    "#     ax1.set_title(\"YEAR RUN TEST\")\n",
    "#     if END < (capm.eno.NO_OF_DAYS*capm.eno.TIME_STEPS):\n",
    "#         ax1.text(0.1, 0, \"REWARD = %.2f\\n\" %(yr_test_record[END,2]),fontsize=13, ha='left')\n",
    "#     plt.xlabel(\"Hour\")\n",
    "#     ax1.set_ylabel('Battery', color='r',fontsize=12)\n",
    "#     ax1.set_ylim([0,1])\n",
    "\n",
    "#     #plot actions for year run\n",
    "#     ax1a = ax1.twinx()\n",
    "#     ax1a.plot(yr_test_record[START:END,3])\n",
    "#     ax1a.set_ylim([0,N_ACTIONS])\n",
    "#     ax1a.set_ylabel('Duty Cycle', color='b',fontsize=12)\n",
    "\n",
    "#     fig.tight_layout()\n",
    "#     st.set_y(0.95)\n",
    "#     fig.subplots_adjust(top=0.75)\n",
    "#     plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
