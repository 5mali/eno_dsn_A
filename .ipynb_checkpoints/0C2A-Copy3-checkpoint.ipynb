{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "from datetime import datetime\n",
    "tic = datetime.now()\n",
    "\n",
    "import os\n",
    "from os.path import dirname, abspath, join\n",
    "from os import getcwd\n",
    "import sys\n",
    "\n",
    "# THIS_DIR = getcwd()\n",
    "# CLASS_DIR = abspath(join(THIS_DIR, 'dsnclasses'))  #abspath(join(THIS_DIR, '../../..', 'dsnclasses'))\n",
    "# sys.path.append(CLASS_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import string\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.ticker as ticker\n",
    "import torch\n",
    "from IPython.display import clear_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 230\n",
    "random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "torch.cuda.manual_seed(seed)\n",
    "np.random.seed(seed)\n",
    "os.environ['PYTHONHASHSEED'] = str(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ENO(object):\n",
    "    \n",
    "    #no. of forecast types is 6 ranging from 0 to 5\n",
    "  \n",
    "    def __init__(self, location='tokyo', year=2010, shuffle=False, day_balance=False):\n",
    "        self.location = location\n",
    "        self.year = year\n",
    "        self.day = None\n",
    "        self.hr = None\n",
    "        \n",
    "        self.shuffle = shuffle\n",
    "        self.day_balance = day_balance\n",
    "\n",
    "        self.TIME_STEPS = None #no. of time steps in one episode\n",
    "        self.NO_OF_DAYS = None #no. of days in one year\n",
    "        \n",
    "        self.NO_OF_DAYTYPE = 10 #no. of daytypes\n",
    "        self.daycounter = 0 #to count number of days that have been passed\n",
    "        \n",
    "        self.sradiation = None #matrix with GSR for the entire year\n",
    "        self.senergy = None #matrix with harvested energy data for the entire year\n",
    "        self.fforecast = None #array with forecast values for each day\n",
    "        \n",
    "\n",
    "        self.henergy = None #harvested energy variable\n",
    "        self.fcast = None #forecast variable\n",
    "        self.sorted_days = [] #days sorted according to day type\n",
    "        \n",
    "        self.SMAX = 1000 # 1 Watt Solar Panel\n",
    "\n",
    "    \n",
    "    #function to get the solar data for the given location and year and prep it\n",
    "    def get_data(self):\n",
    "        #solar_data/CSV files contain the values of GSR (Global Solar Radiation in MegaJoules per meters squared per hour)\n",
    "        #weather_data/CSV files contain the weather summary from 06:00 to 18:00 and 18:00 to 06:00+1\n",
    "        location = self.location\n",
    "        year = self.year\n",
    "\n",
    "        THIS_DIR = getcwd()\n",
    "        SDATA_DIR = abspath(join(THIS_DIR, 'solar_data'))  #abspath(join(THIS_DIR, '../../..', 'data'))\n",
    "        \n",
    "        sfile = SDATA_DIR + '/' + location +'/' + str(year) + '.csv'\n",
    "        \n",
    "        #skiprows=4 to remove unnecessary title texts\n",
    "        #usecols=4 to read only the Global Solar Radiation (GSR) values\n",
    "        solar_radiation = pd.read_csv(sfile, skiprows=4, encoding='shift_jisx0213', usecols=[4])\n",
    "      \n",
    "        #convert dataframe to numpy array\n",
    "        solar_radiation = solar_radiation.values\n",
    "\n",
    "        #convert missing data in CSV files to zero\n",
    "        solar_radiation[np.isnan(solar_radiation)] = 0\n",
    "\n",
    "        #reshape solar_radiation into no_of_daysx24 array\n",
    "        solar_radiation = solar_radiation.reshape(-1,24)\n",
    "\n",
    "        if(self.shuffle): #if class instatiation calls for shuffling the day order. Required when learning\n",
    "            np.random.shuffle(solar_radiation) \n",
    "        self.sradiation = solar_radiation\n",
    "        \n",
    "        #GSR values (in MJ/sq.mts per hour) need to be expressed in mW\n",
    "        # Conversion is accomplished by \n",
    "        # solar_energy = GSR(in MJ/m2/hr) * 1e6 * size of solar cell * efficiency of solar cell /(60x60) *1000 (to express in mW)\n",
    "        # the factor of 2 in the end is assuming two solar cells\n",
    "        self.senergy = 2*self.sradiation * 1e6 * (55e-3 * 70e-3) * 0.15 * 1000/(60*60)\n",
    "\n",
    "        return 0\n",
    "    \n",
    "    #function to map total day radiation into type of day ranging from 0 to 5\n",
    "    #the classification into day types is quite arbitrary. There is no solid logic behind this type of classification.\n",
    "    \n",
    "    def get_day_state(self,tot_day_radiation):\n",
    "        bin_edges = np.array([0, 3.5, 6.5, 9.0, 12.5, 15.5, 18.5, 22.0, 25, 28])\n",
    "        for k in np.arange(1,bin_edges.size):\n",
    "            if (bin_edges[k-1] < tot_day_radiation <= bin_edges[k]):\n",
    "                day_state = k -1\n",
    "            else:\n",
    "                day_state = bin_edges.size - 1\n",
    "        return int(day_state)\n",
    "    \n",
    "    def get_forecast(self):\n",
    "        #create a perfect forecaster.\n",
    "        tot_day_radiation = np.sum(self.sradiation, axis=1) #contains total solar radiation for each day\n",
    "        get_day_state = np.vectorize(self.get_day_state)\n",
    "        self.fforecast = get_day_state(tot_day_radiation)\n",
    "        \n",
    "        #sort days depending on the type of day and shuffle them; maybe required when learning\n",
    "        for fcast in range(0,6):\n",
    "            fcast_days = ([i for i,x in enumerate(self.fforecast) if x == fcast])\n",
    "            np.random.shuffle(fcast_days)\n",
    "            self.sorted_days.append(fcast_days)\n",
    "        return 0\n",
    "    \n",
    "    def reset(self,day=0): #it is possible to reset to the beginning of a certain day\n",
    "        \n",
    "        self.get_data() #first get data for the given year\n",
    "        self.get_forecast() #calculate the forecast\n",
    "        \n",
    "        self.TIME_STEPS = self.senergy.shape[1]\n",
    "        self.NO_OF_DAYS = self.senergy.shape[0]\n",
    "        \n",
    "        self.day = day\n",
    "        self.hr = 0\n",
    "        \n",
    "        self.henergy = self.senergy[self.day][self.hr]\n",
    "        self.fcast = self.fforecast[self.day]\n",
    "        \n",
    "        end_of_day = False\n",
    "        end_of_year = False\n",
    "        return [self.henergy, self.fcast, end_of_day, end_of_year]\n",
    "\n",
    "    \n",
    "    def step(self):\n",
    "        end_of_day = False\n",
    "        end_of_year = False\n",
    "        if not(self.day_balance): #if daytype balance is not required\n",
    "            if(self.hr < self.TIME_STEPS - 1):\n",
    "                self.hr += 1\n",
    "                self.henergy = self.senergy[self.day][self.hr] \n",
    "                self.fcast = self.fforecast[self.day]\n",
    "            else:\n",
    "                if(self.day < self.NO_OF_DAYS -1):\n",
    "                    end_of_day = True\n",
    "                    self.hr = 0\n",
    "                    self.day += 1\n",
    "                    self.henergy = self.senergy[self.day][self.hr] \n",
    "                    self.fcast = self.fforecast[self.day]\n",
    "                else:\n",
    "                    end_of_day = True\n",
    "                    end_of_year = True\n",
    "                    \n",
    "        else: #when training, we want all daytypes to be equally represented for robust policy\n",
    "              #obviously, the days are going to be in random order\n",
    "            if(self.hr < self.TIME_STEPS - 1):\n",
    "                self.hr += 1\n",
    "                self.henergy = self.senergy[self.day][self.hr]\n",
    "                self.fcast = self.fforecast[self.day]\n",
    "            else:\n",
    "                if(self.daycounter < self.NO_OF_DAYS -1):\n",
    "                    end_of_day = True\n",
    "                    self.daycounter += 1\n",
    "                    self.hr = 0\n",
    "                    daytype = random.choice(np.arange(0,self.NO_OF_DAYTYPE)) #choose random daytype\n",
    "                    self.day = np.random.choice(self.sorted_days[daytype]) #choose random day from that daytype\n",
    "                    self.henergy = self.senergy[self.day][self.hr] \n",
    "                    self.fcast = self.fforecast[self.day]\n",
    "                else: \n",
    "                    end_of_day = True\n",
    "                    end_of_year = True\n",
    "                    self.daycounter = 0\n",
    "        \n",
    "        \n",
    "        return [self.henergy, self.fcast, end_of_day, end_of_year]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CAPM (object):\n",
    "    def __init__(self,location='tokyo', year=2010, shuffle=False, trainmode=False):\n",
    "\n",
    "        #all energy values i.e. BMIN, BMAX, BOPT, HMAX are in mWhr. Assuming one timestep is one hour\n",
    "        \n",
    "        self.BMIN = 0.0                #Minimum battery level that is tolerated. Maybe non-zero also\n",
    "        self.BMAX = 9250.0            #Max Battery Level. May not necessarily be equal to total batter capacity [3.6V x 2500mAh]\n",
    "        self.BOPT = 0.5 * self.BMAX    #Optimal Battery Level. Assuming 50% of battery is the optimum\n",
    "        \n",
    "        self.HMIN = 0      #Minimum energy that can be harvested by the solar panel.\n",
    "        self.HMAX = None   #Maximum energy that can be harvested by the solar panel. [500mW]\n",
    "        \n",
    "        self.DMAX = 500      #Maximum energy that can be consumed by the node in one time step. [~ 3.6V x 135mA]\n",
    "        self.N_ACTIONS = 10  #No. of different duty cycles possible\n",
    "        self.DMIN = self.DMAX/self.N_ACTIONS #Minimum energy that can be consumed by the node in one time step. [~ 3.6V x 15mA]\n",
    "        \n",
    "        self.binit = None     #battery at the beginning of day\n",
    "        self.btrack = []      #track the mean battery level for each day\n",
    "        self.atrack = []      #track the duty cycles for each day\n",
    "        self.batt = None      #battery variable\n",
    "        self.enp = None       #enp at end of hr\n",
    "        self.henergy = None   #harvested energy variable\n",
    "        self.fcast = None     #forecast variable\n",
    "        \n",
    "        self.MUBATT = 0.6\n",
    "        self.SDBATT = 0.02\n",
    "        \n",
    "        self.MUHENERGY = 0.5\n",
    "        self.SDHENERGY = 0.2\n",
    "        \n",
    "        self.MUENP = 0\n",
    "        self.SDENP = 0.02\n",
    "        \n",
    "        self.location = location\n",
    "        self.year = year\n",
    "        self.shuffle = shuffle\n",
    "        self.trainmode = trainmode\n",
    "        self.eno = None#ENO(self.location, self.year, shuffle=shuffle, day_balance=trainmode) #if trainmode is enable, then days are automatically balanced according to daytype i.e. day_balance= True\n",
    "        \n",
    "        self.day_violation_flag = False\n",
    "        self.violation_flag = False\n",
    "        self.violation_counter = 0\n",
    "\n",
    "        self.NO_OF_DAYTYPE = 10 #no. of daytypes\n",
    " \n",
    "    def reset(self,day=0,batt=-1):\n",
    "        henergy, fcast, day_end, year_end = self.eno.reset(day) #reset the eno environment\n",
    "        self.violation_flag = False\n",
    "        self.violation_counter = 0\n",
    "        if(batt == -1):\n",
    "            self.batt = self.BOPT\n",
    "        else:\n",
    "            self.batt = batt\n",
    "            \n",
    "        self.batt = np.clip(self.batt, self.BMIN, self.BMAX)\n",
    "        self.binit = self.batt\n",
    "        self.btrack = np.append(self.btrack, self.batt) #track battery levels\n",
    "\n",
    "#         self.enp = self.BOPT - self.batt\n",
    "        self.enp = self.binit - self.batt #enp is calculated\n",
    "        self.henergy = np.clip(henergy, self.HMIN, self.HMAX) #clip henergy within HMIN and HMAX\n",
    "        self.fcast = fcast\n",
    "        \n",
    "        norm_batt = self.batt/self.BMAX\n",
    "        norm_enp = self.enp/(self.BMAX/2)\n",
    "        norm_henergy = self.henergy/self.HMAX\n",
    "        norm_fcast = self.fcast/(self.NO_OF_DAYTYPE-1)\n",
    "\n",
    "        c_state = [norm_batt, norm_enp, norm_henergy, norm_fcast] #continuous states\n",
    "        reward = 0\n",
    "        \n",
    "        return [c_state, reward, day_end, year_end]\n",
    "    \n",
    "    def getstate(self): #query the present state of the system\n",
    "        norm_batt = self.batt/self.BMAX - self.MUBATT\n",
    "        norm_enp = self.enp/(self.BMAX/2)\n",
    "        norm_henergy = self.henergy/self.HMAX\n",
    "        norm_fcast = self.fcast/(self.NO_OF_DAYTYPE-1)        \n",
    "        c_state = [norm_batt, norm_enp, norm_henergy] #continuous states\n",
    "\n",
    "        return c_state\n",
    "    \n",
    "#     def rewardfn(self):\n",
    "#         R_PARAM = 20000 #chosen empirically for best results\n",
    "#         mu = 0\n",
    "#         sig = 0.07*R_PARAM #knee curve starts at approx. 2000mWhr of deviation\n",
    "#         norm_reward = 3*(np.exp(-np.power((self.enp - mu)/sig, 2.)/2) / np.exp(-np.power((0 - mu)/sig, 2.)/2))-1\n",
    "\n",
    "        \n",
    "# #         if(np.abs(self.enp) <= 0.12*R_PARAM):\n",
    "# #             norm_reward = 2*(np.exp(-np.power((self.enp - mu)/sig, 2.)/2) / np.exp(-np.power((0 - mu)/sig, 2.)/2))\n",
    "# #         else:\n",
    "# #             norm_reward = -0.25 - 10*np.abs(self.enp/R_PARAM)\n",
    "#         if(self.day_violation_flag):\n",
    "#             norm_reward -= 3\n",
    "            \n",
    "#         return (norm_reward)\n",
    "        \n",
    "    \n",
    "    #reward function\n",
    "    def rewardfn(self):\n",
    "        \n",
    "        #FIRST REWARD AS A FUNCTION OF DRIFT OF BMEAN FROM BOPT i.e. in terms of BDEV = |BMEAN-BOPT|/BMAX\n",
    "        bmean = np.mean(self.btrack)\n",
    "        bdev = np.abs(self.BOPT - bmean)/self.BMAX\n",
    "        # based on the sigmoid function\n",
    "        # bdev ranges from bdev = (0,0.5) of BMAX\n",
    "        p1_sharpness = 10\n",
    "        n1_sharpness = 20\n",
    "        shift1 = 0.5\n",
    "        # r1(x) = 0.5 when x = 0.25. \n",
    "        # Therefore, shift = 0.5 to make sure that (2*x-shift) evaluates to zero at x = 0.25\n",
    "\n",
    "        if(bdev<=0.25): \n",
    "            r1 = 2*(1-(1 / (1 + np.exp(-p1_sharpness*(2*bdev-shift1)))))-1\n",
    "        else: \n",
    "            r1 = 2*(1-(1 / (1 + np.exp(-n1_sharpness*(2*bdev-shift1)))))-1\n",
    "        # r1 ranges from -1 to 1\n",
    "            \n",
    "        #SECOND REWARD AS A FUNCTION OF ENP AS LONG AS BMAX/4 <= batt <= 3*BMAX/4 i.e. bdev <= 0.25\n",
    "        if(bdev <=0.25):\n",
    "            # enp ranges from enp = (0,3) of DMAX\n",
    "            p2_sharpness = 2\n",
    "            n2_sharpness = 2\n",
    "            shift2 = 6    \n",
    "            # r1(x) = 0.5 when x = 2. \n",
    "            # Therefore, shift = 6 to make sure that (3*x-shift) evaluates to zero at x = 2\n",
    "#             print('Day energy', np.sum(self.eno.senergy[self.eno.day]))\n",
    "#             print('Node energy', np.sum(self.atrack)*self.DMAX/self.N_ACTIONS)\n",
    "#             x = np.abs(np.sum(self.eno.senergy[self.eno.day])-np.sum(self.atrack)*self.DMAX/self.N_ACTIONS )/self.DMAX\n",
    "            x = np.abs(self.enp/self.DMAX)\n",
    "            if(x<=2): \n",
    "                r2 = (1 / (1 + np.exp(p2_sharpness*(3*x-shift2))))\n",
    "            else: \n",
    "                r2 = (1 / (1 + np.exp(n2_sharpness*(3*x-shift2))))\n",
    "        else:\n",
    "            r2 = 0 # if mean battery lies outside bdev limits, then enp reward is not considered.\n",
    "        # r2 ranges from 0 to 1\n",
    "\n",
    "        #REWARD AS A FUNCTION OF BATTERY VIOLATIONS\n",
    "        if(self.day_violation_flag):\n",
    "            violation_penalty = 3\n",
    "        else:\n",
    "            violation_penalty = 0 #penalty for violating battery limits anytime during the day\n",
    "        \n",
    "#         print(\"Reward \", (r1 + r2 - violation_penalty), '\\n')\n",
    "        return (r1*(2**r2) - violation_penalty)\n",
    "    \n",
    "    def step(self, action):\n",
    "        day_end = False\n",
    "        year_end = False\n",
    "        self.violation_flag = False\n",
    "        reward = 0\n",
    "       \n",
    "        action = np.clip(action, 0, self.N_ACTIONS-1) #action values range from (0 to N_ACTIONS-1)\n",
    "        self.atrack = np.append(self.atrack, action+1) #track duty cycles\n",
    "        e_consumed = (action+1)*self.DMAX/self.N_ACTIONS   #energy consumed by the node\n",
    "        \n",
    "        self.batt += (self.henergy - e_consumed)\n",
    "        if(self.batt < 0.02*self.BMAX or self.batt > 0.98*self.BMAX ):\n",
    "            self.violation_flag = True #penalty for violating battery limits everytime it happens\n",
    "            reward = -2\n",
    "        if(self.batt < 0.02*self.BMAX):\n",
    "            reward -= 2\n",
    "            \n",
    "        if(self.violation_flag):\n",
    "            if(self.day_violation_flag == False): #penalty for violating battery limits anytime during the day - triggers once everyday\n",
    "                self.violation_counter += 1\n",
    "                self.day_violation_flag = True\n",
    "            \n",
    "        self.batt = np.clip(self.batt, self.BMIN, self.BMAX) #clip battery values within permitted level\n",
    "        self.btrack = np.append(self.btrack, self.batt) #track battery levels\n",
    "\n",
    "#         self.enp = self.BOPT - self.batt \n",
    "        self.enp = self.binit - self.atrack.sum()*self.DMAX/self.N_ACTIONS\n",
    "        \n",
    "        #proceed to the next time step\n",
    "        self.henergy, self.fcast, day_end, year_end = self.eno.step()\n",
    "        self.henergy = np.clip(self.henergy, self.HMIN, self.HMAX) #clip henergy within HMIN and HMAX\n",
    "                \n",
    "        if(day_end): #if eno object flags that the day has ended then give reward\n",
    "            reward += self.rewardfn()\n",
    "             \n",
    "            if (self.trainmode): #reset battery to optimal level if limits are exceeded when training\n",
    "#                 self.batt = np.random.uniform(self.DMAX*self.eno.TIME_STEPS/self.BMAX,0.8)*self.BMAX\n",
    "#                 if (self.violation_flag):\n",
    "                if np.random.uniform() < HELP : #occasionaly reset the battery\n",
    "                    self.batt = self.BOPT  \n",
    "            \n",
    "            self.day_violation_flag = False\n",
    "            self.binit = self.batt #this will be the new initial battery level for next day\n",
    "            self.btrack = [] #clear battery tracker\n",
    "            self.atrack = [] #clear duty cycle tracker\n",
    "            \n",
    "                    \n",
    "                \n",
    "        norm_batt = self.batt/self.BMAX\n",
    "        norm_enp = self.enp/(self.BMAX/2)\n",
    "        norm_henergy = self.henergy/self.HMAX\n",
    "        norm_fcast = self.fcast/(self.NO_OF_DAYTYPE-1)\n",
    "\n",
    "        c_state = [norm_batt, norm_enp, norm_henergy, norm_fcast] #continuous states\n",
    "        return [c_state, reward, day_end, year_end]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyper Parameters\n",
    "BATCH_SIZE = 32\n",
    "LR = 0.0001          # learning rate\n",
    "EPSILON = 0.9               # greedy policy\n",
    "GAMMA = 0.9                 # reward discount\n",
    "LAMBDA = 0.9                # parameter decay\n",
    "TARGET_REPLACE_ITER = 24*7*4*18    # target update frequency (every two months)\n",
    "MEMORY_CAPACITY     = 24*7*4*12*2      # store upto six month worth of memory   \n",
    "\n",
    "N_ACTIONS = 10 #no. of duty cycles (0,1,2,3,4)\n",
    "N_STATES = 4 #number of state space parameter [batt, enp, henergy, fcast]\n",
    "\n",
    "HIDDEN_LAYER = 50\n",
    "NO_OF_ITERATIONS = 50\n",
    "GPU = False\n",
    "HELP = 0.05"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "\n",
    "#Class definitions for NN model and learning algorithm\n",
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "\n",
    "        self.fc1 = nn.Linear(N_STATES, HIDDEN_LAYER)\n",
    "        nn.init.kaiming_uniform_(self.fc1.weight)\n",
    "        \n",
    "        self.fc2 = nn.Linear(HIDDEN_LAYER, HIDDEN_LAYER)\n",
    "        nn.init.kaiming_uniform_(self.fc2.weight)\n",
    "\n",
    "        self.out = nn.Linear(HIDDEN_LAYER, N_ACTIONS)\n",
    "        nn.init.xavier_uniform_(self.out.weight) \n",
    "\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.fc1(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.fc2(x)\n",
    "        x = F.relu(x)\n",
    "        actions_value = self.out(x)\n",
    "        return actions_value\n",
    "    \n",
    "class DQN(object):\n",
    "    def __init__(self):\n",
    "        if(GPU): \n",
    "            device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "        else:\n",
    "            device = torch.device('cpu')\n",
    "        self.eval_net, self.target_net = Net(), Net()\n",
    "        self.eval_net.to(device)\n",
    "        self.target_net.to(device)\n",
    "        self.device = device\n",
    "#         print(\"Neural net\")\n",
    "#         print(self.eval_net)\n",
    "        self.learn_step_counter = 0                                     # for target updating\n",
    "        self.memory_counter = 0                                         # for storing memory\n",
    "        self.memory = np.zeros((MEMORY_CAPACITY, N_STATES * 2 + 2))     # initialize memory [mem: ([s], a, r, [s_]) ]\n",
    "        self.optimizer = torch.optim.Adam(self.eval_net.parameters(), lr=LR, weight_decay=1e-3)\n",
    "        self.loss_func = nn.MSELoss()\n",
    "        self.nettoggle = False\n",
    "\n",
    "    def choose_action(self, x):\n",
    "        x = torch.unsqueeze(torch.FloatTensor(x), 0)\n",
    "        x = x.to(self.device)\n",
    "\n",
    "        # input only one sample\n",
    "        if np.random.uniform() < EPSILON:   # greedy\n",
    "            actions_value = self.eval_net.forward(x)\n",
    "            actions_value = actions_value.to(torch.device(\"cpu\"))\n",
    "            action = torch.max(actions_value, 1)[1].data.numpy()\n",
    "            action = action[0] # return the argmax index\n",
    "        else:   # random\n",
    "            action = np.random.randint(0, N_ACTIONS)\n",
    "            action = action\n",
    "        return action\n",
    "    \n",
    "    def choose_greedy_action(self, x):\n",
    "        x = torch.unsqueeze(torch.FloatTensor(x), 0)\n",
    "        x = x.to(self.device)\n",
    "\n",
    "        # input only one sample\n",
    "        if True:   # greedy\n",
    "            actions_value = self.eval_net.forward(x)\n",
    "            actions_value = actions_value.to(torch.device(\"cpu\"))\n",
    "            action = torch.max(actions_value, 1)[1].data.numpy()\n",
    "            action = action[0] # return the argmax index\n",
    "        return action\n",
    "\n",
    "    def store_transition(self, s, a, r, s_):\n",
    "        transition = np.hstack((s, [a, r], s_))\n",
    "        # replace the old memory with new memory\n",
    "        index = self.memory_counter % MEMORY_CAPACITY\n",
    "        self.memory[index, :] = transition\n",
    "        self.memory_counter += 1\n",
    "    \n",
    "    def store_day_transition(self, transition_rec):\n",
    "        data = transition_rec\n",
    "        index = self.memory_counter % MEMORY_CAPACITY\n",
    "        self.memory= np.insert(self.memory, index, data,0)\n",
    "        self.memory_counter += transition_rec.shape[0]\n",
    "\n",
    "    def learn(self):\n",
    "        # target parameter update\n",
    "        if self.learn_step_counter % TARGET_REPLACE_ITER == 0:\n",
    "            self.target_net.load_state_dict(self.eval_net.state_dict())\n",
    "            self.nettoggle = not self.nettoggle\n",
    "        self.learn_step_counter += 1\n",
    "\n",
    "        # sample batch transitions\n",
    "        sample_index = np.random.choice(MEMORY_CAPACITY, BATCH_SIZE)\n",
    "        b_memory = self.memory[sample_index, :]\n",
    "        b_s = torch.FloatTensor(b_memory[:, :N_STATES])\n",
    "        b_a = torch.LongTensor(b_memory[:, N_STATES:N_STATES+1].astype(int))\n",
    "        b_r = torch.FloatTensor(b_memory[:, N_STATES+1:N_STATES+2])\n",
    "        b_s_ = torch.FloatTensor(b_memory[:, -N_STATES:])\n",
    "        \n",
    "        b_s = b_s.to(self.device)\n",
    "        b_a = b_a.to(self.device)\n",
    "        b_r = b_r.to(self.device)\n",
    "        b_s_ = b_s_.to(self.device)\n",
    "\n",
    "        # q_eval w.r.t the action in experience\n",
    "        q_eval = self.eval_net(b_s).gather(1, b_a)  # shape (batch, 1)\n",
    "        q_next = self.target_net(b_s_).detach()     # detach from graph, don't backpropagate\n",
    "        q_target = b_r + GAMMA * q_next.max(1)[0].view(BATCH_SIZE, 1)   # shape (batch, 1)\n",
    "        loss = self.loss_func(q_eval, q_target)\n",
    "\n",
    "        self.optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        self.optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def stdize(s):\n",
    "    MU_BATT = 0.5\n",
    "    SD_BATT = 0.15\n",
    "    \n",
    "    MU_ENP = 0\n",
    "    SD_ENP = 0.15\n",
    "    \n",
    "    MU_HENERGY = 0.35\n",
    "    SD_HENERGY = 0.25\n",
    "    \n",
    "    MU_FCAST = 0.42\n",
    "    SD_FCAST = 0.27\n",
    "    \n",
    "    norm_batt, norm_enp, norm_henergy, norm_fcast = s\n",
    "    \n",
    "    std_batt = (norm_batt - MU_BATT)/SD_BATT\n",
    "    std_enp = (norm_enp - MU_ENP)/SD_ENP\n",
    "    std_henergy = (norm_henergy - MU_HENERGY)/SD_HENERGY\n",
    "    std_fcast = (norm_fcast - MU_FCAST)/SD_FCAST\n",
    "\n",
    "\n",
    "    return [std_batt, std_enp, std_henergy, std_fcast]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "TRAINING IN PROGRESS\n",
      "\n",
      "Device:  cpu\n",
      "\n",
      "Iteration 0:  TOKYO, 2008 \n",
      "Average Reward \t\t= -4.267\n",
      "Violation Counter \t= 312\n",
      "\n",
      "Iteration 1:  TOKYO, 2003 \n",
      "Average Reward \t\t= -4.851\n",
      "Violation Counter \t= 328\n",
      "\n",
      "Iteration 2:  TOKYO, 2008 \n",
      "Average Reward \t\t= -4.419\n",
      "Violation Counter \t= 233\n",
      "\n",
      "Iteration 3:  TOKYO, 2002 \n",
      "Average Reward \t\t= -3.991\n",
      "Violation Counter \t= 245\n",
      "\n",
      "Iteration 4:  TOKYO, 2003 \n",
      "Average Reward \t\t= -2.604\n",
      "Violation Counter \t= 185\n",
      "\n",
      "Iteration 5:  TOKYO, 2009 \n",
      "Average Reward \t\t= -0.900\n",
      "Violation Counter \t= 108\n",
      "\n",
      "Iteration 6:  TOKYO, 2007 \n",
      "Average Reward \t\t= 0.124\n",
      "Violation Counter \t= 56\n",
      "\n",
      "Iteration 7:  TOKYO, 2000 \n",
      "Average Reward \t\t= 0.460\n",
      "Violation Counter \t= 31\n",
      "\n",
      "Iteration 8:  TOKYO, 2007 \n",
      "Average Reward \t\t= 0.539\n",
      "Violation Counter \t= 33\n",
      "\n",
      "Iteration 9:  TOKYO, 2007 \n",
      "Average Reward \t\t= 0.913\n",
      "Violation Counter \t= 15\n",
      "\n",
      "Iteration 10:  TOKYO, 2000 \n",
      "Average Reward \t\t= 0.394\n",
      "Violation Counter \t= 41\n",
      "\n",
      "Iteration 11:  TOKYO, 2007 \n",
      "Average Reward \t\t= 0.872\n",
      "Violation Counter \t= 18\n",
      "\n",
      "Iteration 12:  TOKYO, 2000 \n",
      "Average Reward \t\t= 0.792\n",
      "Violation Counter \t= 24\n",
      "\n",
      "Iteration 13:  TOKYO, 2007 \n",
      "Average Reward \t\t= 1.086\n",
      "Violation Counter \t= 8\n",
      "\n",
      "Iteration 14:  TOKYO, 2009 \n",
      "Average Reward \t\t= 1.024\n",
      "Violation Counter \t= 10\n",
      "\n",
      "Iteration 15:  TOKYO, 2001 \n",
      "Average Reward \t\t= 0.938\n",
      "Violation Counter \t= 19\n",
      "\n",
      "Iteration 16:  TOKYO, 2009 \n",
      "Average Reward \t\t= 0.987\n",
      "Violation Counter \t= 15\n",
      "\n",
      "Iteration 17:  TOKYO, 2007 \n",
      "Average Reward \t\t= 0.414\n",
      "Violation Counter \t= 38\n",
      "\n",
      "Iteration 18:  TOKYO, 2000 \n",
      "Average Reward \t\t= 0.399\n",
      "Violation Counter \t= 42\n",
      "\n",
      "Iteration 19:  TOKYO, 2000 \n",
      "Average Reward \t\t= 1.035\n",
      "Violation Counter \t= 13\n",
      "\n",
      "Iteration 20:  TOKYO, 2004 \n",
      "Average Reward \t\t= 1.001\n",
      "Violation Counter \t= 13\n",
      "\n",
      "Iteration 21:  TOKYO, 2008 \n",
      "Average Reward \t\t= 1.104\n",
      "Violation Counter \t= 12\n",
      "\n",
      "Iteration 22:  TOKYO, 2004 \n",
      "Average Reward \t\t= 0.808\n",
      "Violation Counter \t= 22\n",
      "\n",
      "Iteration 23:  TOKYO, 2006 \n",
      "Average Reward \t\t= 0.718\n",
      "Violation Counter \t= 29\n",
      "\n",
      "Iteration 24:  TOKYO, 2007 \n",
      "Average Reward \t\t= 1.106\n",
      "Violation Counter \t= 8\n",
      "\n",
      "Iteration 25:  TOKYO, 2003 \n",
      "Average Reward \t\t= 0.992\n",
      "Violation Counter \t= 15\n",
      "\n",
      "Iteration 26:  TOKYO, 2005 \n",
      "Average Reward \t\t= 1.205\n",
      "Violation Counter \t= 10\n",
      "\n",
      "Iteration 27:  TOKYO, 2007 \n",
      "Average Reward \t\t= 1.121\n",
      "Violation Counter \t= 10\n",
      "\n",
      "Iteration 28:  TOKYO, 2008 \n",
      "Average Reward \t\t= 1.152\n",
      "Violation Counter \t= 7\n",
      "\n",
      "Iteration 29:  TOKYO, 2004 \n",
      "Average Reward \t\t= 1.022\n",
      "Violation Counter \t= 14\n",
      "\n",
      "Iteration 30:  TOKYO, 2006 \n",
      "Average Reward \t\t= 0.735\n",
      "Violation Counter \t= 26\n",
      "\n",
      "Iteration 31:  TOKYO, 2002 \n",
      "Average Reward \t\t= 0.695\n",
      "Violation Counter \t= 26\n",
      "\n",
      "Iteration 32:  TOKYO, 2007 \n",
      "Average Reward \t\t= 1.113\n",
      "Violation Counter \t= 10\n",
      "\n",
      "Iteration 33:  TOKYO, 2000 \n",
      "Average Reward \t\t= 1.042\n",
      "Violation Counter \t= 13\n",
      "\n",
      "Iteration 34:  TOKYO, 2007 \n",
      "Average Reward \t\t= 1.180\n",
      "Violation Counter \t= 5\n",
      "\n",
      "Iteration 35:  TOKYO, 2002 \n",
      "Average Reward \t\t= 0.973\n",
      "Violation Counter \t= 14\n",
      "\n",
      "Iteration 36:  TOKYO, 2003 \n",
      "Average Reward \t\t= 1.023\n",
      "Violation Counter \t= 12\n",
      "\n",
      "Iteration 37:  TOKYO, 2006 \n",
      "Average Reward \t\t= 0.802\n",
      "Violation Counter \t= 23\n",
      "\n",
      "Iteration 38:  TOKYO, 2004 \n",
      "Average Reward \t\t= 1.156\n",
      "Violation Counter \t= 10\n",
      "\n",
      "Iteration 39:  TOKYO, 2008 \n",
      "Average Reward \t\t= 1.112\n",
      "Violation Counter \t= 11\n",
      "\n",
      "Iteration 40:  TOKYO, 2006 \n",
      "Average Reward \t\t= 0.990\n",
      "Violation Counter \t= 14\n",
      "\n",
      "Iteration 41:  TOKYO, 2000 \n",
      "Average Reward \t\t= 1.026\n",
      "Violation Counter \t= 15\n",
      "\n",
      "Iteration 42:  TOKYO, 2000 \n",
      "Average Reward \t\t= 1.005\n",
      "Violation Counter \t= 15\n",
      "\n",
      "Iteration 43:  TOKYO, 2004 \n",
      "Average Reward \t\t= 0.988\n",
      "Violation Counter \t= 17\n",
      "\n",
      "Iteration 44:  TOKYO, 2008 \n",
      "Average Reward \t\t= 1.105\n",
      "Violation Counter \t= 11\n",
      "\n",
      "Iteration 45:  TOKYO, 2007 \n",
      "Average Reward \t\t= 1.069\n",
      "Violation Counter \t= 9\n",
      "\n",
      "Iteration 46:  TOKYO, 2009 \n",
      "Average Reward \t\t= 0.940\n",
      "Violation Counter \t= 18\n",
      "\n",
      "Iteration 47:  TOKYO, 2003 \n",
      "Average Reward \t\t= 1.024\n",
      "Violation Counter \t= 13\n",
      "\n",
      "Iteration 48:  TOKYO, 2007 \n",
      "Average Reward \t\t= 1.168\n",
      "Violation Counter \t= 7\n",
      "\n",
      "Iteration 49:  TOKYO, 2009 \n",
      "Average Reward \t\t= 1.133\n",
      "Violation Counter \t= 6\n"
     ]
    }
   ],
   "source": [
    "#TRAIN \n",
    "dqn = DQN()\n",
    "# for recording weights\n",
    "oldfc1 = dqn.eval_net.fc1.weight.data.cpu().numpy().flatten()\n",
    "old2fc1 = oldfc1\n",
    "\n",
    "oldfc2 = dqn.eval_net.fc2.weight.data.cpu().numpy().flatten()\n",
    "old2fc2 = oldfc2\n",
    "\n",
    "# oldfc3 = dqn.eval_net.fc3.weight.data.cpu().numpy().flatten()\n",
    "# old2fc3 = oldfc3\n",
    "\n",
    "oldout = dqn.eval_net.out.weight.data.cpu().numpy().flatten()\n",
    "old2out = oldout\n",
    "########################################\n",
    "\n",
    "best_iteration = -1\n",
    "best_avg_reward = -1000 #initialize best average reward to very low value\n",
    "reset_counter = 0 #count number of times the battery had to be reset\n",
    "change_hr = 0\n",
    "# PFILENAME = ''.join(random.choice(string.ascii_uppercase + string.digits) for _ in range(8)) #create random filename\n",
    "# BFILENAME = \"best\"+PFILENAME + \".pt\" #this file stores the best model\n",
    "# TFILENAME = \"terminal\"+PFILENAME + \".pt\" #this file stores the last model\n",
    "\n",
    "avg_reward_rec = [] #record the yearly average rewards over the entire duration of training\n",
    "violation_rec = []\n",
    "print('\\nTRAINING IN PROGRESS\\n')\n",
    "print('Device: ', dqn.device)\n",
    "\n",
    "for iteration in range(NO_OF_ITERATIONS):\n",
    "    counter = iteration%10\n",
    "    EPSILON = 0.5*counter/(counter+1) + 0.5#sawtooth learning rate for disruptive learning\n",
    "    print('EPSILON = {:.2}'.format(EPSILON))\n",
    "    \n",
    "    LOCATION = 'tokyo'#random.choice(['tokyo','wakkanai','minamidaito'])\n",
    "    YEAR = random.choice(np.arange(2000,2010))\n",
    "    capm = CAPM(LOCATION,YEAR,shuffle=False, trainmode=False) #instantiate the CAPM class\n",
    "    capm.eno = ENO(LOCATION,YEAR, shuffle=False, day_balance=False) #instantiate the environment inside the CAPM class\n",
    "    capm.HMAX = capm.eno.SMAX #maximum power output of solar cell is set in CAPM object using the value in ENO object\n",
    "#     clear_output()\n",
    "    print('\\nIteration {}:  {}, {} '.format(iteration, LOCATION.upper(), YEAR))\n",
    "\n",
    "    s, r, day_end, year_end = capm.reset()\n",
    "    yr_record = np.empty(4)\n",
    "\n",
    "    record = np.empty(4) #record for battery, henergy, reward and action\n",
    "    transition_rec = np.zeros((capm.eno.TIME_STEPS, N_STATES * 2 + 2)) #record all the transition in one day\n",
    "\n",
    "    while True:\n",
    "        a = dqn.choose_action(stdize(s))\n",
    "\n",
    "        # present state = [batt, enp, henergy]\n",
    "        record = np.vstack((record, [s[0],s[2],r, a])) # record battery, henergy, reward and action for troubleshooting\n",
    "        yr_record = np.vstack((yr_record, [s[0],s[2],r, a]))\n",
    "\n",
    "        # take action\n",
    "        s_, r, day_end, year_end = capm.step(a)\n",
    "        \n",
    "        temp_transitions = np.hstack((stdize(s), [a, r], stdize(s_)))\n",
    "        transition_rec[capm.eno.hr-1,:] = temp_transitions\n",
    "\n",
    "        if (day_end):\n",
    "            transition_rec[:,5] += r #broadcast reward to all states\n",
    "            decay_factor = [i for i in (LAMBDA**n for n in reversed(range(0, capm.eno.TIME_STEPS)))]\n",
    "            transition_rec[:,5] = transition_rec[:,5] * decay_factor #decay reward proportionately\n",
    "            dqn.store_day_transition(transition_rec)\n",
    "\n",
    "        if dqn.memory_counter > MEMORY_CAPACITY:\n",
    "            dqn.learn()\n",
    "\n",
    "        if dqn.nettoggle:\n",
    "            change_hr = capm.eno.day*24+capm.eno.hr #to mark when the DQN is updated.\n",
    "            dqn.nettoggle = not dqn.nettoggle\n",
    "\n",
    "        if (year_end):\n",
    "            break\n",
    "\n",
    "        # transition to new state\n",
    "        s = s_\n",
    "\n",
    "    record = np.delete(record, 0, 0) #remove the first row which is garbage\n",
    "    reward_rec = record[:,2] #extract reward information from the record array\n",
    "    reward_rec = reward_rec[::24] #only consider terminal rewards\n",
    "    print(\"Average Reward \\t\\t= {:.3f}\".format(np.mean(reward_rec)))\n",
    "    print(\"Violation Counter \\t= {}\".format(capm.violation_counter))\n",
    "\n",
    "#     if(best_avg_reward < np.mean(reward_rec)):\n",
    "#         best_avg_reward = np.mean(reward_rec)\n",
    "    \n",
    "#     if(best_avg_reward > 1.5 or iteration > 20):\n",
    "#         EPSILON = 0.9\n",
    "#         LR = 0.01\n",
    "        \n",
    "#     if (capm.violation_counter < 5):\n",
    "#         reset_flag = False\n",
    "#         EPSILON = 0.95\n",
    "#         LR = 0.001\n",
    "        \n",
    "\n",
    "#     # Check if reward beats the High Score and possible save it    \n",
    "#     if (iteration > 19): #save the best models only after 20 iterations\n",
    "#         print(\"Best Score \\t = {:8.3f} @ Iteration No. {}\".format(best_avg_reward, best_iteration))\n",
    "#         if(best_avg_reward < np.mean(reward_rec)):\n",
    "#             best_iteration = iteration\n",
    "#             best_avg_reward = np.mean(reward_rec)\n",
    "#             print(\"Saving Model\")\n",
    "#             torch.save(dqn.eval_net.state_dict(), BFILENAME)\n",
    "#     else:\n",
    "#         print(\"\\r\")\n",
    "\n",
    "    # Log the average reward in avg_reward_rec\n",
    "    avg_reward_rec = np.append(avg_reward_rec, np.mean(reward_rec))\n",
    "    violation_rec = np.append(violation_rec, capm.violation_counter)\n",
    "\n",
    "    \n",
    "###########################################################################################\n",
    "# #   PLOT battery levels, hourly rewards and the weights\n",
    "#     yr_record = np.delete(yr_record, 0, 0) #remove the first row which is garbage\n",
    "# #     NO_OF_DAYS = capm.eno.NO_OF_DAYS\n",
    "#     hourly_yr_reward_rec = yr_record[:,2]\n",
    "#     yr_reward_rec = hourly_yr_reward_rec[::24]\n",
    "\n",
    "    \n",
    "#     fig = plt.figure(figsize=(24,3))\n",
    "#     TIME_STEPS = capm.eno.TIME_STEPS\n",
    "#     NO_OF_DAYS = capm.eno.NO_OF_DAYS\n",
    "#     DAY_SPACING = 15\n",
    "#     TICK_SPACING = TIME_STEPS*DAY_SPACING\n",
    "#     #plot battery\n",
    "#     ax = fig.add_subplot(111)\n",
    "#     ax.plot(np.arange(0,TIME_STEPS*NO_OF_DAYS),yr_record[:,0],'r')\n",
    "#     ax.set_ylim([0,1])\n",
    "#     ax.axvline(x=change_hr)\n",
    "#     ax.xaxis.set_major_locator(ticker.MultipleLocator(TICK_SPACING))\n",
    "# #     labels = [item for item in ax.get_xticklabels()]\n",
    "# #     print(labels)\n",
    "# #     labels [15:-1] = np.arange(0,NO_OF_DAYS,DAY_SPACING) #the first label is reserved to negative values\n",
    "# #     ax.set_xticklabels(labels)\n",
    "#     #plot hourly reward\n",
    "#     ax0 = ax.twinx()\n",
    "#     ax0.plot(hourly_yr_reward_rec, color='m')\n",
    "#     ax0.set_ylim(-7,3)\n",
    "#     plt.show()\n",
    "\n",
    "\n",
    "#     fig = plt.figure(figsize=(18,3))\n",
    "#     ax1 = fig.add_subplot(131)\n",
    "#     newfc1 = dqn.eval_net.fc1.weight.data.cpu().numpy().flatten()\n",
    "#     ax1.plot(old2fc1,color='b', alpha=0.4)\n",
    "#     ax1.plot(oldfc1,color='b',alpha = 0.7)\n",
    "#     ax1.plot(newfc1,color='b')\n",
    "#     old2fc1 = oldfc1\n",
    "#     oldfc1 = newfc1\n",
    "    \n",
    "#     ax2 = fig.add_subplot(132)\n",
    "#     newfc2 = dqn.eval_net.fc2.weight.data.cpu().numpy().flatten()\n",
    "#     ax2.plot(old2fc2,color='y', alpha=0.4)\n",
    "#     ax2.plot(oldfc2,color='y',alpha = 0.7)\n",
    "#     ax2.plot(newfc2,color='y')\n",
    "#     old2fc2 = oldfc2\n",
    "#     oldfc2 = newfc2\n",
    "    \n",
    "# #     ax3 = fig.add_subplot(143)\n",
    "# #     newfc3 = dqn.eval_net.fc3.weight.data.cpu().numpy().flatten()\n",
    "# #     ax3.plot(old2fc3,color='y', alpha=0.4)\n",
    "# #     ax3.plot(oldfc3,color='y',alpha = 0.7)\n",
    "# #     ax3.plot(newfc3,color='y')\n",
    "# #     old2fc3 = oldfc3\n",
    "# #     oldfc3 = newfc3\n",
    "    \n",
    "#     axO = fig.add_subplot(133)\n",
    "#     newout = dqn.eval_net.out.weight.data.cpu().numpy().flatten()\n",
    "#     axO.plot(old2out,color='g', alpha=0.4)\n",
    "#     axO.plot(oldout,color='g',alpha=0.7)\n",
    "#     axO.plot(newout,color='g')\n",
    "#     old2out = oldout\n",
    "#     oldout = newout\n",
    "    \n",
    "#     fig.tight_layout()\n",
    "#     plt.show()\n",
    "\n",
    "\n",
    "    # End of training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 648x216 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig = plt.figure(figsize=(9,3))\n",
    "# Plot the average reward log\n",
    "\n",
    "ax1 = fig.add_subplot(111)\n",
    "ax1.set_ylabel(\"Reward\")\n",
    "# ax1.set_ylim([-3,3]);\n",
    "ax1.plot(avg_reward_rec,'b')\n",
    "ax1.tick_params(axis='y', colors='b')\n",
    "\n",
    "#Plot the violation record log\n",
    "ax2 = ax1.twinx()\n",
    "ax2.set_ylabel(\"Violations\",color = 'r')\n",
    "ax2.plot(violation_rec,'r')\n",
    "for xpt in np.argwhere(violation_rec<1):\n",
    "    ax2.axvline(x=xpt,color='g')\n",
    "ax2.set_ylim([0,50]);\n",
    "ax2.tick_params(axis='y', colors='r')\n",
    "\n",
    "fig.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device:  cpu\n",
      "runtime: 0:13:40.918165\n"
     ]
    }
   ],
   "source": [
    "print('Device: ', dqn.device)\n",
    "print('runtime: {}'.format(datetime.now() - tic))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPSILON =  0.9\n",
      "LR      =  0.0001\n",
      "\n",
      "LAST PHASE ITERATION #0:  TOKYO, 2000 \n",
      "Average Reward \t\t= 1.010\n",
      "Violation Counter \t= 10\n",
      "***MEASURING PERFORMANCE OF THE MODEL***\n",
      "\tBEST AVERAGE ANNUAL AVERAGE REWARD = -1000.000\n",
      "\tBEST TOTAL VIOLATIONS              = 1000\n",
      "\n",
      "\tAverage Annual Average Reward      = 1.416\n",
      "\tTotal Violations                   = 0.0\n",
      "****************************************\n",
      "\n",
      "LAST PHASE ITERATION #1:  TOKYO, 2004 \n",
      "Average Reward \t\t= 1.359\n",
      "Violation Counter \t= 1\n",
      "***MEASURING PERFORMANCE OF THE MODEL***\n",
      "\tBEST AVERAGE ANNUAL AVERAGE REWARD = 1.416\n",
      "\tBEST TOTAL VIOLATIONS              = 0.0\n",
      "\n",
      "\tAverage Annual Average Reward      = 1.278\n",
      "\tTotal Violations                   = 0.0\n",
      "****************************************\n",
      "\n",
      "LAST PHASE ITERATION #2:  TOKYO, 2008 \n",
      "Average Reward \t\t= 1.388\n",
      "Violation Counter \t= 0\n",
      "***MEASURING PERFORMANCE OF THE MODEL***\n",
      "\tBEST AVERAGE ANNUAL AVERAGE REWARD = 1.416\n",
      "\tBEST TOTAL VIOLATIONS              = 0.0\n",
      "\n",
      "\tAverage Annual Average Reward      = 1.410\n",
      "\tTotal Violations                   = 0.0\n",
      "****************************************\n",
      "\n",
      "LAST PHASE ITERATION #3:  TOKYO, 2001 \n",
      "Average Reward \t\t= 1.419\n",
      "Violation Counter \t= 0\n",
      "***MEASURING PERFORMANCE OF THE MODEL***\n",
      "\tBEST AVERAGE ANNUAL AVERAGE REWARD = 1.416\n",
      "\tBEST TOTAL VIOLATIONS              = 0.0\n",
      "\n",
      "\tAverage Annual Average Reward      = 1.365\n",
      "\tTotal Violations                   = 0.0\n",
      "****************************************\n",
      "\n",
      "LAST PHASE ITERATION #4:  TOKYO, 2008 \n",
      "Average Reward \t\t= 1.419\n",
      "Violation Counter \t= 0\n",
      "***MEASURING PERFORMANCE OF THE MODEL***\n",
      "\tBEST AVERAGE ANNUAL AVERAGE REWARD = 1.416\n",
      "\tBEST TOTAL VIOLATIONS              = 0.0\n",
      "\n",
      "\tAverage Annual Average Reward      = 1.403\n",
      "\tTotal Violations                   = 0.0\n",
      "****************************************\n",
      "\n",
      "LAST PHASE ITERATION #5:  TOKYO, 2005 \n",
      "Average Reward \t\t= 1.453\n",
      "Violation Counter \t= 0\n",
      "***MEASURING PERFORMANCE OF THE MODEL***\n",
      "\tBEST AVERAGE ANNUAL AVERAGE REWARD = 1.416\n",
      "\tBEST TOTAL VIOLATIONS              = 0.0\n",
      "\n",
      "\tAverage Annual Average Reward      = 1.358\n",
      "\tTotal Violations                   = 0.0\n",
      "****************************************\n",
      "\n",
      "LAST PHASE ITERATION #6:  TOKYO, 2003 \n",
      "Average Reward \t\t= 1.254\n",
      "Violation Counter \t= 5\n",
      "***MEASURING PERFORMANCE OF THE MODEL***\n",
      "\tBEST AVERAGE ANNUAL AVERAGE REWARD = 1.416\n",
      "\tBEST TOTAL VIOLATIONS              = 0.0\n",
      "\n",
      "\tAverage Annual Average Reward      = 1.423\n",
      "\tTotal Violations                   = 0.0\n",
      "****************************************\n",
      "\n",
      "LAST PHASE ITERATION #7:  TOKYO, 2008 \n",
      "Average Reward \t\t= 1.401\n",
      "Violation Counter \t= 0\n",
      "***MEASURING PERFORMANCE OF THE MODEL***\n",
      "\tBEST AVERAGE ANNUAL AVERAGE REWARD = 1.423\n",
      "\tBEST TOTAL VIOLATIONS              = 0.0\n",
      "\n",
      "\tAverage Annual Average Reward      = 1.349\n",
      "\tTotal Violations                   = 0.0\n",
      "****************************************\n",
      "\n",
      "LAST PHASE ITERATION #8:  TOKYO, 2005 \n",
      "Average Reward \t\t= 1.450\n",
      "Violation Counter \t= 1\n",
      "***MEASURING PERFORMANCE OF THE MODEL***\n",
      "\tBEST AVERAGE ANNUAL AVERAGE REWARD = 1.423\n",
      "\tBEST TOTAL VIOLATIONS              = 0.0\n",
      "\n",
      "\tAverage Annual Average Reward      = 1.402\n",
      "\tTotal Violations                   = 0.0\n",
      "****************************************\n",
      "\n",
      "LAST PHASE ITERATION #9:  TOKYO, 2003 \n",
      "Average Reward \t\t= 1.379\n",
      "Violation Counter \t= 1\n",
      "***MEASURING PERFORMANCE OF THE MODEL***\n",
      "\tBEST AVERAGE ANNUAL AVERAGE REWARD = 1.423\n",
      "\tBEST TOTAL VIOLATIONS              = 0.0\n",
      "\n",
      "\tAverage Annual Average Reward      = -0.302\n",
      "\tTotal Violations                   = 335.0\n",
      "****************************************\n"
     ]
    }
   ],
   "source": [
    "#END OF TRAINING PHASE - CHOOSING THE BEST MODEL INSTANCE\n",
    "#INCREASE GREEDY RATE\n",
    "#VALIDATE AFTER EVERY ITERATION\n",
    "\n",
    "# Use this model and its output as base standards for the last phase of training\n",
    "best_avg_avg_reward = -1000\n",
    "best_net_avg_reward = dqn.eval_net\n",
    "best_avg_v_counter = 1000\n",
    "best_net_v_counter = dqn.eval_net\n",
    "\n",
    "\n",
    "NO_OF_LAST_PHASE_ITERATIONS = 10\n",
    "EPSILON = 0.95\n",
    "print(\"EPSILON = \", EPSILON)\n",
    "print(\"LR      = \", LR)\n",
    "\n",
    "for iteration in range(NO_OF_LAST_PHASE_ITERATIONS):\n",
    "    LOCATION = 'tokyo'#random.choice(['tokyo','wakkanai','minamidaito'])\n",
    "    YEAR = random.choice(np.arange(2000,2010))\n",
    "    capm = CAPM(LOCATION,YEAR,shuffle=False, trainmode=False) #instantiate the CAPM class\n",
    "    capm.eno = ENO(LOCATION,YEAR, shuffle=False, day_balance=False) #instantiate the environment inside the CAPM class\n",
    "    capm.HMAX = capm.eno.SMAX #maximum power output of solar cell is set in CAPM object using the value in ENO object\n",
    "\n",
    "    print('\\nLAST PHASE ITERATION #{}:  {}, {} '.format(iteration, LOCATION.upper(), YEAR))\n",
    "    \n",
    "    \n",
    "    my_avg_reward = -1000\n",
    "    my_v_counter = 1000\n",
    "    \n",
    "    s, r, day_end, year_end = capm.reset()\n",
    "    yr_record = np.empty(4)\n",
    "\n",
    "    record = np.empty(4) #record for battery, henergy, reward and action\n",
    "    transition_rec = np.zeros((capm.eno.TIME_STEPS, N_STATES * 2 + 2)) #record all the transition in one day\n",
    "\n",
    "    while True:\n",
    "        a = dqn.choose_action(stdize(s))\n",
    "\n",
    "        # present state = [batt, enp, henergy]\n",
    "        record = np.vstack((record, [s[0],s[2],r, a])) # record battery, henergy, reward and action for troubleshooting\n",
    "        yr_record = np.vstack((yr_record, [s[0],s[2],r, a]))\n",
    "\n",
    "        # take action\n",
    "        s_, r, day_end, year_end = capm.step(a)\n",
    "        \n",
    "        temp_transitions = np.hstack((stdize(s), [a, r], stdize(s_)))\n",
    "        transition_rec[capm.eno.hr-1,:] = temp_transitions\n",
    "\n",
    "        if (day_end):\n",
    "            transition_rec[:,5] += r #broadcast reward to all states\n",
    "            decay_factor = [i for i in (LAMBDA**n for n in reversed(range(0, capm.eno.TIME_STEPS)))]\n",
    "            transition_rec[:,5] = transition_rec[:,5] * decay_factor #decay reward proportionately\n",
    "            dqn.store_day_transition(transition_rec)\n",
    "\n",
    "        if dqn.memory_counter > MEMORY_CAPACITY:\n",
    "            dqn.learn()\n",
    "\n",
    "        if dqn.nettoggle:\n",
    "            change_hr = capm.eno.day*24+capm.eno.hr #to mark when the DQN is updated.\n",
    "            dqn.nettoggle = not dqn.nettoggle\n",
    "\n",
    "        if (year_end):\n",
    "            break\n",
    "\n",
    "        # transition to new state\n",
    "        s = s_\n",
    "\n",
    "    record = np.delete(record, 0, 0) #remove the first row which is garbage\n",
    "    reward_rec = record[:,2] #extract reward information from the record array\n",
    "    reward_rec = reward_rec[::24] #only consider terminal rewards\n",
    "    print(\"Average Reward \\t\\t= {:.3f}\".format(np.mean(reward_rec)))\n",
    "    print(\"Violation Counter \\t= {}\".format(capm.violation_counter))\n",
    "    \n",
    "    \n",
    "    print(\"***MEASURING PERFORMANCE OF THE MODEL***\")\n",
    "    print(\"\\tBEST AVERAGE ANNUAL AVERAGE REWARD = {:.3f}\".format(best_avg_avg_reward))\n",
    "    print(\"\\tBEST TOTAL VIOLATIONS              = {}\".format(best_avg_v_counter))\n",
    "    LOCATION = 'tokyo'\n",
    "    results = np.empty(3)\n",
    "    for YEAR in np.arange(2010,2015):\n",
    "        capm = CAPM(LOCATION,YEAR,shuffle=False, trainmode=False) #instantiate the CAPM class\n",
    "        capm.eno = ENO(LOCATION,YEAR, shuffle=False, day_balance=False) #instantiate the environment inside the CAPM class\n",
    "        capm.HMAX = capm.eno.SMAX #maximum power output of solar cell is set in CAPM object using the value in ENO object\n",
    "\n",
    "        s, r, day_end, year_end = capm.reset()\n",
    "        yr_test_record = np.empty(4)\n",
    "        EPSILON = 1\n",
    "\n",
    "        while True:\n",
    "            a = dqn.choose_greedy_action(stdize(s))\n",
    "            #state = [batt, enp, henergy, fcast]\n",
    "            yr_test_record = np.vstack((yr_test_record, [s[0],s[2],r, a])) #record battery, henergy, reward and action\n",
    "            # take action\n",
    "            s_, r, day_end, year_end = capm.step(a)\n",
    "            if year_end:\n",
    "                break\n",
    "            s = s_\n",
    "\n",
    "        yr_test_record = np.delete(yr_test_record, 0, 0) #remove the first row which is garbage\n",
    "        yr_test_reward_rec = yr_test_record[:,2]\n",
    "        yr_test_reward_rec = yr_test_reward_rec[::24] #annual average reward\n",
    "        results = np.vstack((results, [int(YEAR), np.mean(yr_test_reward_rec), int(capm.violation_counter)]))\n",
    "    results = np.delete(results,0,0)\n",
    "    my_avg_reward = np.mean(results[:,1]) #the average of annual average rewards\n",
    "    my_v_counter = np.sum(results[:,-1]) #total sum of violations\n",
    "    print(\"\\n\\tAverage Annual Average Reward      = {:.3f}\".format(my_avg_reward))\n",
    "    print(\"\\tTotal Violations                   = {}\".format(my_v_counter))\n",
    "\n",
    "    if (my_avg_reward > best_avg_avg_reward):\n",
    "            best_avg_avg_reward = my_avg_reward\n",
    "            best_net_avg_reward = dqn.eval_net\n",
    "\n",
    "    if (my_v_counter < best_avg_v_counter):\n",
    "        best_avg_v_counter = my_v_counter\n",
    "        best_net_v_counter = dqn.eval_net\n",
    "    elif (my_v_counter == best_avg_v_counter):\n",
    "        if (my_avg_reward > best_avg_avg_reward):\n",
    "            best_avg_v_counter = my_v_counterO\n",
    "            best_net_v_counter = dqn.eval_net\n",
    "    print(\"****************************************\")\n",
    "\n",
    "    \n",
    "\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TESTING BEST MODEL BASED ON AVERAGE REWARD METRIC\n",
      "YEAR\t\tAVG_RWD\t\tVIOLATIONS\n",
      "2011 \t\t 1.19 \t\t 10\n",
      "2012 \t\t -0.12 \t\t 60\n",
      "2013 \t\t -1.03 \t\t 96\n",
      "2014 \t\t -0.44 \t\t 70\n",
      "2015 \t\t -0.42 \t\t 72\n",
      "2016 \t\t -1.77 \t\t 124\n",
      "2017 \t\t -1.3 \t\t 108\n",
      "2018 \t\t -2.35 \t\t 146\n"
     ]
    }
   ],
   "source": [
    "#TESTING BEST MODEL BASED ON AVERAGE REWARD METRIC\n",
    "dqn = DQN()\n",
    "dqn.eval_net = best_net_avg_reward\n",
    "LOCATION = 'tokyo'\n",
    "results = np.empty(3)\n",
    "for YEAR in np.arange(2011,2019):\n",
    "    capm = CAPM(LOCATION,YEAR,shuffle=False, trainmode=False) #instantiate the CAPM class\n",
    "    capm.eno = ENO(LOCATION,YEAR, shuffle=False, day_balance=False) #instantiate the environment inside the CAPM class\n",
    "    capm.HMAX = capm.eno.SMAX #maximum power output of solar cell is set in CAPM object using the value in ENO object\n",
    "\n",
    "    s, r, day_end, year_end = capm.reset()\n",
    "    yr_test_record = np.empty(4)\n",
    "    EPSILON = 1\n",
    "\n",
    "    while True:\n",
    "        a = dqn.choose_greedy_action(stdize(s))\n",
    "\n",
    "        #state = [batt, enp, henergy, fcast]\n",
    "        yr_test_record = np.vstack((yr_test_record, [s[0],s[2],r, a])) #record battery, henergy, reward and action\n",
    "\n",
    "        # take action\n",
    "        s_, r, day_end, year_end = capm.step(a)\n",
    "\n",
    "        if year_end:\n",
    "            break\n",
    "\n",
    "        s = s_\n",
    "\n",
    "    yr_test_record = np.delete(yr_test_record, 0, 0) #remove the first row which is garbage\n",
    "\n",
    "#     #Plot the reward and battery for the entire year run\n",
    "#     title = LOCATION.upper() + ',' + str(YEAR)\n",
    "\n",
    "#     NO_OF_DAYS = capm.eno.NO_OF_DAYS\n",
    "    yr_test_reward_rec = yr_test_record[:,2]\n",
    "    yr_test_reward_rec = yr_test_reward_rec[::24]\n",
    "#     print('Average Reward for',title, '=', np.mean(yr_test_reward_rec))\n",
    "#     print('Violations for',title, '=', capm.violation_counter)\n",
    "    \n",
    "    results = np.vstack((results, [int(YEAR), np.mean(yr_test_reward_rec), int(capm.violation_counter)]))\n",
    "\n",
    "#     fig = plt.figure(figsize=(24,3))\n",
    "#     fig.suptitle(title, fontsize=15)\n",
    "\n",
    "#     #     ax1 = fig.add_subplot(211)\n",
    "#     #     ax1.plot(yr_test_reward_rec)\n",
    "#     #     ax1.set_title(\"\\n\\nYear Run Reward\")\n",
    "#     #     ax1.set_ylim([-3,1])\n",
    "\n",
    "#     ax2 = fig.add_subplot(111)\n",
    "#     ax2.plot(yr_test_record[:,0],'r')\n",
    "#     ax2.set_title(\"\\n\\nYear Run Battery\")\n",
    "#     ax2.set_ylim([0,1])\n",
    "#     plt.sca(ax2)\n",
    "#     plt.xticks(np.arange(0, NO_OF_DAYS*24, 50*24),np.arange(0,NO_OF_DAYS,50))\n",
    "\n",
    "#     fig.tight_layout()\n",
    "#     plt.show()\n",
    "\n",
    "results = np.delete(results,0,0)\n",
    "\n",
    "print(\"TESTING BEST MODEL BASED ON AVERAGE REWARD METRIC\")\n",
    "print('YEAR\\t\\tAVG_RWD\\t\\tVIOLATIONS')\n",
    "for x in np.arange(0,results.shape[0]):\n",
    "    print('{} \\t\\t {} \\t\\t {}'.format(int(results[x,0]), np.around(results[x,1],2), int(results[x,-1])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TESTING BASED ON VIOLATION COUNTER METRIC\n",
      "YEAR\t\tAVG_RWD\t\tVIOLATIONS\n",
      "2011 \t\t 1.19 \t\t 10\n",
      "2012 \t\t -0.12 \t\t 60\n",
      "2013 \t\t -1.03 \t\t 96\n",
      "2014 \t\t -0.44 \t\t 70\n",
      "2015 \t\t -0.42 \t\t 72\n",
      "2016 \t\t -1.77 \t\t 124\n",
      "2017 \t\t -1.3 \t\t 108\n",
      "2018 \t\t -2.35 \t\t 146\n"
     ]
    }
   ],
   "source": [
    "#TESTING BASED ON VIOLATION COUNTER METRIC\n",
    "dqn = DQN()\n",
    "dqn.eval_net = best_net_v_counter\n",
    "\n",
    "LOCATION = 'tokyo'\n",
    "results = np.empty(3)\n",
    "for YEAR in np.arange(2011,2019):\n",
    "    capm = CAPM(LOCATION,YEAR,shuffle=False, trainmode=False) #instantiate the CAPM class\n",
    "    capm.eno = ENO(LOCATION,YEAR, shuffle=False, day_balance=False) #instantiate the environment inside the CAPM class\n",
    "    capm.HMAX = capm.eno.SMAX #maximum power output of solar cell is set in CAPM object using the value in ENO object\n",
    "\n",
    "    s, r, day_end, year_end = capm.reset()\n",
    "    yr_test_record = np.empty(4)\n",
    "    EPSILON = 1\n",
    "\n",
    "    while True:\n",
    "        a = dqn.choose_greedy_action(stdize(s))\n",
    "\n",
    "        #state = [batt, enp, henergy, fcast]\n",
    "        yr_test_record = np.vstack((yr_test_record, [s[0],s[2],r, a])) #record battery, henergy, reward and action\n",
    "\n",
    "        # take action\n",
    "        s_, r, day_end, year_end = capm.step(a)\n",
    "\n",
    "        if year_end:\n",
    "            break\n",
    "\n",
    "        s = s_\n",
    "\n",
    "    yr_test_record = np.delete(yr_test_record, 0, 0) #remove the first row which is garbage\n",
    "\n",
    "#     #Plot the reward and battery for the entire year run\n",
    "#     title = LOCATION.upper() + ',' + str(YEAR)\n",
    "\n",
    "#     NO_OF_DAYS = capm.eno.NO_OF_DAYS\n",
    "    yr_test_reward_rec = yr_test_record[:,2]\n",
    "    yr_test_reward_rec = yr_test_reward_rec[::24]\n",
    "#     print('Average Reward for',title, '=', np.mean(yr_test_reward_rec))\n",
    "#     print('Violations for',title, '=', capm.violation_counter)\n",
    "    \n",
    "    results = np.vstack((results, [int(YEAR), np.mean(yr_test_reward_rec), int(capm.violation_counter)]))\n",
    "\n",
    "#     fig = plt.figure(figsize=(24,3))\n",
    "#     fig.suptitle(title, fontsize=15)\n",
    "\n",
    "#     #     ax1 = fig.add_subplot(211)\n",
    "#     #     ax1.plot(yr_test_reward_rec)\n",
    "#     #     ax1.set_title(\"\\n\\nYear Run Reward\")\n",
    "#     #     ax1.set_ylim([-3,1])\n",
    "\n",
    "#     ax2 = fig.add_subplot(111)\n",
    "#     ax2.plot(yr_test_record[:,0],'r')\n",
    "#     ax2.set_title(\"\\n\\nYear Run Battery\")\n",
    "#     ax2.set_ylim([0,1])\n",
    "#     plt.sca(ax2)\n",
    "#     plt.xticks(np.arange(0, NO_OF_DAYS*24, 50*24),np.arange(0,NO_OF_DAYS,50))\n",
    "\n",
    "#     fig.tight_layout()\n",
    "#     plt.show()\n",
    "\n",
    "results = np.delete(results,0,0)\n",
    "\n",
    "print(\"TESTING BASED ON VIOLATION COUNTER METRIC\")\n",
    "print('YEAR\\t\\tAVG_RWD\\t\\tVIOLATIONS')\n",
    "for x in np.arange(0,results.shape[0]):\n",
    "    print('{} \\t\\t {} \\t\\t {}'.format(int(results[x,0]), np.around(results[x,1],2), int(results[x,-1])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #Plot the reward and battery for the entire year run on a day by day basis\n",
    "# title = LOCATION.upper() + ',' + str(YEAR)\n",
    "# TIME_AXIS = np.arange(0,capm.eno.TIME_STEPS)\n",
    "# for DAY in range(0,10):#capm.eno.NO_OF_DAYS):\n",
    "#     START = DAY*24\n",
    "#     END = START+24\n",
    "\n",
    "#     daytitle = title + ' - DAY ' + str(DAY)\n",
    "#     fig = plt.figure(figsize=(16,4))\n",
    "#     st = fig.suptitle(daytitle)\n",
    "\n",
    "#     ax2 = fig.add_subplot(121)\n",
    "#     ax2.plot(yr_test_record[START:END,1],'g')\n",
    "#     ax2.set_title(\"HARVESTED ENERGY\")\n",
    "#     plt.xlabel(\"Hour\")\n",
    "#     ax2.set_ylim([0,1])\n",
    "\n",
    "#     #plot battery for year run\n",
    "#     ax1 = fig.add_subplot(122)\n",
    "#     ax1.plot(TIME_AXIS,yr_test_record[START:END,0],'r') \n",
    "# #     ax1.plot(TIME_AXIS, np.ones(capm.eno.TIME_STEPS)*capm.BOPT/capm.BMAX,'r--')\n",
    "#     ax1.plot(TIME_AXIS, np.ones(capm.eno.TIME_STEPS)*capm.BOPT/capm.BMAX,'r--')\n",
    "#     ax1.text(0.1, 0.2, \"BINIT = %.2f\\n\" %(yr_test_record[START,0]),fontsize=11, ha='left')\n",
    "#     ax1.text(0.1, 0.4, \"TENP = %.2f\\n\" %(capm.BOPT/capm.BMAX-yr_test_record[END,0]),fontsize=11, ha='left')\n",
    "#     ax1.text(0.1, 0.3, \"BMEAN = %.2f\\n\" %(np.mean(yr_test_record[START:END,0])),fontsize=11, ha='left')\n",
    "\n",
    "\n",
    "\n",
    "#     ax1.set_title(\"YEAR RUN TEST\")\n",
    "#     if END < (capm.eno.NO_OF_DAYS*capm.eno.TIME_STEPS):\n",
    "#         ax1.text(0.1, 0, \"REWARD = %.2f\\n\" %(yr_test_record[END,2]),fontsize=13, ha='left')\n",
    "#     plt.xlabel(\"Hour\")\n",
    "#     ax1.set_ylabel('Battery', color='r',fontsize=12)\n",
    "#     ax1.set_ylim([0,1])\n",
    "\n",
    "#     #plot actions for year run\n",
    "#     ax1a = ax1.twinx()\n",
    "#     ax1a.plot(yr_test_record[START:END,3])\n",
    "#     ax1a.set_ylim([0,N_ACTIONS])\n",
    "#     ax1a.set_ylabel('Duty Cycle', color='b',fontsize=12)\n",
    "\n",
    "#     fig.tight_layout()\n",
    "#     st.set_y(0.95)\n",
    "#     fig.subplots_adjust(top=0.75)\n",
    "#     plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
