{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "from datetime import datetime\n",
    "tic = datetime.now()\n",
    "\n",
    "import os\n",
    "from os.path import dirname, abspath, join\n",
    "from os import getcwd\n",
    "import sys\n",
    "\n",
    "# THIS_DIR = getcwd()\n",
    "# CLASS_DIR = abspath(join(THIS_DIR, 'dsnclasses'))  #abspath(join(THIS_DIR, '../../..', 'dsnclasses'))\n",
    "# sys.path.append(CLASS_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import string\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.ticker as ticker\n",
    "import torch\n",
    "from IPython.display import clear_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 230\n",
    "random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "torch.cuda.manual_seed(seed)\n",
    "np.random.seed(seed)\n",
    "os.environ['PYTHONHASHSEED'] = str(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ENO(object):\n",
    "    \n",
    "    #no. of forecast types is 6 ranging from 0 to 5\n",
    "  \n",
    "    def __init__(self, location='tokyo', year=2010, shuffle=False, day_balance=False):\n",
    "        self.location = location\n",
    "        self.year = year\n",
    "        self.day = None\n",
    "        self.hr = None\n",
    "        \n",
    "        self.shuffle = shuffle\n",
    "        self.day_balance = day_balance\n",
    "\n",
    "        self.TIME_STEPS = None #no. of time steps in one episode\n",
    "        self.NO_OF_DAYS = None #no. of days in one year\n",
    "        \n",
    "        self.NO_OF_DAYTYPE = 10 #no. of daytypes\n",
    "        self.daycounter = 0 #to count number of days that have been passed\n",
    "        \n",
    "        self.sradiation = None #matrix with GSR for the entire year\n",
    "        self.senergy = None #matrix with harvested energy data for the entire year\n",
    "        self.fforecast = None #array with forecast values for each day\n",
    "        \n",
    "\n",
    "        self.henergy = None #harvested energy variable\n",
    "        self.fcast = None #forecast variable\n",
    "        self.sorted_days = [] #days sorted according to day type\n",
    "        \n",
    "        self.SMAX = 1000 # 1 Watt Solar Panel\n",
    "\n",
    "    \n",
    "    #function to get the solar data for the given location and year and prep it\n",
    "    def get_data(self):\n",
    "        #solar_data/CSV files contain the values of GSR (Global Solar Radiation in MegaJoules per meters squared per hour)\n",
    "        #weather_data/CSV files contain the weather summary from 06:00 to 18:00 and 18:00 to 06:00+1\n",
    "        location = self.location\n",
    "        year = self.year\n",
    "\n",
    "        THIS_DIR = getcwd()\n",
    "        SDATA_DIR = abspath(join(THIS_DIR, 'solar_data'))  #abspath(join(THIS_DIR, '../../..', 'data'))\n",
    "        \n",
    "        sfile = SDATA_DIR + '/' + location +'/' + str(year) + '.csv'\n",
    "        \n",
    "        #skiprows=4 to remove unnecessary title texts\n",
    "        #usecols=4 to read only the Global Solar Radiation (GSR) values\n",
    "        solar_radiation = pd.read_csv(sfile, skiprows=4, encoding='shift_jisx0213', usecols=[4])\n",
    "      \n",
    "        #convert dataframe to numpy array\n",
    "        solar_radiation = solar_radiation.values\n",
    "\n",
    "        #convert missing data in CSV files to zero\n",
    "        solar_radiation[np.isnan(solar_radiation)] = 0\n",
    "\n",
    "        #reshape solar_radiation into no_of_daysx24 array\n",
    "        solar_radiation = solar_radiation.reshape(-1,24)\n",
    "\n",
    "        if(self.shuffle): #if class instatiation calls for shuffling the day order. Required when learning\n",
    "            np.random.shuffle(solar_radiation) \n",
    "        self.sradiation = solar_radiation\n",
    "        \n",
    "        #GSR values (in MJ/sq.mts per hour) need to be expressed in mW\n",
    "        # Conversion is accomplished by \n",
    "        # solar_energy = GSR(in MJ/m2/hr) * 1e6 * size of solar cell * efficiency of solar cell /(60x60) *1000 (to express in mW)\n",
    "        # the factor of 2 in the end is assuming two solar cells\n",
    "        self.senergy = 2*self.sradiation * 1e6 * (55e-3 * 70e-3) * 0.15 * 1000/(60*60)\n",
    "\n",
    "        return 0\n",
    "    \n",
    "    #function to map total day radiation into type of day ranging from 0 to 5\n",
    "    #the classification into day types is quite arbitrary. There is no solid logic behind this type of classification.\n",
    "    \n",
    "    def get_day_state(self,tot_day_radiation):\n",
    "        bin_edges = np.array([0, 3.5, 6.5, 9.0, 12.5, 15.5, 18.5, 22.0, 25, 28])\n",
    "        for k in np.arange(1,bin_edges.size):\n",
    "            if (bin_edges[k-1] < tot_day_radiation <= bin_edges[k]):\n",
    "                day_state = k -1\n",
    "            else:\n",
    "                day_state = bin_edges.size - 1\n",
    "        return int(day_state)\n",
    "    \n",
    "    def get_forecast(self):\n",
    "        #create a perfect forecaster.\n",
    "        tot_day_radiation = np.sum(self.sradiation, axis=1) #contains total solar radiation for each day\n",
    "        get_day_state = np.vectorize(self.get_day_state)\n",
    "        self.fforecast = get_day_state(tot_day_radiation)\n",
    "        \n",
    "        #sort days depending on the type of day and shuffle them; maybe required when learning\n",
    "        for fcast in range(0,6):\n",
    "            fcast_days = ([i for i,x in enumerate(self.fforecast) if x == fcast])\n",
    "            np.random.shuffle(fcast_days)\n",
    "            self.sorted_days.append(fcast_days)\n",
    "        return 0\n",
    "    \n",
    "    def reset(self,day=0): #it is possible to reset to the beginning of a certain day\n",
    "        \n",
    "        self.get_data() #first get data for the given year\n",
    "        self.get_forecast() #calculate the forecast\n",
    "        \n",
    "        self.TIME_STEPS = self.senergy.shape[1]\n",
    "        self.NO_OF_DAYS = self.senergy.shape[0]\n",
    "        \n",
    "        self.day = day\n",
    "        self.hr = 0\n",
    "        \n",
    "        self.henergy = self.senergy[self.day][self.hr]\n",
    "        self.fcast = self.fforecast[self.day]\n",
    "        \n",
    "        end_of_day = False\n",
    "        end_of_year = False\n",
    "        return [self.henergy, self.fcast, end_of_day, end_of_year]\n",
    "\n",
    "    \n",
    "    def step(self):\n",
    "        end_of_day = False\n",
    "        end_of_year = False\n",
    "        if not(self.day_balance): #if daytype balance is not required\n",
    "            if(self.hr < self.TIME_STEPS - 1):\n",
    "                self.hr += 1\n",
    "                self.henergy = self.senergy[self.day][self.hr] \n",
    "                self.fcast = self.fforecast[self.day]\n",
    "            else:\n",
    "                if(self.day < self.NO_OF_DAYS -1):\n",
    "                    end_of_day = True\n",
    "                    self.hr = 0\n",
    "                    self.day += 1\n",
    "                    self.henergy = self.senergy[self.day][self.hr] \n",
    "                    self.fcast = self.fforecast[self.day]\n",
    "                else:\n",
    "                    end_of_day = True\n",
    "                    end_of_year = True\n",
    "                    \n",
    "        else: #when training, we want all daytypes to be equally represented for robust policy\n",
    "              #obviously, the days are going to be in random order\n",
    "            if(self.hr < self.TIME_STEPS - 1):\n",
    "                self.hr += 1\n",
    "                self.henergy = self.senergy[self.day][self.hr]\n",
    "                self.fcast = self.fforecast[self.day]\n",
    "            else:\n",
    "                if(self.daycounter < self.NO_OF_DAYS -1):\n",
    "                    end_of_day = True\n",
    "                    self.daycounter += 1\n",
    "                    self.hr = 0\n",
    "                    daytype = random.choice(np.arange(0,self.NO_OF_DAYTYPE)) #choose random daytype\n",
    "                    self.day = np.random.choice(self.sorted_days[daytype]) #choose random day from that daytype\n",
    "                    self.henergy = self.senergy[self.day][self.hr] \n",
    "                    self.fcast = self.fforecast[self.day]\n",
    "                else: \n",
    "                    end_of_day = True\n",
    "                    end_of_year = True\n",
    "                    self.daycounter = 0\n",
    "        \n",
    "        \n",
    "        return [self.henergy, self.fcast, end_of_day, end_of_year]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CAPM (object):\n",
    "    def __init__(self,location='tokyo', year=2010, shuffle=False, trainmode=False):\n",
    "\n",
    "        #all energy values i.e. BMIN, BMAX, BOPT, HMAX are in mWhr. Assuming one timestep is one hour\n",
    "        \n",
    "        self.BMIN = 0.0                #Minimum battery level that is tolerated. Maybe non-zero also\n",
    "        self.BMAX = 9250.0            #Max Battery Level. May not necessarily be equal to total batter capacity [3.6V x 2500mAh]\n",
    "        self.BOPT = 0.5 * self.BMAX    #Optimal Battery Level. Assuming 50% of battery is the optimum\n",
    "        \n",
    "        self.HMIN = 0      #Minimum energy that can be harvested by the solar panel.\n",
    "        self.HMAX = None   #Maximum energy that can be harvested by the solar panel. [500mW]\n",
    "        \n",
    "        self.DMAX = 500      #Maximum energy that can be consumed by the node in one time step. [~ 3.6V x 135mA]\n",
    "        self.N_ACTIONS = 10  #No. of different duty cycles possible\n",
    "        self.DMIN = self.DMAX/self.N_ACTIONS #Minimum energy that can be consumed by the node in one time step. [~ 3.6V x 15mA]\n",
    "        \n",
    "        self.binit = None     #battery at the beginning of day\n",
    "        self.btrack = []      #track the mean battery level for each day\n",
    "        self.atrack = []      #track the duty cycles for each day\n",
    "        self.batt = None      #battery variable\n",
    "        self.enp = None       #enp at end of hr\n",
    "        self.henergy = None   #harvested energy variable\n",
    "        self.fcast = None     #forecast variable\n",
    "        \n",
    "        self.MUBATT = 0.6\n",
    "        self.SDBATT = 0.02\n",
    "        \n",
    "        self.MUHENERGY = 0.5\n",
    "        self.SDHENERGY = 0.2\n",
    "        \n",
    "        self.MUENP = 0\n",
    "        self.SDENP = 0.02\n",
    "        \n",
    "        self.location = location\n",
    "        self.year = year\n",
    "        self.shuffle = shuffle\n",
    "        self.trainmode = trainmode\n",
    "        self.eno = None#ENO(self.location, self.year, shuffle=shuffle, day_balance=trainmode) #if trainmode is enable, then days are automatically balanced according to daytype i.e. day_balance= True\n",
    "        \n",
    "        self.day_violation_flag = False\n",
    "        self.violation_flag = False\n",
    "        self.violation_counter = 0\n",
    "\n",
    "        self.NO_OF_DAYTYPE = 10 #no. of daytypes\n",
    " \n",
    "    def reset(self,day=0,batt=-1):\n",
    "        henergy, fcast, day_end, year_end = self.eno.reset(day) #reset the eno environment\n",
    "        self.violation_flag = False\n",
    "        self.violation_counter = 0\n",
    "        if(batt == -1):\n",
    "            self.batt = self.BOPT\n",
    "        else:\n",
    "            self.batt = batt\n",
    "            \n",
    "        self.batt = np.clip(self.batt, self.BMIN, self.BMAX)\n",
    "        self.binit = self.batt\n",
    "        self.btrack = np.append(self.btrack, self.batt) #track battery levels\n",
    "\n",
    "#         self.enp = self.BOPT - self.batt\n",
    "        self.enp = self.binit - self.batt #enp is calculated\n",
    "        self.henergy = np.clip(henergy, self.HMIN, self.HMAX) #clip henergy within HMIN and HMAX\n",
    "        self.fcast = fcast\n",
    "        \n",
    "        norm_batt = self.batt/self.BMAX\n",
    "        norm_enp = self.enp/(self.BMAX/2)\n",
    "        norm_henergy = self.henergy/self.HMAX\n",
    "        norm_fcast = self.fcast/(self.NO_OF_DAYTYPE-1)\n",
    "\n",
    "        c_state = [norm_batt, norm_enp, norm_henergy, norm_fcast] #continuous states\n",
    "        reward = 0\n",
    "        \n",
    "        return [c_state, reward, day_end, year_end]\n",
    "    \n",
    "    def getstate(self): #query the present state of the system\n",
    "        norm_batt = self.batt/self.BMAX - self.MUBATT\n",
    "        norm_enp = self.enp/(self.BMAX/2)\n",
    "        norm_henergy = self.henergy/self.HMAX\n",
    "        norm_fcast = self.fcast/(self.NO_OF_DAYTYPE-1)        \n",
    "        c_state = [norm_batt, norm_enp, norm_henergy] #continuous states\n",
    "\n",
    "        return c_state\n",
    "    \n",
    "#     def rewardfn(self):\n",
    "#         R_PARAM = 20000 #chosen empirically for best results\n",
    "#         mu = 0\n",
    "#         sig = 0.07*R_PARAM #knee curve starts at approx. 2000mWhr of deviation\n",
    "#         norm_reward = 3*(np.exp(-np.power((self.enp - mu)/sig, 2.)/2) / np.exp(-np.power((0 - mu)/sig, 2.)/2))-1\n",
    "\n",
    "        \n",
    "# #         if(np.abs(self.enp) <= 0.12*R_PARAM):\n",
    "# #             norm_reward = 2*(np.exp(-np.power((self.enp - mu)/sig, 2.)/2) / np.exp(-np.power((0 - mu)/sig, 2.)/2))\n",
    "# #         else:\n",
    "# #             norm_reward = -0.25 - 10*np.abs(self.enp/R_PARAM)\n",
    "#         if(self.day_violation_flag):\n",
    "#             norm_reward -= 3\n",
    "            \n",
    "#         return (norm_reward)\n",
    "        \n",
    "    \n",
    "    #reward function\n",
    "    def rewardfn(self):\n",
    "        \n",
    "        #FIRST REWARD AS A FUNCTION OF DRIFT OF BMEAN FROM BOPT i.e. in terms of BDEV = |BMEAN-BOPT|/BMAX\n",
    "        bmean = np.mean(self.btrack)\n",
    "        bdev = np.abs(self.BOPT - bmean)/self.BMAX\n",
    "        # based on the sigmoid function\n",
    "        # bdev ranges from bdev = (0,0.5) of BMAX\n",
    "        p1_sharpness = 10\n",
    "        n1_sharpness = 20\n",
    "        shift1 = 0.5\n",
    "        # r1(x) = 0.5 when x = 0.25. \n",
    "        # Therefore, shift = 0.5 to make sure that (2*x-shift) evaluates to zero at x = 0.25\n",
    "\n",
    "        if(bdev<=0.25): \n",
    "            r1 = 2*(1-(1 / (1 + np.exp(-p1_sharpness*(2*bdev-shift1)))))-1\n",
    "        else: \n",
    "            r1 = 2*(1-(1 / (1 + np.exp(-n1_sharpness*(2*bdev-shift1)))))-1\n",
    "        # r1 ranges from -1 to 1\n",
    "            \n",
    "        #SECOND REWARD AS A FUNCTION OF ENP AS LONG AS BMAX/4 <= batt <= 3*BMAX/4 i.e. bdev <= 0.25\n",
    "        if(bdev <=0.25):\n",
    "            # enp ranges from enp = (0,3) of DMAX\n",
    "            p2_sharpness = 2\n",
    "            n2_sharpness = 2\n",
    "            shift2 = 6    \n",
    "            # r1(x) = 0.5 when x = 2. \n",
    "            # Therefore, shift = 6 to make sure that (3*x-shift) evaluates to zero at x = 2\n",
    "#             print('Day energy', np.sum(self.eno.senergy[self.eno.day]))\n",
    "#             print('Node energy', np.sum(self.atrack)*self.DMAX/self.N_ACTIONS)\n",
    "#             x = np.abs(np.sum(self.eno.senergy[self.eno.day])-np.sum(self.atrack)*self.DMAX/self.N_ACTIONS )/self.DMAX\n",
    "            x = np.abs(self.enp/self.DMAX)\n",
    "            if(x<=2): \n",
    "                r2 = (1 / (1 + np.exp(p2_sharpness*(3*x-shift2))))\n",
    "            else: \n",
    "                r2 = (1 / (1 + np.exp(n2_sharpness*(3*x-shift2))))\n",
    "        else:\n",
    "            r2 = 0 # if mean battery lies outside bdev limits, then enp reward is not considered.\n",
    "        # r2 ranges from 0 to 1\n",
    "\n",
    "        #REWARD AS A FUNCTION OF BATTERY VIOLATIONS\n",
    "        if(self.day_violation_flag):\n",
    "            violation_penalty = 3\n",
    "        else:\n",
    "            violation_penalty = 0 #penalty for violating battery limits anytime during the day\n",
    "        \n",
    "#         print(\"Reward \", (r1 + r2 - violation_penalty), '\\n')\n",
    "        return (r1*(2**r2) - violation_penalty)\n",
    "    \n",
    "    def step(self, action):\n",
    "        day_end = False\n",
    "        year_end = False\n",
    "        self.violation_flag = False\n",
    "        reward = 0\n",
    "       \n",
    "        action = np.clip(action, 0, self.N_ACTIONS-1) #action values range from (0 to N_ACTIONS-1)\n",
    "        self.atrack = np.append(self.atrack, action+1) #track duty cycles\n",
    "        e_consumed = (action+1)*self.DMAX/self.N_ACTIONS   #energy consumed by the node\n",
    "        \n",
    "        self.batt += (self.henergy - e_consumed)\n",
    "        if(self.batt < 0.02*self.BMAX or self.batt > 0.98*self.BMAX ):\n",
    "            self.violation_flag = True #penalty for violating battery limits everytime it happens\n",
    "            reward = -2\n",
    "        if(self.batt < 0.02*self.BMAX):\n",
    "            reward -= 2\n",
    "            \n",
    "        if(self.violation_flag):\n",
    "            if(self.day_violation_flag == False): #penalty for violating battery limits anytime during the day - triggers once everyday\n",
    "                self.violation_counter += 1\n",
    "                self.day_violation_flag = True\n",
    "            \n",
    "        self.batt = np.clip(self.batt, self.BMIN, self.BMAX) #clip battery values within permitted level\n",
    "        self.btrack = np.append(self.btrack, self.batt) #track battery levels\n",
    "\n",
    "#         self.enp = self.BOPT - self.batt \n",
    "        self.enp = self.binit - self.atrack.sum()*self.DMAX/self.N_ACTIONS\n",
    "        \n",
    "        #proceed to the next time step\n",
    "        self.henergy, self.fcast, day_end, year_end = self.eno.step()\n",
    "        self.henergy = np.clip(self.henergy, self.HMIN, self.HMAX) #clip henergy within HMIN and HMAX\n",
    "                \n",
    "        if(day_end): #if eno object flags that the day has ended then give reward\n",
    "            reward += self.rewardfn()\n",
    "             \n",
    "            if (self.trainmode): #reset battery to optimal level if limits are exceeded when training\n",
    "#                 self.batt = np.random.uniform(self.DMAX*self.eno.TIME_STEPS/self.BMAX,0.8)*self.BMAX\n",
    "#                 if (self.violation_flag):\n",
    "                if np.random.uniform() < HELP : #occasionaly reset the battery\n",
    "                    self.batt = self.BOPT  \n",
    "            \n",
    "            self.day_violation_flag = False\n",
    "            self.binit = self.batt #this will be the new initial battery level for next day\n",
    "            self.btrack = [] #clear battery tracker\n",
    "            self.atrack = [] #clear duty cycle tracker\n",
    "            \n",
    "                    \n",
    "                \n",
    "        norm_batt = self.batt/self.BMAX\n",
    "        norm_enp = self.enp/(self.BMAX/2)\n",
    "        norm_henergy = self.henergy/self.HMAX\n",
    "        norm_fcast = self.fcast/(self.NO_OF_DAYTYPE-1)\n",
    "\n",
    "        c_state = [norm_batt, norm_enp, norm_henergy, norm_fcast] #continuous states\n",
    "        return [c_state, reward, day_end, year_end]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyper Parameters\n",
    "BATCH_SIZE = 32\n",
    "LR = 0.0001          # learning rate\n",
    "EPSILON = 0.9               # greedy policy\n",
    "GAMMA = 0.9                 # reward discount\n",
    "LAMBDA = 0.9                # parameter decay\n",
    "TARGET_REPLACE_ITER = 24*7*4*18    # target update frequency (every two months)\n",
    "MEMORY_CAPACITY     = 24*7*4*12*2      # store upto six month worth of memory   \n",
    "\n",
    "N_ACTIONS = 10 #no. of duty cycles (0,1,2,3,4)\n",
    "N_STATES = 4 #number of state space parameter [batt, enp, henergy, fcast]\n",
    "\n",
    "HIDDEN_LAYER = 20\n",
    "NO_OF_ITERATIONS = 50\n",
    "GPU = False\n",
    "HELP = 0.05"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "\n",
    "#Class definitions for NN model and learning algorithm\n",
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "\n",
    "        self.fc1 = nn.Linear(N_STATES, HIDDEN_LAYER)\n",
    "        nn.init.kaiming_uniform_(self.fc1.weight)\n",
    "        \n",
    "        self.fc2 = nn.Linear(HIDDEN_LAYER, HIDDEN_LAYER)\n",
    "        nn.init.kaiming_uniform_(self.fc2.weight)\n",
    "\n",
    "        self.out = nn.Linear(HIDDEN_LAYER, N_ACTIONS)\n",
    "        nn.init.xavier_uniform_(self.out.weight) \n",
    "\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.fc1(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.fc2(x)\n",
    "        x = F.relu(x)\n",
    "        actions_value = self.out(x)\n",
    "        return actions_value\n",
    "    \n",
    "class DQN(object):\n",
    "    def __init__(self):\n",
    "        if(GPU): \n",
    "            device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "        else:\n",
    "            device = torch.device('cpu')\n",
    "        self.eval_net, self.target_net = Net(), Net()\n",
    "        self.eval_net.to(device)\n",
    "        self.target_net.to(device)\n",
    "        self.device = device\n",
    "#         print(\"Neural net\")\n",
    "#         print(self.eval_net)\n",
    "        self.learn_step_counter = 0                                     # for target updating\n",
    "        self.memory_counter = 0                                         # for storing memory\n",
    "        self.memory = np.zeros((MEMORY_CAPACITY, N_STATES * 2 + 2))     # initialize memory [mem: ([s], a, r, [s_]) ]\n",
    "        self.optimizer = torch.optim.Adam(self.eval_net.parameters(), lr=LR, weight_decay=1e-3)\n",
    "        self.loss_func = nn.MSELoss()\n",
    "        self.nettoggle = False\n",
    "\n",
    "    def choose_action(self, x):\n",
    "        x = torch.unsqueeze(torch.FloatTensor(x), 0)\n",
    "        x = x.to(self.device)\n",
    "\n",
    "        # input only one sample\n",
    "        if np.random.uniform() < EPSILON:   # greedy\n",
    "            actions_value = self.eval_net.forward(x)\n",
    "            actions_value = actions_value.to(torch.device(\"cpu\"))\n",
    "            action = torch.max(actions_value, 1)[1].data.numpy()\n",
    "            action = action[0] # return the argmax index\n",
    "        else:   # random\n",
    "            action = np.random.randint(0, N_ACTIONS)\n",
    "            action = action\n",
    "        return action\n",
    "    \n",
    "    def choose_greedy_action(self, x):\n",
    "        x = torch.unsqueeze(torch.FloatTensor(x), 0)\n",
    "        x = x.to(self.device)\n",
    "\n",
    "        # input only one sample\n",
    "        if True:   # greedy\n",
    "            actions_value = self.eval_net.forward(x)\n",
    "            actions_value = actions_value.to(torch.device(\"cpu\"))\n",
    "            action = torch.max(actions_value, 1)[1].data.numpy()\n",
    "            action = action[0] # return the argmax index\n",
    "        return action\n",
    "\n",
    "    def store_transition(self, s, a, r, s_):\n",
    "        transition = np.hstack((s, [a, r], s_))\n",
    "        # replace the old memory with new memory\n",
    "        index = self.memory_counter % MEMORY_CAPACITY\n",
    "        self.memory[index, :] = transition\n",
    "        self.memory_counter += 1\n",
    "    \n",
    "    def store_day_transition(self, transition_rec):\n",
    "        data = transition_rec\n",
    "        index = self.memory_counter % MEMORY_CAPACITY\n",
    "        self.memory= np.insert(self.memory, index, data,0)\n",
    "        self.memory_counter += transition_rec.shape[0]\n",
    "\n",
    "    def learn(self):\n",
    "        # target parameter update\n",
    "        if self.learn_step_counter % TARGET_REPLACE_ITER == 0:\n",
    "            self.target_net.load_state_dict(self.eval_net.state_dict())\n",
    "            self.nettoggle = not self.nettoggle\n",
    "        self.learn_step_counter += 1\n",
    "\n",
    "        # sample batch transitions\n",
    "        sample_index = np.random.choice(MEMORY_CAPACITY, BATCH_SIZE)\n",
    "        b_memory = self.memory[sample_index, :]\n",
    "        b_s = torch.FloatTensor(b_memory[:, :N_STATES])\n",
    "        b_a = torch.LongTensor(b_memory[:, N_STATES:N_STATES+1].astype(int))\n",
    "        b_r = torch.FloatTensor(b_memory[:, N_STATES+1:N_STATES+2])\n",
    "        b_s_ = torch.FloatTensor(b_memory[:, -N_STATES:])\n",
    "        \n",
    "        b_s = b_s.to(self.device)\n",
    "        b_a = b_a.to(self.device)\n",
    "        b_r = b_r.to(self.device)\n",
    "        b_s_ = b_s_.to(self.device)\n",
    "\n",
    "        # q_eval w.r.t the action in experience\n",
    "        q_eval = self.eval_net(b_s).gather(1, b_a)  # shape (batch, 1)\n",
    "        q_next = self.target_net(b_s_).detach()     # detach from graph, don't backpropagate\n",
    "        q_target = b_r + GAMMA * q_next.max(1)[0].view(BATCH_SIZE, 1)   # shape (batch, 1)\n",
    "        loss = self.loss_func(q_eval, q_target)\n",
    "\n",
    "        self.optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        self.optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def stdize(s):\n",
    "    MU_BATT = 0.5\n",
    "    SD_BATT = 0.15\n",
    "    \n",
    "    MU_ENP = 0\n",
    "    SD_ENP = 0.15\n",
    "    \n",
    "    MU_HENERGY = 0.35\n",
    "    SD_HENERGY = 0.25\n",
    "    \n",
    "    MU_FCAST = 0.42\n",
    "    SD_FCAST = 0.27\n",
    "    \n",
    "    norm_batt, norm_enp, norm_henergy, norm_fcast = s\n",
    "    \n",
    "    std_batt = (norm_batt - MU_BATT)/SD_BATT\n",
    "    std_enp = (norm_enp - MU_ENP)/SD_ENP\n",
    "    std_henergy = (norm_henergy - MU_HENERGY)/SD_HENERGY\n",
    "    std_fcast = (norm_fcast - MU_FCAST)/SD_FCAST\n",
    "\n",
    "\n",
    "    return [std_batt, std_enp, std_henergy, std_fcast]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "TRAINING IN PROGRESS\n",
      "\n",
      "Device:  cpu\n",
      "\n",
      "Iteration 0:  TOKYO, 2002 \n",
      "Average Reward \t\t= -6.732\n",
      "Violation Counter \t= 343\n",
      "\n",
      "Iteration 1:  TOKYO, 2004 \n",
      "Average Reward \t\t= -6.236\n",
      "Violation Counter \t= 324\n",
      "\n",
      "Iteration 2:  TOKYO, 2008 \n",
      "Average Reward \t\t= -6.109\n",
      "Violation Counter \t= 328\n",
      "\n",
      "Iteration 3:  TOKYO, 2000 \n",
      "Average Reward \t\t= -6.769\n",
      "Violation Counter \t= 352\n",
      "\n",
      "Iteration 4:  TOKYO, 2006 \n",
      "Average Reward \t\t= -7.039\n",
      "Violation Counter \t= 354\n",
      "\n",
      "Iteration 5:  TOKYO, 2000 \n",
      "Average Reward \t\t= -5.867\n",
      "Violation Counter \t= 313\n",
      "\n",
      "Iteration 6:  TOKYO, 2007 \n",
      "Average Reward \t\t= -3.803\n",
      "Violation Counter \t= 242\n",
      "\n",
      "Iteration 7:  TOKYO, 2007 \n",
      "Average Reward \t\t= -3.381\n",
      "Violation Counter \t= 221\n",
      "\n",
      "Iteration 8:  TOKYO, 2006 \n",
      "Average Reward \t\t= -2.265\n",
      "Violation Counter \t= 163\n",
      "\n",
      "Iteration 9:  TOKYO, 2009 \n",
      "Average Reward \t\t= -1.270\n",
      "Violation Counter \t= 107\n",
      "\n",
      "Iteration 10:  TOKYO, 2008 \n",
      "Average Reward \t\t= -0.988\n",
      "Violation Counter \t= 100\n",
      "\n",
      "Iteration 11:  TOKYO, 2004 \n",
      "Average Reward \t\t= -0.931\n",
      "Violation Counter \t= 95\n",
      "\n",
      "Iteration 12:  TOKYO, 2009 \n",
      "Average Reward \t\t= -0.096\n",
      "Violation Counter \t= 58\n",
      "\n",
      "Iteration 13:  TOKYO, 2000 \n",
      "Average Reward \t\t= 0.013\n",
      "Violation Counter \t= 52\n",
      "\n",
      "Iteration 14:  TOKYO, 2005 \n",
      "Average Reward \t\t= 0.588\n",
      "Violation Counter \t= 28\n",
      "\n",
      "Iteration 15:  TOKYO, 2003 \n",
      "Average Reward \t\t= 0.178\n",
      "Violation Counter \t= 40\n",
      "\n",
      "Iteration 16:  TOKYO, 2000 \n",
      "Average Reward \t\t= 0.169\n",
      "Violation Counter \t= 45\n",
      "\n",
      "Iteration 17:  TOKYO, 2005 \n",
      "Average Reward \t\t= 0.748\n",
      "Violation Counter \t= 25\n",
      "\n",
      "Iteration 18:  TOKYO, 2004 \n",
      "Average Reward \t\t= 0.479\n",
      "Violation Counter \t= 34\n",
      "\n",
      "Iteration 19:  TOKYO, 2002 \n",
      "Average Reward \t\t= 0.059\n",
      "Violation Counter \t= 51\n",
      "\n",
      "Iteration 20:  TOKYO, 2002 \n",
      "Average Reward \t\t= 0.299\n",
      "Violation Counter \t= 38\n",
      "\n",
      "Iteration 21:  TOKYO, 2007 \n",
      "Average Reward \t\t= 0.494\n",
      "Violation Counter \t= 32\n",
      "\n",
      "Iteration 22:  TOKYO, 2009 \n",
      "Average Reward \t\t= 0.532\n",
      "Violation Counter \t= 30\n",
      "\n",
      "Iteration 23:  TOKYO, 2006 \n",
      "Average Reward \t\t= -0.062\n",
      "Violation Counter \t= 58\n",
      "\n",
      "Iteration 24:  TOKYO, 2009 \n",
      "Average Reward \t\t= 0.509\n",
      "Violation Counter \t= 32\n",
      "\n",
      "Iteration 25:  TOKYO, 2007 \n",
      "Average Reward \t\t= 0.446\n",
      "Violation Counter \t= 36\n",
      "\n",
      "Iteration 26:  TOKYO, 2004 \n",
      "Average Reward \t\t= 0.432\n",
      "Violation Counter \t= 37\n",
      "\n",
      "Iteration 27:  TOKYO, 2005 \n",
      "Average Reward \t\t= 0.701\n",
      "Violation Counter \t= 27\n",
      "\n",
      "Iteration 28:  TOKYO, 2000 \n",
      "Average Reward \t\t= 0.472\n",
      "Violation Counter \t= 34\n",
      "\n",
      "Iteration 29:  TOKYO, 2000 \n",
      "Average Reward \t\t= 0.387\n",
      "Violation Counter \t= 38\n",
      "\n",
      "Iteration 30:  TOKYO, 2001 \n",
      "Average Reward \t\t= 0.431\n",
      "Violation Counter \t= 36\n",
      "\n",
      "Iteration 31:  TOKYO, 2005 \n",
      "Average Reward \t\t= 0.675\n",
      "Violation Counter \t= 26\n",
      "\n",
      "Iteration 32:  TOKYO, 2007 \n",
      "Average Reward \t\t= 0.753\n",
      "Violation Counter \t= 19\n",
      "\n",
      "Iteration 33:  TOKYO, 2008 \n",
      "Average Reward \t\t= 0.845\n",
      "Violation Counter \t= 19\n",
      "\n",
      "Iteration 34:  TOKYO, 2004 \n",
      "Average Reward \t\t= 0.676\n",
      "Violation Counter \t= 27\n",
      "\n",
      "Iteration 35:  TOKYO, 2003 \n",
      "Average Reward \t\t= 0.587\n",
      "Violation Counter \t= 23\n",
      "\n",
      "Iteration 36:  TOKYO, 2003 \n",
      "Average Reward \t\t= 0.534\n",
      "Violation Counter \t= 33\n",
      "\n",
      "Iteration 37:  TOKYO, 2007 \n",
      "Average Reward \t\t= 0.559\n",
      "Violation Counter \t= 34\n",
      "\n",
      "Iteration 38:  TOKYO, 2006 \n",
      "Average Reward \t\t= 0.330\n",
      "Violation Counter \t= 41\n",
      "\n",
      "Iteration 39:  TOKYO, 2001 \n",
      "Average Reward \t\t= 0.697\n",
      "Violation Counter \t= 20\n",
      "\n",
      "Iteration 40:  TOKYO, 2009 \n",
      "Average Reward \t\t= 0.520\n",
      "Violation Counter \t= 34\n",
      "\n",
      "Iteration 41:  TOKYO, 2006 \n",
      "Average Reward \t\t= 0.388\n",
      "Violation Counter \t= 38\n",
      "\n",
      "Iteration 42:  TOKYO, 2007 \n",
      "Average Reward \t\t= 0.847\n",
      "Violation Counter \t= 16\n",
      "\n",
      "Iteration 43:  TOKYO, 2007 \n",
      "Average Reward \t\t= 0.791\n",
      "Violation Counter \t= 21\n",
      "\n",
      "Iteration 44:  TOKYO, 2005 \n",
      "Average Reward \t\t= 0.873\n",
      "Violation Counter \t= 21\n",
      "\n",
      "Iteration 45:  TOKYO, 2000 \n",
      "Average Reward \t\t= 0.543\n",
      "Violation Counter \t= 34\n",
      "\n",
      "Iteration 46:  TOKYO, 2004 \n",
      "Average Reward \t\t= 0.728\n",
      "Violation Counter \t= 26\n",
      "\n",
      "Iteration 47:  TOKYO, 2004 \n",
      "Average Reward \t\t= 0.700\n",
      "Violation Counter \t= 25\n",
      "\n",
      "Iteration 48:  TOKYO, 2005 \n",
      "Average Reward \t\t= 0.901\n",
      "Violation Counter \t= 18\n",
      "\n",
      "Iteration 49:  TOKYO, 2002 \n",
      "Average Reward \t\t= 0.163\n",
      "Violation Counter \t= 48\n"
     ]
    }
   ],
   "source": [
    "#TRAIN \n",
    "dqn = DQN()\n",
    "# for recording weights\n",
    "oldfc1 = dqn.eval_net.fc1.weight.data.cpu().numpy().flatten()\n",
    "old2fc1 = oldfc1\n",
    "\n",
    "oldfc2 = dqn.eval_net.fc2.weight.data.cpu().numpy().flatten()\n",
    "old2fc2 = oldfc2\n",
    "\n",
    "# oldfc3 = dqn.eval_net.fc3.weight.data.cpu().numpy().flatten()\n",
    "# old2fc3 = oldfc3\n",
    "\n",
    "oldout = dqn.eval_net.out.weight.data.cpu().numpy().flatten()\n",
    "old2out = oldout\n",
    "########################################\n",
    "\n",
    "best_iteration = -1\n",
    "best_avg_reward = -1000 #initialize best average reward to very low value\n",
    "reset_counter = 0 #count number of times the battery had to be reset\n",
    "change_hr = 0\n",
    "# PFILENAME = ''.join(random.choice(string.ascii_uppercase + string.digits) for _ in range(8)) #create random filename\n",
    "# BFILENAME = \"best\"+PFILENAME + \".pt\" #this file stores the best model\n",
    "# TFILENAME = \"terminal\"+PFILENAME + \".pt\" #this file stores the last model\n",
    "\n",
    "avg_reward_rec = [] #record the yearly average rewards over the entire duration of training\n",
    "violation_rec = []\n",
    "print('\\nTRAINING IN PROGRESS\\n')\n",
    "print('Device: ', dqn.device)\n",
    "\n",
    "for iteration in range(NO_OF_ITERATIONS):\n",
    "    LOCATION = 'tokyo'#random.choice(['tokyo','wakkanai','minamidaito'])\n",
    "    YEAR = random.choice(np.arange(2000,2010))\n",
    "    capm = CAPM(LOCATION,YEAR,shuffle=False, trainmode=False) #instantiate the CAPM class\n",
    "    capm.eno = ENO(LOCATION,YEAR, shuffle=False, day_balance=False) #instantiate the environment inside the CAPM class\n",
    "    capm.HMAX = capm.eno.SMAX #maximum power output of solar cell is set in CAPM object using the value in ENO object\n",
    "#     clear_output()\n",
    "    print('\\nIteration {}:  {}, {} '.format(iteration, LOCATION.upper(), YEAR))\n",
    "\n",
    "    s, r, day_end, year_end = capm.reset()\n",
    "    yr_record = np.empty(4)\n",
    "\n",
    "    record = np.empty(4) #record for battery, henergy, reward and action\n",
    "    transition_rec = np.zeros((capm.eno.TIME_STEPS, N_STATES * 2 + 2)) #record all the transition in one day\n",
    "\n",
    "    while True:\n",
    "        a = dqn.choose_action(stdize(s))\n",
    "\n",
    "        # present state = [batt, enp, henergy]\n",
    "        record = np.vstack((record, [s[0],s[2],r, a])) # record battery, henergy, reward and action for troubleshooting\n",
    "        yr_record = np.vstack((yr_record, [s[0],s[2],r, a]))\n",
    "\n",
    "        # take action\n",
    "        s_, r, day_end, year_end = capm.step(a)\n",
    "        \n",
    "        temp_transitions = np.hstack((stdize(s), [a, r], stdize(s_)))\n",
    "        transition_rec[capm.eno.hr-1,:] = temp_transitions\n",
    "\n",
    "        if (day_end):\n",
    "            transition_rec[:,5] += r #broadcast reward to all states\n",
    "            decay_factor = [i for i in (LAMBDA**n for n in reversed(range(0, capm.eno.TIME_STEPS)))]\n",
    "            transition_rec[:,5] = transition_rec[:,5] * decay_factor #decay reward proportionately\n",
    "            dqn.store_day_transition(transition_rec)\n",
    "\n",
    "        if dqn.memory_counter > MEMORY_CAPACITY:\n",
    "            dqn.learn()\n",
    "\n",
    "        if dqn.nettoggle:\n",
    "            change_hr = capm.eno.day*24+capm.eno.hr #to mark when the DQN is updated.\n",
    "            dqn.nettoggle = not dqn.nettoggle\n",
    "\n",
    "        if (year_end):\n",
    "            break\n",
    "\n",
    "        # transition to new state\n",
    "        s = s_\n",
    "\n",
    "    record = np.delete(record, 0, 0) #remove the first row which is garbage\n",
    "    reward_rec = record[:,2] #extract reward information from the record array\n",
    "    reward_rec = reward_rec[::24] #only consider terminal rewards\n",
    "    print(\"Average Reward \\t\\t= {:.3f}\".format(np.mean(reward_rec)))\n",
    "    print(\"Violation Counter \\t= {}\".format(capm.violation_counter))\n",
    "\n",
    "#     if(best_avg_reward < np.mean(reward_rec)):\n",
    "#         best_avg_reward = np.mean(reward_rec)\n",
    "    \n",
    "#     if(best_avg_reward > 1.5 or iteration > 20):\n",
    "#         EPSILON = 0.9\n",
    "#         LR = 0.01\n",
    "        \n",
    "#     if (capm.violation_counter < 5):\n",
    "#         reset_flag = False\n",
    "#         EPSILON = 0.95\n",
    "#         LR = 0.001\n",
    "        \n",
    "\n",
    "#     # Check if reward beats the High Score and possible save it    \n",
    "#     if (iteration > 19): #save the best models only after 20 iterations\n",
    "#         print(\"Best Score \\t = {:8.3f} @ Iteration No. {}\".format(best_avg_reward, best_iteration))\n",
    "#         if(best_avg_reward < np.mean(reward_rec)):\n",
    "#             best_iteration = iteration\n",
    "#             best_avg_reward = np.mean(reward_rec)\n",
    "#             print(\"Saving Model\")\n",
    "#             torch.save(dqn.eval_net.state_dict(), BFILENAME)\n",
    "#     else:\n",
    "#         print(\"\\r\")\n",
    "\n",
    "    # Log the average reward in avg_reward_rec\n",
    "    avg_reward_rec = np.append(avg_reward_rec, np.mean(reward_rec))\n",
    "    violation_rec = np.append(violation_rec, capm.violation_counter)\n",
    "\n",
    "    \n",
    "###########################################################################################\n",
    "# #   PLOT battery levels, hourly rewards and the weights\n",
    "#     yr_record = np.delete(yr_record, 0, 0) #remove the first row which is garbage\n",
    "# #     NO_OF_DAYS = capm.eno.NO_OF_DAYS\n",
    "#     hourly_yr_reward_rec = yr_record[:,2]\n",
    "#     yr_reward_rec = hourly_yr_reward_rec[::24]\n",
    "\n",
    "    \n",
    "#     fig = plt.figure(figsize=(24,3))\n",
    "#     TIME_STEPS = capm.eno.TIME_STEPS\n",
    "#     NO_OF_DAYS = capm.eno.NO_OF_DAYS\n",
    "#     DAY_SPACING = 15\n",
    "#     TICK_SPACING = TIME_STEPS*DAY_SPACING\n",
    "#     #plot battery\n",
    "#     ax = fig.add_subplot(111)\n",
    "#     ax.plot(np.arange(0,TIME_STEPS*NO_OF_DAYS),yr_record[:,0],'r')\n",
    "#     ax.set_ylim([0,1])\n",
    "#     ax.axvline(x=change_hr)\n",
    "#     ax.xaxis.set_major_locator(ticker.MultipleLocator(TICK_SPACING))\n",
    "# #     labels = [item for item in ax.get_xticklabels()]\n",
    "# #     print(labels)\n",
    "# #     labels [15:-1] = np.arange(0,NO_OF_DAYS,DAY_SPACING) #the first label is reserved to negative values\n",
    "# #     ax.set_xticklabels(labels)\n",
    "#     #plot hourly reward\n",
    "#     ax0 = ax.twinx()\n",
    "#     ax0.plot(hourly_yr_reward_rec, color='m')\n",
    "#     ax0.set_ylim(-7,3)\n",
    "#     plt.show()\n",
    "\n",
    "\n",
    "#     fig = plt.figure(figsize=(18,3))\n",
    "#     ax1 = fig.add_subplot(131)\n",
    "#     newfc1 = dqn.eval_net.fc1.weight.data.cpu().numpy().flatten()\n",
    "#     ax1.plot(old2fc1,color='b', alpha=0.4)\n",
    "#     ax1.plot(oldfc1,color='b',alpha = 0.7)\n",
    "#     ax1.plot(newfc1,color='b')\n",
    "#     old2fc1 = oldfc1\n",
    "#     oldfc1 = newfc1\n",
    "    \n",
    "#     ax2 = fig.add_subplot(132)\n",
    "#     newfc2 = dqn.eval_net.fc2.weight.data.cpu().numpy().flatten()\n",
    "#     ax2.plot(old2fc2,color='y', alpha=0.4)\n",
    "#     ax2.plot(oldfc2,color='y',alpha = 0.7)\n",
    "#     ax2.plot(newfc2,color='y')\n",
    "#     old2fc2 = oldfc2\n",
    "#     oldfc2 = newfc2\n",
    "    \n",
    "# #     ax3 = fig.add_subplot(143)\n",
    "# #     newfc3 = dqn.eval_net.fc3.weight.data.cpu().numpy().flatten()\n",
    "# #     ax3.plot(old2fc3,color='y', alpha=0.4)\n",
    "# #     ax3.plot(oldfc3,color='y',alpha = 0.7)\n",
    "# #     ax3.plot(newfc3,color='y')\n",
    "# #     old2fc3 = oldfc3\n",
    "# #     oldfc3 = newfc3\n",
    "    \n",
    "#     axO = fig.add_subplot(133)\n",
    "#     newout = dqn.eval_net.out.weight.data.cpu().numpy().flatten()\n",
    "#     axO.plot(old2out,color='g', alpha=0.4)\n",
    "#     axO.plot(oldout,color='g',alpha=0.7)\n",
    "#     axO.plot(newout,color='g')\n",
    "#     old2out = oldout\n",
    "#     oldout = newout\n",
    "    \n",
    "#     fig.tight_layout()\n",
    "#     plt.show()\n",
    "\n",
    "\n",
    "    # End of training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 648x216 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig = plt.figure(figsize=(9,3))\n",
    "# Plot the average reward log\n",
    "\n",
    "ax1 = fig.add_subplot(111)\n",
    "ax1.set_ylabel(\"Reward\")\n",
    "# ax1.set_ylim([-3,3]);\n",
    "ax1.plot(avg_reward_rec,'b')\n",
    "ax1.tick_params(axis='y', colors='b')\n",
    "\n",
    "#Plot the violation record log\n",
    "ax2 = ax1.twinx()\n",
    "ax2.set_ylabel(\"Violations\",color = 'r')\n",
    "ax2.plot(violation_rec,'r')\n",
    "for xpt in np.argwhere(violation_rec<1):\n",
    "    ax2.axvline(x=xpt,color='g')\n",
    "ax2.set_ylim([0,50]);\n",
    "ax2.tick_params(axis='y', colors='r')\n",
    "\n",
    "fig.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device:  cpu\n",
      "runtime: 0:12:01.289695\n"
     ]
    }
   ],
   "source": [
    "print('Device: ', dqn.device)\n",
    "print('runtime: {}'.format(datetime.now() - tic))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPSILON =  0.98\n",
      "LR      =  5e-05\n",
      "\n",
      "LAST PHASE ITERATION #0:  TOKYO, 2006 \n",
      "Average Reward \t\t= 0.837\n",
      "Violation Counter \t= 18\n",
      "***MEASURING PERFORMANCE OF THE MODEL***\n",
      "\tBEST AVERAGE ANNUAL AVERAGE REWARD = -1000.000\n",
      "\tBEST TOTAL VIOLATIONS              = 1000\n",
      "\n",
      "\tAverage Annual Average Reward      = 1.295\n",
      "\tTotal Violations                   = 31.0\n",
      "****************************************\n",
      "\n",
      "LAST PHASE ITERATION #1:  TOKYO, 2003 \n",
      "Average Reward \t\t= 1.198\n",
      "Violation Counter \t= 6\n",
      "***MEASURING PERFORMANCE OF THE MODEL***\n",
      "\tBEST AVERAGE ANNUAL AVERAGE REWARD = 1.295\n",
      "\tBEST TOTAL VIOLATIONS              = 31.0\n",
      "\n",
      "\tAverage Annual Average Reward      = 1.232\n",
      "\tTotal Violations                   = 37.0\n",
      "****************************************\n",
      "\n",
      "LAST PHASE ITERATION #2:  TOKYO, 2001 \n",
      "Average Reward \t\t= 1.264\n",
      "Violation Counter \t= 5\n",
      "***MEASURING PERFORMANCE OF THE MODEL***\n",
      "\tBEST AVERAGE ANNUAL AVERAGE REWARD = 1.295\n",
      "\tBEST TOTAL VIOLATIONS              = 31.0\n",
      "\n",
      "\tAverage Annual Average Reward      = 1.223\n",
      "\tTotal Violations                   = 17.0\n",
      "****************************************\n",
      "\n",
      "LAST PHASE ITERATION #3:  TOKYO, 2001 \n",
      "Average Reward \t\t= 1.302\n",
      "Violation Counter \t= 1\n",
      "***MEASURING PERFORMANCE OF THE MODEL***\n",
      "\tBEST AVERAGE ANNUAL AVERAGE REWARD = 1.295\n",
      "\tBEST TOTAL VIOLATIONS              = 17.0\n",
      "\n",
      "\tAverage Annual Average Reward      = 1.291\n",
      "\tTotal Violations                   = 20.0\n",
      "****************************************\n",
      "\n",
      "LAST PHASE ITERATION #4:  TOKYO, 2008 \n",
      "Average Reward \t\t= 1.285\n",
      "Violation Counter \t= 2\n",
      "***MEASURING PERFORMANCE OF THE MODEL***\n",
      "\tBEST AVERAGE ANNUAL AVERAGE REWARD = 1.295\n",
      "\tBEST TOTAL VIOLATIONS              = 17.0\n",
      "\n",
      "\tAverage Annual Average Reward      = 1.182\n",
      "\tTotal Violations                   = 51.0\n",
      "****************************************\n",
      "\n",
      "LAST PHASE ITERATION #5:  TOKYO, 2002 \n",
      "Average Reward \t\t= 0.949\n",
      "Violation Counter \t= 19\n",
      "***MEASURING PERFORMANCE OF THE MODEL***\n",
      "\tBEST AVERAGE ANNUAL AVERAGE REWARD = 1.295\n",
      "\tBEST TOTAL VIOLATIONS              = 17.0\n",
      "\n",
      "\tAverage Annual Average Reward      = 1.282\n",
      "\tTotal Violations                   = 23.0\n",
      "****************************************\n",
      "\n",
      "LAST PHASE ITERATION #6:  TOKYO, 2008 \n",
      "Average Reward \t\t= 1.288\n",
      "Violation Counter \t= 3\n",
      "***MEASURING PERFORMANCE OF THE MODEL***\n",
      "\tBEST AVERAGE ANNUAL AVERAGE REWARD = 1.295\n",
      "\tBEST TOTAL VIOLATIONS              = 17.0\n",
      "\n",
      "\tAverage Annual Average Reward      = 1.252\n",
      "\tTotal Violations                   = 24.0\n",
      "****************************************\n",
      "\n",
      "LAST PHASE ITERATION #7:  TOKYO, 2000 \n",
      "Average Reward \t\t= 1.188\n",
      "Violation Counter \t= 5\n",
      "***MEASURING PERFORMANCE OF THE MODEL***\n",
      "\tBEST AVERAGE ANNUAL AVERAGE REWARD = 1.295\n",
      "\tBEST TOTAL VIOLATIONS              = 17.0\n",
      "\n",
      "\tAverage Annual Average Reward      = 1.200\n",
      "\tTotal Violations                   = 27.0\n",
      "****************************************\n",
      "\n",
      "LAST PHASE ITERATION #8:  TOKYO, 2002 \n",
      "Average Reward \t\t= 1.026\n",
      "Violation Counter \t= 10\n",
      "***MEASURING PERFORMANCE OF THE MODEL***\n",
      "\tBEST AVERAGE ANNUAL AVERAGE REWARD = 1.295\n",
      "\tBEST TOTAL VIOLATIONS              = 17.0\n",
      "\n",
      "\tAverage Annual Average Reward      = 1.250\n",
      "\tTotal Violations                   = 21.0\n",
      "****************************************\n",
      "\n",
      "LAST PHASE ITERATION #9:  TOKYO, 2001 \n",
      "Average Reward \t\t= 1.251\n",
      "Violation Counter \t= 2\n",
      "***MEASURING PERFORMANCE OF THE MODEL***\n",
      "\tBEST AVERAGE ANNUAL AVERAGE REWARD = 1.295\n",
      "\tBEST TOTAL VIOLATIONS              = 17.0\n",
      "\n",
      "\tAverage Annual Average Reward      = 1.289\n",
      "\tTotal Violations                   = 14.0\n",
      "****************************************\n"
     ]
    }
   ],
   "source": [
    "#END OF TRAINING PHASE - CHOOSING THE BEST MODEL INSTANCE\n",
    "#INCREASE GREEDY RATE\n",
    "#VALIDATE AFTER EVERY ITERATION\n",
    "\n",
    "# Use this model and its output as base standards for the last phase of training\n",
    "best_avg_avg_reward = -1000\n",
    "best_net_avg_reward = dqn.eval_net\n",
    "best_avg_v_counter = 1000\n",
    "best_net_v_counter = dqn.eval_net\n",
    "\n",
    "\n",
    "NO_OF_LAST_PHASE_ITERATIONS = 10\n",
    "EPSILON = 0.98\n",
    "LR = 0.00005\n",
    "\n",
    "print(\"EPSILON = \", EPSILON)\n",
    "print(\"LR      = \", LR)\n",
    "\n",
    "for iteration in range(NO_OF_LAST_PHASE_ITERATIONS):\n",
    "    LOCATION = 'tokyo'#random.choice(['tokyo','wakkanai','minamidaito'])\n",
    "    YEAR = random.choice(np.arange(2000,2010))\n",
    "    capm = CAPM(LOCATION,YEAR,shuffle=False, trainmode=False) #instantiate the CAPM class\n",
    "    capm.eno = ENO(LOCATION,YEAR, shuffle=False, day_balance=False) #instantiate the environment inside the CAPM class\n",
    "    capm.HMAX = capm.eno.SMAX #maximum power output of solar cell is set in CAPM object using the value in ENO object\n",
    "\n",
    "    print('\\nLAST PHASE ITERATION #{}:  {}, {} '.format(iteration, LOCATION.upper(), YEAR))\n",
    "    \n",
    "    \n",
    "    my_avg_reward = -1000\n",
    "    my_v_counter = 1000\n",
    "    \n",
    "    s, r, day_end, year_end = capm.reset()\n",
    "    yr_record = np.empty(4)\n",
    "\n",
    "    record = np.empty(4) #record for battery, henergy, reward and action\n",
    "    transition_rec = np.zeros((capm.eno.TIME_STEPS, N_STATES * 2 + 2)) #record all the transition in one day\n",
    "\n",
    "    while True:\n",
    "        a = dqn.choose_action(stdize(s))\n",
    "\n",
    "        # present state = [batt, enp, henergy]\n",
    "        record = np.vstack((record, [s[0],s[2],r, a])) # record battery, henergy, reward and action for troubleshooting\n",
    "        yr_record = np.vstack((yr_record, [s[0],s[2],r, a]))\n",
    "\n",
    "        # take action\n",
    "        s_, r, day_end, year_end = capm.step(a)\n",
    "        \n",
    "        temp_transitions = np.hstack((stdize(s), [a, r], stdize(s_)))\n",
    "        transition_rec[capm.eno.hr-1,:] = temp_transitions\n",
    "\n",
    "        if (day_end):\n",
    "            transition_rec[:,5] += r #broadcast reward to all states\n",
    "            decay_factor = [i for i in (LAMBDA**n for n in reversed(range(0, capm.eno.TIME_STEPS)))]\n",
    "            transition_rec[:,5] = transition_rec[:,5] * decay_factor #decay reward proportionately\n",
    "            dqn.store_day_transition(transition_rec)\n",
    "\n",
    "        if dqn.memory_counter > MEMORY_CAPACITY:\n",
    "            dqn.learn()\n",
    "\n",
    "        if dqn.nettoggle:\n",
    "            change_hr = capm.eno.day*24+capm.eno.hr #to mark when the DQN is updated.\n",
    "            dqn.nettoggle = not dqn.nettoggle\n",
    "\n",
    "        if (year_end):\n",
    "            break\n",
    "\n",
    "        # transition to new state\n",
    "        s = s_\n",
    "\n",
    "    record = np.delete(record, 0, 0) #remove the first row which is garbage\n",
    "    reward_rec = record[:,2] #extract reward information from the record array\n",
    "    reward_rec = reward_rec[::24] #only consider terminal rewards\n",
    "    print(\"Average Reward \\t\\t= {:.3f}\".format(np.mean(reward_rec)))\n",
    "    print(\"Violation Counter \\t= {}\".format(capm.violation_counter))\n",
    "    \n",
    "    \n",
    "    print(\"***MEASURING PERFORMANCE OF THE MODEL***\")\n",
    "    print(\"\\tBEST AVERAGE ANNUAL AVERAGE REWARD = {:.3f}\".format(best_avg_avg_reward))\n",
    "    print(\"\\tBEST TOTAL VIOLATIONS              = {}\".format(best_avg_v_counter))\n",
    "    LOCATION = 'tokyo'\n",
    "    results = np.empty(3)\n",
    "    for YEAR in np.arange(2010,2015):\n",
    "        capm = CAPM(LOCATION,YEAR,shuffle=False, trainmode=False) #instantiate the CAPM class\n",
    "        capm.eno = ENO(LOCATION,YEAR, shuffle=False, day_balance=False) #instantiate the environment inside the CAPM class\n",
    "        capm.HMAX = capm.eno.SMAX #maximum power output of solar cell is set in CAPM object using the value in ENO object\n",
    "\n",
    "        s, r, day_end, year_end = capm.reset()\n",
    "        yr_test_record = np.empty(4)\n",
    "        EPSILON = 1\n",
    "\n",
    "        while True:\n",
    "            a = dqn.choose_greedy_action(stdize(s))\n",
    "            #state = [batt, enp, henergy, fcast]\n",
    "            yr_test_record = np.vstack((yr_test_record, [s[0],s[2],r, a])) #record battery, henergy, reward and action\n",
    "            # take action\n",
    "            s_, r, day_end, year_end = capm.step(a)\n",
    "            if year_end:\n",
    "                break\n",
    "            s = s_\n",
    "\n",
    "        yr_test_record = np.delete(yr_test_record, 0, 0) #remove the first row which is garbage\n",
    "        yr_test_reward_rec = yr_test_record[:,2]\n",
    "        yr_test_reward_rec = yr_test_reward_rec[::24] #annual average reward\n",
    "        results = np.vstack((results, [int(YEAR), np.mean(yr_test_reward_rec), int(capm.violation_counter)]))\n",
    "    results = np.delete(results,0,0)\n",
    "    my_avg_reward = np.mean(results[:,1]) #the average of annual average rewards\n",
    "    my_v_counter = np.sum(results[:,-1]) #total sum of violations\n",
    "    print(\"\\n\\tAverage Annual Average Reward      = {:.3f}\".format(my_avg_reward))\n",
    "    print(\"\\tTotal Violations                   = {}\".format(my_v_counter))\n",
    "\n",
    "    if (my_avg_reward > best_avg_avg_reward):\n",
    "            best_avg_avg_reward = my_avg_reward\n",
    "            best_net_avg_reward = dqn.eval_net\n",
    "\n",
    "    if (my_v_counter < best_avg_v_counter):\n",
    "        best_avg_v_counter = my_v_counter\n",
    "        best_net_v_counter = dqn.eval_net\n",
    "    elif (my_v_counter == best_avg_v_counter):\n",
    "        if (my_avg_reward > best_avg_avg_reward):\n",
    "            best_avg_v_counter = my_v_counter\n",
    "            best_net_v_counter = dqn.eval_net\n",
    "    print(\"****************************************\")\n",
    "\n",
    "    \n",
    "\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TESTING BEST MODEL BASED ON AVERAGE REWARD METRIC\n",
      "YEAR\t\tAVG_RWD\t\tVIOLATIONS\n",
      "2015 \t\t 1.19 \t\t 2\n",
      "2016 \t\t 1.25 \t\t 2\n",
      "2017 \t\t 1.24 \t\t 8\n",
      "2018 \t\t 1.25 \t\t 3\n"
     ]
    }
   ],
   "source": [
    "#TESTING BEST MODEL BASED ON AVERAGE REWARD METRIC\n",
    "dqn = DQN()\n",
    "dqn.eval_net = best_net_avg_reward\n",
    "LOCATION = 'tokyo'\n",
    "results = np.empty(3)\n",
    "for YEAR in np.arange(2015,2019):\n",
    "    capm = CAPM(LOCATION,YEAR,shuffle=False, trainmode=False) #instantiate the CAPM class\n",
    "    capm.eno = ENO(LOCATION,YEAR, shuffle=False, day_balance=False) #instantiate the environment inside the CAPM class\n",
    "    capm.HMAX = capm.eno.SMAX #maximum power output of solar cell is set in CAPM object using the value in ENO object\n",
    "\n",
    "    s, r, day_end, year_end = capm.reset()\n",
    "    yr_test_record = np.empty(4)\n",
    "    EPSILON = 1\n",
    "\n",
    "    while True:\n",
    "        a = dqn.choose_greedy_action(stdize(s))\n",
    "\n",
    "        #state = [batt, enp, henergy, fcast]\n",
    "        yr_test_record = np.vstack((yr_test_record, [s[0],s[2],r, a])) #record battery, henergy, reward and action\n",
    "\n",
    "        # take action\n",
    "        s_, r, day_end, year_end = capm.step(a)\n",
    "\n",
    "        if year_end:\n",
    "            break\n",
    "\n",
    "        s = s_\n",
    "\n",
    "    yr_test_record = np.delete(yr_test_record, 0, 0) #remove the first row which is garbage\n",
    "\n",
    "#     #Plot the reward and battery for the entire year run\n",
    "#     title = LOCATION.upper() + ',' + str(YEAR)\n",
    "\n",
    "#     NO_OF_DAYS = capm.eno.NO_OF_DAYS\n",
    "    yr_test_reward_rec = yr_test_record[:,2]\n",
    "    yr_test_reward_rec = yr_test_reward_rec[::24]\n",
    "#     print('Average Reward for',title, '=', np.mean(yr_test_reward_rec))\n",
    "#     print('Violations for',title, '=', capm.violation_counter)\n",
    "    \n",
    "    results = np.vstack((results, [int(YEAR), np.mean(yr_test_reward_rec), int(capm.violation_counter)]))\n",
    "\n",
    "#     fig = plt.figure(figsize=(24,3))\n",
    "#     fig.suptitle(title, fontsize=15)\n",
    "\n",
    "#     #     ax1 = fig.add_subplot(211)\n",
    "#     #     ax1.plot(yr_test_reward_rec)\n",
    "#     #     ax1.set_title(\"\\n\\nYear Run Reward\")\n",
    "#     #     ax1.set_ylim([-3,1])\n",
    "\n",
    "#     ax2 = fig.add_subplot(111)\n",
    "#     ax2.plot(yr_test_record[:,0],'r')\n",
    "#     ax2.set_title(\"\\n\\nYear Run Battery\")\n",
    "#     ax2.set_ylim([0,1])\n",
    "#     plt.sca(ax2)\n",
    "#     plt.xticks(np.arange(0, NO_OF_DAYS*24, 50*24),np.arange(0,NO_OF_DAYS,50))\n",
    "\n",
    "#     fig.tight_layout()\n",
    "#     plt.show()\n",
    "\n",
    "results = np.delete(results,0,0)\n",
    "\n",
    "print(\"TESTING BEST MODEL BASED ON AVERAGE REWARD METRIC\")\n",
    "print('YEAR\\t\\tAVG_RWD\\t\\tVIOLATIONS')\n",
    "for x in np.arange(0,results.shape[0]):\n",
    "    print('{} \\t\\t {} \\t\\t {}'.format(int(results[x,0]), np.around(results[x,1],2), int(results[x,-1])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TESTING BASED ON VIOLATION COUNTER METRIC\n",
      "YEAR\t\tAVG_RWD\t\tVIOLATIONS\n",
      "2015 \t\t 1.19 \t\t 2\n",
      "2016 \t\t 1.25 \t\t 2\n",
      "2017 \t\t 1.24 \t\t 8\n",
      "2018 \t\t 1.25 \t\t 3\n"
     ]
    }
   ],
   "source": [
    "#TESTING BASED ON VIOLATION COUNTER METRIC\n",
    "dqn = DQN()\n",
    "dqn.eval_net = best_net_v_counter\n",
    "\n",
    "LOCATION = 'tokyo'\n",
    "results = np.empty(3)\n",
    "for YEAR in np.arange(2015,2019):\n",
    "    capm = CAPM(LOCATION,YEAR,shuffle=False, trainmode=False) #instantiate the CAPM class\n",
    "    capm.eno = ENO(LOCATION,YEAR, shuffle=False, day_balance=False) #instantiate the environment inside the CAPM class\n",
    "    capm.HMAX = capm.eno.SMAX #maximum power output of solar cell is set in CAPM object using the value in ENO object\n",
    "\n",
    "    s, r, day_end, year_end = capm.reset()\n",
    "    yr_test_record = np.empty(4)\n",
    "    EPSILON = 1\n",
    "\n",
    "    while True:\n",
    "        a = dqn.choose_greedy_action(stdize(s))\n",
    "\n",
    "        #state = [batt, enp, henergy, fcast]\n",
    "        yr_test_record = np.vstack((yr_test_record, [s[0],s[2],r, a])) #record battery, henergy, reward and action\n",
    "\n",
    "        # take action\n",
    "        s_, r, day_end, year_end = capm.step(a)\n",
    "\n",
    "        if year_end:\n",
    "            break\n",
    "\n",
    "        s = s_\n",
    "\n",
    "    yr_test_record = np.delete(yr_test_record, 0, 0) #remove the first row which is garbage\n",
    "\n",
    "#     #Plot the reward and battery for the entire year run\n",
    "#     title = LOCATION.upper() + ',' + str(YEAR)\n",
    "\n",
    "#     NO_OF_DAYS = capm.eno.NO_OF_DAYS\n",
    "    yr_test_reward_rec = yr_test_record[:,2]\n",
    "    yr_test_reward_rec = yr_test_reward_rec[::24]\n",
    "#     print('Average Reward for',title, '=', np.mean(yr_test_reward_rec))\n",
    "#     print('Violations for',title, '=', capm.violation_counter)\n",
    "    \n",
    "    results = np.vstack((results, [int(YEAR), np.mean(yr_test_reward_rec), int(capm.violation_counter)]))\n",
    "\n",
    "#     fig = plt.figure(figsize=(24,3))\n",
    "#     fig.suptitle(title, fontsize=15)\n",
    "\n",
    "#     #     ax1 = fig.add_subplot(211)\n",
    "#     #     ax1.plot(yr_test_reward_rec)\n",
    "#     #     ax1.set_title(\"\\n\\nYear Run Reward\")\n",
    "#     #     ax1.set_ylim([-3,1])\n",
    "\n",
    "#     ax2 = fig.add_subplot(111)\n",
    "#     ax2.plot(yr_test_record[:,0],'r')\n",
    "#     ax2.set_title(\"\\n\\nYear Run Battery\")\n",
    "#     ax2.set_ylim([0,1])\n",
    "#     plt.sca(ax2)\n",
    "#     plt.xticks(np.arange(0, NO_OF_DAYS*24, 50*24),np.arange(0,NO_OF_DAYS,50))\n",
    "\n",
    "#     fig.tight_layout()\n",
    "#     plt.show()\n",
    "\n",
    "results = np.delete(results,0,0)\n",
    "\n",
    "print(\"TESTING BASED ON VIOLATION COUNTER METRIC\")\n",
    "print('YEAR\\t\\tAVG_RWD\\t\\tVIOLATIONS')\n",
    "for x in np.arange(0,results.shape[0]):\n",
    "    print('{} \\t\\t {} \\t\\t {}'.format(int(results[x,0]), np.around(results[x,1],2), int(results[x,-1])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #Plot the reward and battery for the entire year run on a day by day basis\n",
    "# title = LOCATION.upper() + ',' + str(YEAR)\n",
    "# TIME_AXIS = np.arange(0,capm.eno.TIME_STEPS)\n",
    "# for DAY in range(0,10):#capm.eno.NO_OF_DAYS):\n",
    "#     START = DAY*24\n",
    "#     END = START+24\n",
    "\n",
    "#     daytitle = title + ' - DAY ' + str(DAY)\n",
    "#     fig = plt.figure(figsize=(16,4))\n",
    "#     st = fig.suptitle(daytitle)\n",
    "\n",
    "#     ax2 = fig.add_subplot(121)\n",
    "#     ax2.plot(yr_test_record[START:END,1],'g')\n",
    "#     ax2.set_title(\"HARVESTED ENERGY\")\n",
    "#     plt.xlabel(\"Hour\")\n",
    "#     ax2.set_ylim([0,1])\n",
    "\n",
    "#     #plot battery for year run\n",
    "#     ax1 = fig.add_subplot(122)\n",
    "#     ax1.plot(TIME_AXIS,yr_test_record[START:END,0],'r') \n",
    "# #     ax1.plot(TIME_AXIS, np.ones(capm.eno.TIME_STEPS)*capm.BOPT/capm.BMAX,'r--')\n",
    "#     ax1.plot(TIME_AXIS, np.ones(capm.eno.TIME_STEPS)*capm.BOPT/capm.BMAX,'r--')\n",
    "#     ax1.text(0.1, 0.2, \"BINIT = %.2f\\n\" %(yr_test_record[START,0]),fontsize=11, ha='left')\n",
    "#     ax1.text(0.1, 0.4, \"TENP = %.2f\\n\" %(capm.BOPT/capm.BMAX-yr_test_record[END,0]),fontsize=11, ha='left')\n",
    "#     ax1.text(0.1, 0.3, \"BMEAN = %.2f\\n\" %(np.mean(yr_test_record[START:END,0])),fontsize=11, ha='left')\n",
    "\n",
    "\n",
    "\n",
    "#     ax1.set_title(\"YEAR RUN TEST\")\n",
    "#     if END < (capm.eno.NO_OF_DAYS*capm.eno.TIME_STEPS):\n",
    "#         ax1.text(0.1, 0, \"REWARD = %.2f\\n\" %(yr_test_record[END,2]),fontsize=13, ha='left')\n",
    "#     plt.xlabel(\"Hour\")\n",
    "#     ax1.set_ylabel('Battery', color='r',fontsize=12)\n",
    "#     ax1.set_ylim([0,1])\n",
    "\n",
    "#     #plot actions for year run\n",
    "#     ax1a = ax1.twinx()\n",
    "#     ax1a.plot(yr_test_record[START:END,3])\n",
    "#     ax1a.set_ylim([0,N_ACTIONS])\n",
    "#     ax1a.set_ylabel('Duty Cycle', color='b',fontsize=12)\n",
    "\n",
    "#     fig.tight_layout()\n",
    "#     st.set_y(0.95)\n",
    "#     fig.subplots_adjust(top=0.75)\n",
    "#     plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
