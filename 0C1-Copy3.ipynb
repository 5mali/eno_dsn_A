{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "from datetime import datetime\n",
    "tic = datetime.now()\n",
    "\n",
    "import os\n",
    "from os.path import dirname, abspath, join\n",
    "from os import getcwd\n",
    "import sys\n",
    "\n",
    "# THIS_DIR = getcwd()\n",
    "# CLASS_DIR = abspath(join(THIS_DIR, 'dsnclasses'))  #abspath(join(THIS_DIR, '../../..', 'dsnclasses'))\n",
    "# sys.path.append(CLASS_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import string\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.ticker as ticker\n",
    "import torch\n",
    "from IPython.display import clear_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 230\n",
    "random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "torch.cuda.manual_seed(seed)\n",
    "np.random.seed(seed)\n",
    "os.environ['PYTHONHASHSEED'] = str(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ENO(object):\n",
    "    \n",
    "    #no. of forecast types is 6 ranging from 0 to 5\n",
    "  \n",
    "    def __init__(self, location='tokyo', year=2010, shuffle=False, day_balance=False):\n",
    "        self.location = location\n",
    "        self.year = year\n",
    "        self.day = None\n",
    "        self.hr = None\n",
    "        \n",
    "        self.shuffle = shuffle\n",
    "        self.day_balance = day_balance\n",
    "\n",
    "        self.TIME_STEPS = None #no. of time steps in one episode\n",
    "        self.NO_OF_DAYS = None #no. of days in one year\n",
    "        \n",
    "        self.NO_OF_DAYTYPE = 10 #no. of daytypes\n",
    "        self.daycounter = 0 #to count number of days that have been passed\n",
    "        \n",
    "        self.sradiation = None #matrix with GSR for the entire year\n",
    "        self.senergy = None #matrix with harvested energy data for the entire year\n",
    "        self.fforecast = None #array with forecast values for each day\n",
    "        \n",
    "\n",
    "        self.henergy = None #harvested energy variable\n",
    "        self.fcast = None #forecast variable\n",
    "        self.sorted_days = [] #days sorted according to day type\n",
    "        \n",
    "        self.SMAX = 1000 # 1 Watt Solar Panel\n",
    "\n",
    "    \n",
    "    #function to get the solar data for the given location and year and prep it\n",
    "    def get_data(self):\n",
    "        #solar_data/CSV files contain the values of GSR (Global Solar Radiation in MegaJoules per meters squared per hour)\n",
    "        #weather_data/CSV files contain the weather summary from 06:00 to 18:00 and 18:00 to 06:00+1\n",
    "        location = self.location\n",
    "        year = self.year\n",
    "\n",
    "        THIS_DIR = getcwd()\n",
    "        SDATA_DIR = abspath(join(THIS_DIR, 'solar_data'))  #abspath(join(THIS_DIR, '../../..', 'data'))\n",
    "        \n",
    "        sfile = SDATA_DIR + '/' + location +'/' + str(year) + '.csv'\n",
    "        \n",
    "        #skiprows=4 to remove unnecessary title texts\n",
    "        #usecols=4 to read only the Global Solar Radiation (GSR) values\n",
    "        solar_radiation = pd.read_csv(sfile, skiprows=4, encoding='shift_jisx0213', usecols=[4])\n",
    "      \n",
    "        #convert dataframe to numpy array\n",
    "        solar_radiation = solar_radiation.values\n",
    "\n",
    "        #convert missing data in CSV files to zero\n",
    "        solar_radiation[np.isnan(solar_radiation)] = 0\n",
    "\n",
    "        #reshape solar_radiation into no_of_daysx24 array\n",
    "        solar_radiation = solar_radiation.reshape(-1,24)\n",
    "\n",
    "        if(self.shuffle): #if class instatiation calls for shuffling the day order. Required when learning\n",
    "            np.random.shuffle(solar_radiation) \n",
    "        self.sradiation = solar_radiation\n",
    "        \n",
    "        #GSR values (in MJ/sq.mts per hour) need to be expressed in mW\n",
    "        # Conversion is accomplished by \n",
    "        # solar_energy = GSR(in MJ/m2/hr) * 1e6 * size of solar cell * efficiency of solar cell /(60x60) *1000 (to express in mW)\n",
    "        # the factor of 2 in the end is assuming two solar cells\n",
    "        self.senergy = 2*self.sradiation * 1e6 * (55e-3 * 70e-3) * 0.15 * 1000/(60*60)\n",
    "\n",
    "        return 0\n",
    "    \n",
    "    #function to map total day radiation into type of day ranging from 0 to 5\n",
    "    #the classification into day types is quite arbitrary. There is no solid logic behind this type of classification.\n",
    "    \n",
    "    def get_day_state(self,tot_day_radiation):\n",
    "        bin_edges = np.array([0, 3.5, 6.5, 9.0, 12.5, 15.5, 18.5, 22.0, 25, 28])\n",
    "        for k in np.arange(1,bin_edges.size):\n",
    "            if (bin_edges[k-1] < tot_day_radiation <= bin_edges[k]):\n",
    "                day_state = k -1\n",
    "            else:\n",
    "                day_state = bin_edges.size - 1\n",
    "        return int(day_state)\n",
    "    \n",
    "    def get_forecast(self):\n",
    "        #create a perfect forecaster.\n",
    "        tot_day_radiation = np.sum(self.sradiation, axis=1) #contains total solar radiation for each day\n",
    "        get_day_state = np.vectorize(self.get_day_state)\n",
    "        self.fforecast = get_day_state(tot_day_radiation)\n",
    "        \n",
    "        #sort days depending on the type of day and shuffle them; maybe required when learning\n",
    "        for fcast in range(0,6):\n",
    "            fcast_days = ([i for i,x in enumerate(self.fforecast) if x == fcast])\n",
    "            np.random.shuffle(fcast_days)\n",
    "            self.sorted_days.append(fcast_days)\n",
    "        return 0\n",
    "    \n",
    "    def reset(self,day=0): #it is possible to reset to the beginning of a certain day\n",
    "        \n",
    "        self.get_data() #first get data for the given year\n",
    "        self.get_forecast() #calculate the forecast\n",
    "        \n",
    "        self.TIME_STEPS = self.senergy.shape[1]\n",
    "        self.NO_OF_DAYS = self.senergy.shape[0]\n",
    "        \n",
    "        self.day = day\n",
    "        self.hr = 0\n",
    "        \n",
    "        self.henergy = self.senergy[self.day][self.hr]\n",
    "        self.fcast = self.fforecast[self.day]\n",
    "        \n",
    "        end_of_day = False\n",
    "        end_of_year = False\n",
    "        return [self.henergy, self.fcast, end_of_day, end_of_year]\n",
    "\n",
    "    \n",
    "    def step(self):\n",
    "        end_of_day = False\n",
    "        end_of_year = False\n",
    "        if not(self.day_balance): #if daytype balance is not required\n",
    "            if(self.hr < self.TIME_STEPS - 1):\n",
    "                self.hr += 1\n",
    "                self.henergy = self.senergy[self.day][self.hr] \n",
    "                self.fcast = self.fforecast[self.day]\n",
    "            else:\n",
    "                if(self.day < self.NO_OF_DAYS -1):\n",
    "                    end_of_day = True\n",
    "                    self.hr = 0\n",
    "                    self.day += 1\n",
    "                    self.henergy = self.senergy[self.day][self.hr] \n",
    "                    self.fcast = self.fforecast[self.day]\n",
    "                else:\n",
    "                    end_of_day = True\n",
    "                    end_of_year = True\n",
    "                    \n",
    "        else: #when training, we want all daytypes to be equally represented for robust policy\n",
    "              #obviously, the days are going to be in random order\n",
    "            if(self.hr < self.TIME_STEPS - 1):\n",
    "                self.hr += 1\n",
    "                self.henergy = self.senergy[self.day][self.hr]\n",
    "                self.fcast = self.fforecast[self.day]\n",
    "            else:\n",
    "                if(self.daycounter < self.NO_OF_DAYS -1):\n",
    "                    end_of_day = True\n",
    "                    self.daycounter += 1\n",
    "                    self.hr = 0\n",
    "                    daytype = random.choice(np.arange(0,self.NO_OF_DAYTYPE)) #choose random daytype\n",
    "                    self.day = np.random.choice(self.sorted_days[daytype]) #choose random day from that daytype\n",
    "                    self.henergy = self.senergy[self.day][self.hr] \n",
    "                    self.fcast = self.fforecast[self.day]\n",
    "                else: \n",
    "                    end_of_day = True\n",
    "                    end_of_year = True\n",
    "                    self.daycounter = 0\n",
    "        \n",
    "        \n",
    "        return [self.henergy, self.fcast, end_of_day, end_of_year]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CAPM (object):\n",
    "    def __init__(self,location='tokyo', year=2010, shuffle=False, trainmode=False):\n",
    "\n",
    "        #all energy values i.e. BMIN, BMAX, BOPT, HMAX are in mWhr. Assuming one timestep is one hour\n",
    "        \n",
    "        self.BMIN = 0.0                #Minimum battery level that is tolerated. Maybe non-zero also\n",
    "        self.BMAX = 9250.0            #Max Battery Level. May not necessarily be equal to total batter capacity [3.6V x 2500mAh]\n",
    "        self.BOPT = 0.5 * self.BMAX    #Optimal Battery Level. Assuming 50% of battery is the optimum\n",
    "        \n",
    "        self.HMIN = 0      #Minimum energy that can be harvested by the solar panel.\n",
    "        self.HMAX = None   #Maximum energy that can be harvested by the solar panel. [500mW]\n",
    "        \n",
    "        self.DMAX = 500      #Maximum energy that can be consumed by the node in one time step. [~ 3.6V x 135mA]\n",
    "        self.N_ACTIONS = 10  #No. of different duty cycles possible\n",
    "        self.DMIN = self.DMAX/self.N_ACTIONS #Minimum energy that can be consumed by the node in one time step. [~ 3.6V x 15mA]\n",
    "        \n",
    "        self.binit = None     #battery at the beginning of day\n",
    "        self.btrack = []      #track the mean battery level for each day\n",
    "        self.atrack = []      #track the duty cycles for each day\n",
    "        self.batt = None      #battery variable\n",
    "        self.enp = None       #enp at end of hr\n",
    "        self.henergy = None   #harvested energy variable\n",
    "        self.fcast = None     #forecast variable\n",
    "        \n",
    "        self.MUBATT = 0.6\n",
    "        self.SDBATT = 0.02\n",
    "        \n",
    "        self.MUHENERGY = 0.5\n",
    "        self.SDHENERGY = 0.2\n",
    "        \n",
    "        self.MUENP = 0\n",
    "        self.SDENP = 0.02\n",
    "        \n",
    "        self.location = location\n",
    "        self.year = year\n",
    "        self.shuffle = shuffle\n",
    "        self.trainmode = trainmode\n",
    "        self.eno = None#ENO(self.location, self.year, shuffle=shuffle, day_balance=trainmode) #if trainmode is enable, then days are automatically balanced according to daytype i.e. day_balance= True\n",
    "        \n",
    "        self.day_violation_flag = False\n",
    "        self.violation_flag = False\n",
    "        self.violation_counter = 0\n",
    "\n",
    "        self.NO_OF_DAYTYPE = 10 #no. of daytypes\n",
    " \n",
    "    def reset(self,day=0,batt=-1):\n",
    "        henergy, fcast, day_end, year_end = self.eno.reset(day) #reset the eno environment\n",
    "        self.violation_flag = False\n",
    "        self.violation_counter = 0\n",
    "        if(batt == -1):\n",
    "            self.batt = self.BOPT\n",
    "        else:\n",
    "            self.batt = batt\n",
    "            \n",
    "        self.batt = np.clip(self.batt, self.BMIN, self.BMAX)\n",
    "        self.binit = self.batt\n",
    "        self.btrack = np.append(self.btrack, self.batt) #track battery levels\n",
    "\n",
    "#         self.enp = self.BOPT - self.batt\n",
    "        self.enp = self.binit - self.batt #enp is calculated\n",
    "        self.henergy = np.clip(henergy, self.HMIN, self.HMAX) #clip henergy within HMIN and HMAX\n",
    "        self.fcast = fcast\n",
    "        \n",
    "        norm_batt = self.batt/self.BMAX\n",
    "        norm_enp = self.enp/(self.BMAX/2)\n",
    "        norm_henergy = self.henergy/self.HMAX\n",
    "        norm_fcast = self.fcast/(self.NO_OF_DAYTYPE-1)\n",
    "\n",
    "        c_state = [norm_batt, norm_enp, norm_henergy, norm_fcast] #continuous states\n",
    "        reward = 0\n",
    "        \n",
    "        return [c_state, reward, day_end, year_end]\n",
    "    \n",
    "    def getstate(self): #query the present state of the system\n",
    "        norm_batt = self.batt/self.BMAX - self.MUBATT\n",
    "        norm_enp = self.enp/(self.BMAX/2)\n",
    "        norm_henergy = self.henergy/self.HMAX\n",
    "        norm_fcast = self.fcast/(self.NO_OF_DAYTYPE-1)        \n",
    "        c_state = [norm_batt, norm_enp, norm_henergy] #continuous states\n",
    "\n",
    "        return c_state\n",
    "    \n",
    "#     def rewardfn(self):\n",
    "#         R_PARAM = 20000 #chosen empirically for best results\n",
    "#         mu = 0\n",
    "#         sig = 0.07*R_PARAM #knee curve starts at approx. 2000mWhr of deviation\n",
    "#         norm_reward = 3*(np.exp(-np.power((self.enp - mu)/sig, 2.)/2) / np.exp(-np.power((0 - mu)/sig, 2.)/2))-1\n",
    "\n",
    "        \n",
    "# #         if(np.abs(self.enp) <= 0.12*R_PARAM):\n",
    "# #             norm_reward = 2*(np.exp(-np.power((self.enp - mu)/sig, 2.)/2) / np.exp(-np.power((0 - mu)/sig, 2.)/2))\n",
    "# #         else:\n",
    "# #             norm_reward = -0.25 - 10*np.abs(self.enp/R_PARAM)\n",
    "#         if(self.day_violation_flag):\n",
    "#             norm_reward -= 3\n",
    "            \n",
    "#         return (norm_reward)\n",
    "        \n",
    "    \n",
    "    #reward function\n",
    "    def rewardfn(self):\n",
    "        \n",
    "        #FIRST REWARD AS A FUNCTION OF DRIFT OF BMEAN FROM BOPT i.e. in terms of BDEV = |BMEAN-BOPT|/BMAX\n",
    "        bmean = np.mean(self.btrack)\n",
    "        bdev = np.abs(self.BOPT - bmean)/self.BMAX\n",
    "        # based on the sigmoid function\n",
    "        # bdev ranges from bdev = (0,0.5) of BMAX\n",
    "        p1_sharpness = 10\n",
    "        n1_sharpness = 20\n",
    "        shift1 = 0.5\n",
    "        # r1(x) = 0.5 when x = 0.25. \n",
    "        # Therefore, shift = 0.5 to make sure that (2*x-shift) evaluates to zero at x = 0.25\n",
    "\n",
    "        if(bdev<=0.25): \n",
    "            r1 = 2*(1-(1 / (1 + np.exp(-p1_sharpness*(2*bdev-shift1)))))-1\n",
    "        else: \n",
    "            r1 = 2*(1-(1 / (1 + np.exp(-n1_sharpness*(2*bdev-shift1)))))-1\n",
    "        # r1 ranges from -1 to 1\n",
    "            \n",
    "        #SECOND REWARD AS A FUNCTION OF ENP AS LONG AS BMAX/4 <= batt <= 3*BMAX/4 i.e. bdev <= 0.25\n",
    "        if(bdev <=0.25):\n",
    "            # enp ranges from enp = (0,3) of DMAX\n",
    "            p2_sharpness = 2\n",
    "            n2_sharpness = 2\n",
    "            shift2 = 6    \n",
    "            # r1(x) = 0.5 when x = 2. \n",
    "            # Therefore, shift = 6 to make sure that (3*x-shift) evaluates to zero at x = 2\n",
    "#             print('Day energy', np.sum(self.eno.senergy[self.eno.day]))\n",
    "#             print('Node energy', np.sum(self.atrack)*self.DMAX/self.N_ACTIONS)\n",
    "#             x = np.abs(np.sum(self.eno.senergy[self.eno.day])-np.sum(self.atrack)*self.DMAX/self.N_ACTIONS )/self.DMAX\n",
    "            x = np.abs(self.enp/self.DMAX)\n",
    "            if(x<=2): \n",
    "                r2 = (1 / (1 + np.exp(p2_sharpness*(3*x-shift2))))\n",
    "            else: \n",
    "                r2 = (1 / (1 + np.exp(n2_sharpness*(3*x-shift2))))\n",
    "        else:\n",
    "            r2 = 0 # if mean battery lies outside bdev limits, then enp reward is not considered.\n",
    "        # r2 ranges from 0 to 1\n",
    "\n",
    "        #REWARD AS A FUNCTION OF BATTERY VIOLATIONS\n",
    "        if(self.day_violation_flag):\n",
    "            violation_penalty = 3\n",
    "        else:\n",
    "            violation_penalty = 0 #penalty for violating battery limits anytime during the day\n",
    "        \n",
    "#         print(\"Reward \", (r1 + r2 - violation_penalty), '\\n')\n",
    "        return (r1*(2**r2) - violation_penalty)\n",
    "    \n",
    "    def step(self, action):\n",
    "        day_end = False\n",
    "        year_end = False\n",
    "        self.violation_flag = False\n",
    "        reward = 0\n",
    "       \n",
    "        action = np.clip(action, 0, self.N_ACTIONS-1) #action values range from (0 to N_ACTIONS-1)\n",
    "        self.atrack = np.append(self.atrack, action+1) #track duty cycles\n",
    "        e_consumed = (action+1)*self.DMAX/self.N_ACTIONS   #energy consumed by the node\n",
    "        \n",
    "        self.batt += (self.henergy - e_consumed)\n",
    "        if(self.batt < 0.02*self.BMAX or self.batt > 0.98*self.BMAX ):\n",
    "            self.violation_flag = True #penalty for violating battery limits everytime it happens\n",
    "            reward = -2\n",
    "        if(self.batt < 0.02*self.BMAX):\n",
    "            reward -= 2\n",
    "            \n",
    "        if(self.violation_flag):\n",
    "            if(self.day_violation_flag == False): #penalty for violating battery limits anytime during the day - triggers once everyday\n",
    "                self.violation_counter += 1\n",
    "                self.day_violation_flag = True\n",
    "            \n",
    "        self.batt = np.clip(self.batt, self.BMIN, self.BMAX) #clip battery values within permitted level\n",
    "        self.btrack = np.append(self.btrack, self.batt) #track battery levels\n",
    "\n",
    "#         self.enp = self.BOPT - self.batt \n",
    "        self.enp = self.binit - self.atrack.sum()*self.DMAX/self.N_ACTIONS\n",
    "        \n",
    "        #proceed to the next time step\n",
    "        self.henergy, self.fcast, day_end, year_end = self.eno.step()\n",
    "        self.henergy = np.clip(self.henergy, self.HMIN, self.HMAX) #clip henergy within HMIN and HMAX\n",
    "                \n",
    "        if(day_end): #if eno object flags that the day has ended then give reward\n",
    "            reward += self.rewardfn()\n",
    "             \n",
    "            if (self.trainmode): #reset battery to optimal level if limits are exceeded when training\n",
    "#                 self.batt = np.random.uniform(self.DMAX*self.eno.TIME_STEPS/self.BMAX,0.8)*self.BMAX\n",
    "#                 if (self.violation_flag):\n",
    "                if np.random.uniform() < HELP : #occasionaly reset the battery\n",
    "                    self.batt = self.BOPT  \n",
    "            \n",
    "            self.day_violation_flag = False\n",
    "            self.binit = self.batt #this will be the new initial battery level for next day\n",
    "            self.btrack = [] #clear battery tracker\n",
    "            self.atrack = [] #clear duty cycle tracker\n",
    "            \n",
    "                    \n",
    "                \n",
    "        norm_batt = self.batt/self.BMAX\n",
    "        norm_enp = self.enp/(self.BMAX/2)\n",
    "        norm_henergy = self.henergy/self.HMAX\n",
    "        norm_fcast = self.fcast/(self.NO_OF_DAYTYPE-1)\n",
    "\n",
    "        c_state = [norm_batt, norm_enp, norm_henergy, norm_fcast] #continuous states\n",
    "        return [c_state, reward, day_end, year_end]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyper Parameters\n",
    "BATCH_SIZE = 32\n",
    "LR = 0.0001          # learning rate\n",
    "EPSILON = 0.9               # greedy policy\n",
    "GAMMA = 0.9                 # reward discount\n",
    "LAMBDA = 0.9                # parameter decay\n",
    "TARGET_REPLACE_ITER = 24*7*4*18    # target update frequency (every two months)\n",
    "MEMORY_CAPACITY     = 24*7*4*12*2      # store upto six month worth of memory   \n",
    "\n",
    "N_ACTIONS = 10 #no. of duty cycles (0,1,2,3,4)\n",
    "N_STATES = 4 #number of state space parameter [batt, enp, henergy, fcast]\n",
    "\n",
    "HIDDEN_LAYER = 50\n",
    "NO_OF_ITERATIONS = 50\n",
    "GPU = False\n",
    "HELP = 0.05"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "\n",
    "#Class definitions for NN model and learning algorithm\n",
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "\n",
    "        self.fc1 = nn.Linear(N_STATES, HIDDEN_LAYER)\n",
    "        nn.init.kaiming_uniform_(self.fc1.weight)\n",
    "        \n",
    "        self.fc2 = nn.Linear(HIDDEN_LAYER, HIDDEN_LAYER)\n",
    "        nn.init.kaiming_uniform_(self.fc2.weight)\n",
    "\n",
    "        self.out = nn.Linear(HIDDEN_LAYER, N_ACTIONS)\n",
    "        nn.init.xavier_uniform_(self.out.weight) \n",
    "\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.fc1(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.fc2(x)\n",
    "        x = F.relu(x)\n",
    "        actions_value = self.out(x)\n",
    "        return actions_value\n",
    "    \n",
    "class DQN(object):\n",
    "    def __init__(self):\n",
    "        if(GPU): \n",
    "            device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "        else:\n",
    "            device = torch.device('cpu')\n",
    "        self.eval_net, self.target_net = Net(), Net()\n",
    "        self.eval_net.to(device)\n",
    "        self.target_net.to(device)\n",
    "        self.device = device\n",
    "#         print(\"Neural net\")\n",
    "#         print(self.eval_net)\n",
    "        self.learn_step_counter = 0                                     # for target updating\n",
    "        self.memory_counter = 0                                         # for storing memory\n",
    "        self.memory = np.zeros((MEMORY_CAPACITY, N_STATES * 2 + 2))     # initialize memory [mem: ([s], a, r, [s_]) ]\n",
    "        self.optimizer = torch.optim.Adam(self.eval_net.parameters(), lr=LR, weight_decay=1e-3)\n",
    "        self.loss_func = nn.MSELoss()\n",
    "        self.nettoggle = False\n",
    "\n",
    "    def choose_action(self, x):\n",
    "        x = torch.unsqueeze(torch.FloatTensor(x), 0)\n",
    "        x = x.to(self.device)\n",
    "\n",
    "        # input only one sample\n",
    "        if np.random.uniform() < EPSILON:   # greedy\n",
    "            actions_value = self.eval_net.forward(x)\n",
    "            actions_value = actions_value.to(torch.device(\"cpu\"))\n",
    "            action = torch.max(actions_value, 1)[1].data.numpy()\n",
    "            action = action[0] # return the argmax index\n",
    "        else:   # random\n",
    "            action = np.random.randint(0, N_ACTIONS)\n",
    "            action = action\n",
    "        return action\n",
    "    \n",
    "    def choose_greedy_action(self, x):\n",
    "        x = torch.unsqueeze(torch.FloatTensor(x), 0)\n",
    "        x = x.to(self.device)\n",
    "\n",
    "        # input only one sample\n",
    "        if True:   # greedy\n",
    "            actions_value = self.eval_net.forward(x)\n",
    "            actions_value = actions_value.to(torch.device(\"cpu\"))\n",
    "            action = torch.max(actions_value, 1)[1].data.numpy()\n",
    "            action = action[0] # return the argmax index\n",
    "        return action\n",
    "\n",
    "    def store_transition(self, s, a, r, s_):\n",
    "        transition = np.hstack((s, [a, r], s_))\n",
    "        # replace the old memory with new memory\n",
    "        index = self.memory_counter % MEMORY_CAPACITY\n",
    "        self.memory[index, :] = transition\n",
    "        self.memory_counter += 1\n",
    "    \n",
    "    def store_day_transition(self, transition_rec):\n",
    "        data = transition_rec\n",
    "        index = self.memory_counter % MEMORY_CAPACITY\n",
    "        self.memory= np.insert(self.memory, index, data,0)\n",
    "        self.memory_counter += transition_rec.shape[0]\n",
    "\n",
    "    def learn(self):\n",
    "        # target parameter update\n",
    "        if self.learn_step_counter % TARGET_REPLACE_ITER == 0:\n",
    "            self.target_net.load_state_dict(self.eval_net.state_dict())\n",
    "            self.nettoggle = not self.nettoggle\n",
    "        self.learn_step_counter += 1\n",
    "\n",
    "        # sample batch transitions\n",
    "        sample_index = np.random.choice(MEMORY_CAPACITY, BATCH_SIZE)\n",
    "        b_memory = self.memory[sample_index, :]\n",
    "        b_s = torch.FloatTensor(b_memory[:, :N_STATES])\n",
    "        b_a = torch.LongTensor(b_memory[:, N_STATES:N_STATES+1].astype(int))\n",
    "        b_r = torch.FloatTensor(b_memory[:, N_STATES+1:N_STATES+2])\n",
    "        b_s_ = torch.FloatTensor(b_memory[:, -N_STATES:])\n",
    "        \n",
    "        b_s = b_s.to(self.device)\n",
    "        b_a = b_a.to(self.device)\n",
    "        b_r = b_r.to(self.device)\n",
    "        b_s_ = b_s_.to(self.device)\n",
    "\n",
    "        # q_eval w.r.t the action in experience\n",
    "        q_eval = self.eval_net(b_s).gather(1, b_a)  # shape (batch, 1)\n",
    "        q_next = self.target_net(b_s_).detach()     # detach from graph, don't backpropagate\n",
    "        q_target = b_r + GAMMA * q_next.max(1)[0].view(BATCH_SIZE, 1)   # shape (batch, 1)\n",
    "        loss = self.loss_func(q_eval, q_target)\n",
    "\n",
    "        self.optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        self.optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def stdize(s):\n",
    "    MU_BATT = 0.5\n",
    "    SD_BATT = 0.15\n",
    "    \n",
    "    MU_ENP = 0\n",
    "    SD_ENP = 0.15\n",
    "    \n",
    "    MU_HENERGY = 0.35\n",
    "    SD_HENERGY = 0.25\n",
    "    \n",
    "    MU_FCAST = 0.42\n",
    "    SD_FCAST = 0.27\n",
    "    \n",
    "    norm_batt, norm_enp, norm_henergy, norm_fcast = s\n",
    "    \n",
    "    std_batt = (norm_batt - MU_BATT)/SD_BATT\n",
    "    std_enp = (norm_enp - MU_ENP)/SD_ENP\n",
    "    std_henergy = (norm_henergy - MU_HENERGY)/SD_HENERGY\n",
    "    std_fcast = (norm_fcast - MU_FCAST)/SD_FCAST\n",
    "\n",
    "\n",
    "    return [std_batt, std_enp, std_henergy, std_fcast]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "TRAINING IN PROGRESS\n",
      "\n",
      "Device:  cpu\n",
      "\n",
      "Iteration 0:  TOKYO, 2002 \n",
      "Average Reward \t\t= -3.153\n",
      "Violation Counter \t= 243\n",
      "\n",
      "Iteration 1:  TOKYO, 2004 \n",
      "Average Reward \t\t= -3.468\n",
      "Violation Counter \t= 250\n",
      "\n",
      "Iteration 2:  TOKYO, 2008 \n",
      "Average Reward \t\t= -2.994\n",
      "Violation Counter \t= 189\n",
      "\n",
      "Iteration 3:  TOKYO, 2000 \n",
      "Average Reward \t\t= -3.657\n",
      "Violation Counter \t= 241\n",
      "\n",
      "Iteration 4:  TOKYO, 2006 \n",
      "Average Reward \t\t= -2.142\n",
      "Violation Counter \t= 166\n",
      "\n",
      "Iteration 5:  TOKYO, 2000 \n",
      "Average Reward \t\t= -1.357\n",
      "Violation Counter \t= 129\n",
      "\n",
      "Iteration 6:  TOKYO, 2007 \n",
      "Average Reward \t\t= -1.067\n",
      "Violation Counter \t= 110\n",
      "\n",
      "Iteration 7:  TOKYO, 2007 \n",
      "Average Reward \t\t= 0.041\n",
      "Violation Counter \t= 47\n",
      "\n",
      "Iteration 8:  TOKYO, 2006 \n",
      "Average Reward \t\t= 0.164\n",
      "Violation Counter \t= 45\n",
      "\n",
      "Iteration 9:  TOKYO, 2009 \n",
      "Average Reward \t\t= 0.773\n",
      "Violation Counter \t= 21\n",
      "\n",
      "Iteration 10:  TOKYO, 2008 \n",
      "Average Reward \t\t= 0.750\n",
      "Violation Counter \t= 22\n",
      "\n",
      "Iteration 11:  TOKYO, 2004 \n",
      "Average Reward \t\t= 0.532\n",
      "Violation Counter \t= 31\n",
      "\n",
      "Iteration 12:  TOKYO, 2009 \n",
      "Average Reward \t\t= 0.819\n",
      "Violation Counter \t= 16\n",
      "\n",
      "Iteration 13:  TOKYO, 2000 \n",
      "Average Reward \t\t= 0.253\n",
      "Violation Counter \t= 42\n",
      "\n",
      "Iteration 14:  TOKYO, 2005 \n",
      "Average Reward \t\t= 0.925\n",
      "Violation Counter \t= 18\n",
      "\n",
      "Iteration 15:  TOKYO, 2003 \n",
      "Average Reward \t\t= 0.678\n",
      "Violation Counter \t= 21\n",
      "\n",
      "Iteration 16:  TOKYO, 2000 \n",
      "Average Reward \t\t= 0.634\n",
      "Violation Counter \t= 23\n",
      "\n",
      "Iteration 17:  TOKYO, 2005 \n",
      "Average Reward \t\t= 0.984\n",
      "Violation Counter \t= 12\n",
      "\n",
      "Iteration 18:  TOKYO, 2004 \n",
      "Average Reward \t\t= 0.785\n",
      "Violation Counter \t= 18\n",
      "\n",
      "Iteration 19:  TOKYO, 2002 \n",
      "Average Reward \t\t= 0.239\n",
      "Violation Counter \t= 39\n",
      "\n",
      "Iteration 20:  TOKYO, 2002 \n",
      "Average Reward \t\t= 0.459\n",
      "Violation Counter \t= 31\n",
      "\n",
      "Iteration 21:  TOKYO, 2007 \n",
      "Average Reward \t\t= 0.736\n",
      "Violation Counter \t= 20\n",
      "\n",
      "Iteration 22:  TOKYO, 2009 \n",
      "Average Reward \t\t= 0.818\n",
      "Violation Counter \t= 21\n",
      "\n",
      "Iteration 23:  TOKYO, 2006 \n",
      "Average Reward \t\t= 0.571\n",
      "Violation Counter \t= 29\n",
      "\n",
      "Iteration 24:  TOKYO, 2009 \n",
      "Average Reward \t\t= 0.934\n",
      "Violation Counter \t= 16\n",
      "\n",
      "Iteration 25:  TOKYO, 2007 \n",
      "Average Reward \t\t= 0.981\n",
      "Violation Counter \t= 14\n",
      "\n",
      "Iteration 26:  TOKYO, 2004 \n",
      "Average Reward \t\t= 1.039\n",
      "Violation Counter \t= 12\n",
      "\n",
      "Iteration 27:  TOKYO, 2005 \n",
      "Average Reward \t\t= 1.187\n",
      "Violation Counter \t= 8\n",
      "\n",
      "Iteration 28:  TOKYO, 2000 \n",
      "Average Reward \t\t= 0.973\n",
      "Violation Counter \t= 12\n",
      "\n",
      "Iteration 29:  TOKYO, 2000 \n",
      "Average Reward \t\t= 0.768\n",
      "Violation Counter \t= 22\n",
      "\n",
      "Iteration 30:  TOKYO, 2001 \n",
      "Average Reward \t\t= 0.850\n",
      "Violation Counter \t= 23\n",
      "\n",
      "Iteration 31:  TOKYO, 2005 \n",
      "Average Reward \t\t= 1.117\n",
      "Violation Counter \t= 14\n",
      "\n",
      "Iteration 32:  TOKYO, 2007 \n",
      "Average Reward \t\t= 1.086\n",
      "Violation Counter \t= 10\n",
      "\n",
      "Iteration 33:  TOKYO, 2008 \n",
      "Average Reward \t\t= 1.128\n",
      "Violation Counter \t= 9\n",
      "\n",
      "Iteration 34:  TOKYO, 2004 \n",
      "Average Reward \t\t= 1.092\n",
      "Violation Counter \t= 13\n",
      "\n",
      "Iteration 35:  TOKYO, 2003 \n",
      "Average Reward \t\t= 0.935\n",
      "Violation Counter \t= 15\n",
      "\n",
      "Iteration 36:  TOKYO, 2003 \n",
      "Average Reward \t\t= 0.766\n",
      "Violation Counter \t= 21\n",
      "\n",
      "Iteration 37:  TOKYO, 2007 \n",
      "Average Reward \t\t= 0.988\n",
      "Violation Counter \t= 17\n",
      "\n",
      "Iteration 38:  TOKYO, 2006 \n",
      "Average Reward \t\t= 1.041\n",
      "Violation Counter \t= 10\n",
      "\n",
      "Iteration 39:  TOKYO, 2001 \n",
      "Average Reward \t\t= 1.159\n",
      "Violation Counter \t= 6\n",
      "\n",
      "Iteration 40:  TOKYO, 2009 \n",
      "Average Reward \t\t= 0.915\n",
      "Violation Counter \t= 16\n",
      "\n",
      "Iteration 41:  TOKYO, 2006 \n",
      "Average Reward \t\t= 0.734\n",
      "Violation Counter \t= 26\n",
      "\n",
      "Iteration 42:  TOKYO, 2007 \n",
      "Average Reward \t\t= 0.987\n",
      "Violation Counter \t= 14\n",
      "\n",
      "Iteration 43:  TOKYO, 2007 \n",
      "Average Reward \t\t= 0.878\n",
      "Violation Counter \t= 21\n",
      "\n",
      "Iteration 44:  TOKYO, 2005 \n",
      "Average Reward \t\t= 1.215\n",
      "Violation Counter \t= 7\n",
      "\n",
      "Iteration 45:  TOKYO, 2000 \n",
      "Average Reward \t\t= 0.973\n",
      "Violation Counter \t= 13\n",
      "\n",
      "Iteration 46:  TOKYO, 2004 \n",
      "Average Reward \t\t= 1.033\n",
      "Violation Counter \t= 17\n",
      "\n",
      "Iteration 47:  TOKYO, 2004 \n",
      "Average Reward \t\t= 1.002\n",
      "Violation Counter \t= 15\n",
      "\n",
      "Iteration 48:  TOKYO, 2005 \n",
      "Average Reward \t\t= 0.818\n",
      "Violation Counter \t= 28\n",
      "\n",
      "Iteration 49:  TOKYO, 2002 \n",
      "Average Reward \t\t= 0.646\n",
      "Violation Counter \t= 32\n"
     ]
    }
   ],
   "source": [
    "#TRAIN \n",
    "dqn = DQN()\n",
    "# for recording weights\n",
    "oldfc1 = dqn.eval_net.fc1.weight.data.cpu().numpy().flatten()\n",
    "old2fc1 = oldfc1\n",
    "\n",
    "oldfc2 = dqn.eval_net.fc2.weight.data.cpu().numpy().flatten()\n",
    "old2fc2 = oldfc2\n",
    "\n",
    "# oldfc3 = dqn.eval_net.fc3.weight.data.cpu().numpy().flatten()\n",
    "# old2fc3 = oldfc3\n",
    "\n",
    "oldout = dqn.eval_net.out.weight.data.cpu().numpy().flatten()\n",
    "old2out = oldout\n",
    "########################################\n",
    "\n",
    "best_iteration = -1\n",
    "best_avg_reward = -1000 #initialize best average reward to very low value\n",
    "reset_counter = 0 #count number of times the battery had to be reset\n",
    "change_hr = 0\n",
    "# PFILENAME = ''.join(random.choice(string.ascii_uppercase + string.digits) for _ in range(8)) #create random filename\n",
    "# BFILENAME = \"best\"+PFILENAME + \".pt\" #this file stores the best model\n",
    "# TFILENAME = \"terminal\"+PFILENAME + \".pt\" #this file stores the last model\n",
    "\n",
    "avg_reward_rec = [] #record the yearly average rewards over the entire duration of training\n",
    "violation_rec = []\n",
    "print('\\nTRAINING IN PROGRESS\\n')\n",
    "print('Device: ', dqn.device)\n",
    "\n",
    "for iteration in range(NO_OF_ITERATIONS):\n",
    "    LOCATION = 'tokyo'#random.choice(['tokyo','wakkanai','minamidaito'])\n",
    "    YEAR = random.choice(np.arange(2000,2010))\n",
    "    capm = CAPM(LOCATION,YEAR,shuffle=False, trainmode=False) #instantiate the CAPM class\n",
    "    capm.eno = ENO(LOCATION,YEAR, shuffle=False, day_balance=False) #instantiate the environment inside the CAPM class\n",
    "    capm.HMAX = capm.eno.SMAX #maximum power output of solar cell is set in CAPM object using the value in ENO object\n",
    "#     clear_output()\n",
    "    print('\\nIteration {}:  {}, {} '.format(iteration, LOCATION.upper(), YEAR))\n",
    "\n",
    "    s, r, day_end, year_end = capm.reset()\n",
    "    yr_record = np.empty(4)\n",
    "\n",
    "    record = np.empty(4) #record for battery, henergy, reward and action\n",
    "    transition_rec = np.zeros((capm.eno.TIME_STEPS, N_STATES * 2 + 2)) #record all the transition in one day\n",
    "\n",
    "    while True:\n",
    "        a = dqn.choose_action(stdize(s))\n",
    "\n",
    "        # present state = [batt, enp, henergy]\n",
    "        record = np.vstack((record, [s[0],s[2],r, a])) # record battery, henergy, reward and action for troubleshooting\n",
    "        yr_record = np.vstack((yr_record, [s[0],s[2],r, a]))\n",
    "\n",
    "        # take action\n",
    "        s_, r, day_end, year_end = capm.step(a)\n",
    "        \n",
    "        temp_transitions = np.hstack((stdize(s), [a, r], stdize(s_)))\n",
    "        transition_rec[capm.eno.hr-1,:] = temp_transitions\n",
    "\n",
    "        if (day_end):\n",
    "            transition_rec[:,5] += r #broadcast reward to all states\n",
    "            decay_factor = [i for i in (LAMBDA**n for n in reversed(range(0, capm.eno.TIME_STEPS)))]\n",
    "            transition_rec[:,5] = transition_rec[:,5] * decay_factor #decay reward proportionately\n",
    "            dqn.store_day_transition(transition_rec)\n",
    "\n",
    "        if dqn.memory_counter > MEMORY_CAPACITY:\n",
    "            dqn.learn()\n",
    "\n",
    "        if dqn.nettoggle:\n",
    "            change_hr = capm.eno.day*24+capm.eno.hr #to mark when the DQN is updated.\n",
    "            dqn.nettoggle = not dqn.nettoggle\n",
    "\n",
    "        if (year_end):\n",
    "            break\n",
    "\n",
    "        # transition to new state\n",
    "        s = s_\n",
    "\n",
    "    record = np.delete(record, 0, 0) #remove the first row which is garbage\n",
    "    reward_rec = record[:,2] #extract reward information from the record array\n",
    "    reward_rec = reward_rec[::24] #only consider terminal rewards\n",
    "    print(\"Average Reward \\t\\t= {:.3f}\".format(np.mean(reward_rec)))\n",
    "    print(\"Violation Counter \\t= {}\".format(capm.violation_counter))\n",
    "\n",
    "#     if(best_avg_reward < np.mean(reward_rec)):\n",
    "#         best_avg_reward = np.mean(reward_rec)\n",
    "    \n",
    "#     if(best_avg_reward > 1.5 or iteration > 20):\n",
    "#         EPSILON = 0.9\n",
    "#         LR = 0.01\n",
    "        \n",
    "#     if (capm.violation_counter < 5):\n",
    "#         reset_flag = False\n",
    "#         EPSILON = 0.95\n",
    "#         LR = 0.001\n",
    "        \n",
    "\n",
    "#     # Check if reward beats the High Score and possible save it    \n",
    "#     if (iteration > 19): #save the best models only after 20 iterations\n",
    "#         print(\"Best Score \\t = {:8.3f} @ Iteration No. {}\".format(best_avg_reward, best_iteration))\n",
    "#         if(best_avg_reward < np.mean(reward_rec)):\n",
    "#             best_iteration = iteration\n",
    "#             best_avg_reward = np.mean(reward_rec)\n",
    "#             print(\"Saving Model\")\n",
    "#             torch.save(dqn.eval_net.state_dict(), BFILENAME)\n",
    "#     else:\n",
    "#         print(\"\\r\")\n",
    "\n",
    "    # Log the average reward in avg_reward_rec\n",
    "    avg_reward_rec = np.append(avg_reward_rec, np.mean(reward_rec))\n",
    "    violation_rec = np.append(violation_rec, capm.violation_counter)\n",
    "\n",
    "    \n",
    "###########################################################################################\n",
    "# #   PLOT battery levels, hourly rewards and the weights\n",
    "#     yr_record = np.delete(yr_record, 0, 0) #remove the first row which is garbage\n",
    "# #     NO_OF_DAYS = capm.eno.NO_OF_DAYS\n",
    "#     hourly_yr_reward_rec = yr_record[:,2]\n",
    "#     yr_reward_rec = hourly_yr_reward_rec[::24]\n",
    "\n",
    "    \n",
    "#     fig = plt.figure(figsize=(24,3))\n",
    "#     TIME_STEPS = capm.eno.TIME_STEPS\n",
    "#     NO_OF_DAYS = capm.eno.NO_OF_DAYS\n",
    "#     DAY_SPACING = 15\n",
    "#     TICK_SPACING = TIME_STEPS*DAY_SPACING\n",
    "#     #plot battery\n",
    "#     ax = fig.add_subplot(111)\n",
    "#     ax.plot(np.arange(0,TIME_STEPS*NO_OF_DAYS),yr_record[:,0],'r')\n",
    "#     ax.set_ylim([0,1])\n",
    "#     ax.axvline(x=change_hr)\n",
    "#     ax.xaxis.set_major_locator(ticker.MultipleLocator(TICK_SPACING))\n",
    "# #     labels = [item for item in ax.get_xticklabels()]\n",
    "# #     print(labels)\n",
    "# #     labels [15:-1] = np.arange(0,NO_OF_DAYS,DAY_SPACING) #the first label is reserved to negative values\n",
    "# #     ax.set_xticklabels(labels)\n",
    "#     #plot hourly reward\n",
    "#     ax0 = ax.twinx()\n",
    "#     ax0.plot(hourly_yr_reward_rec, color='m')\n",
    "#     ax0.set_ylim(-7,3)\n",
    "#     plt.show()\n",
    "\n",
    "\n",
    "#     fig = plt.figure(figsize=(18,3))\n",
    "#     ax1 = fig.add_subplot(131)\n",
    "#     newfc1 = dqn.eval_net.fc1.weight.data.cpu().numpy().flatten()\n",
    "#     ax1.plot(old2fc1,color='b', alpha=0.4)\n",
    "#     ax1.plot(oldfc1,color='b',alpha = 0.7)\n",
    "#     ax1.plot(newfc1,color='b')\n",
    "#     old2fc1 = oldfc1\n",
    "#     oldfc1 = newfc1\n",
    "    \n",
    "#     ax2 = fig.add_subplot(132)\n",
    "#     newfc2 = dqn.eval_net.fc2.weight.data.cpu().numpy().flatten()\n",
    "#     ax2.plot(old2fc2,color='y', alpha=0.4)\n",
    "#     ax2.plot(oldfc2,color='y',alpha = 0.7)\n",
    "#     ax2.plot(newfc2,color='y')\n",
    "#     old2fc2 = oldfc2\n",
    "#     oldfc2 = newfc2\n",
    "    \n",
    "# #     ax3 = fig.add_subplot(143)\n",
    "# #     newfc3 = dqn.eval_net.fc3.weight.data.cpu().numpy().flatten()\n",
    "# #     ax3.plot(old2fc3,color='y', alpha=0.4)\n",
    "# #     ax3.plot(oldfc3,color='y',alpha = 0.7)\n",
    "# #     ax3.plot(newfc3,color='y')\n",
    "# #     old2fc3 = oldfc3\n",
    "# #     oldfc3 = newfc3\n",
    "    \n",
    "#     axO = fig.add_subplot(133)\n",
    "#     newout = dqn.eval_net.out.weight.data.cpu().numpy().flatten()\n",
    "#     axO.plot(old2out,color='g', alpha=0.4)\n",
    "#     axO.plot(oldout,color='g',alpha=0.7)\n",
    "#     axO.plot(newout,color='g')\n",
    "#     old2out = oldout\n",
    "#     oldout = newout\n",
    "    \n",
    "#     fig.tight_layout()\n",
    "#     plt.show()\n",
    "\n",
    "\n",
    "    # End of training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 648x216 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig = plt.figure(figsize=(9,3))\n",
    "# Plot the average reward log\n",
    "\n",
    "ax1 = fig.add_subplot(111)\n",
    "ax1.set_ylabel(\"Reward\")\n",
    "# ax1.set_ylim([-3,3]);\n",
    "ax1.plot(avg_reward_rec,'b')\n",
    "ax1.tick_params(axis='y', colors='b')\n",
    "\n",
    "#Plot the violation record log\n",
    "ax2 = ax1.twinx()\n",
    "ax2.set_ylabel(\"Violations\",color = 'r')\n",
    "ax2.plot(violation_rec,'r')\n",
    "for xpt in np.argwhere(violation_rec<1):\n",
    "    ax2.axvline(x=xpt,color='g')\n",
    "ax2.set_ylim([0,50]);\n",
    "ax2.tick_params(axis='y', colors='r')\n",
    "\n",
    "fig.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device:  cpu\n",
      "runtime: 0:20:45.564672\n"
     ]
    }
   ],
   "source": [
    "print('Device: ', dqn.device)\n",
    "print('runtime: {}'.format(datetime.now() - tic))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPSILON =  0.98\n",
      "LR      =  5e-05\n",
      "\n",
      "LAST PHASE ITERATION #0:  TOKYO, 2006 \n",
      "Average Reward \t\t= 1.247\n",
      "Violation Counter \t= 3\n",
      "***MEASURING PERFORMANCE OF THE MODEL***\n",
      "\tBEST AVERAGE ANNUAL AVERAGE REWARD = -1000.000\n",
      "\tBEST TOTAL VIOLATIONS              = 1000\n",
      "\n",
      "\tAverage Annual Average Reward      = 1.316\n",
      "\tTotal Violations                   = 26.0\n",
      "****************************************\n",
      "\n",
      "LAST PHASE ITERATION #1:  TOKYO, 2003 \n",
      "Average Reward \t\t= 1.302\n",
      "Violation Counter \t= 3\n",
      "***MEASURING PERFORMANCE OF THE MODEL***\n",
      "\tBEST AVERAGE ANNUAL AVERAGE REWARD = 1.316\n",
      "\tBEST TOTAL VIOLATIONS              = 26.0\n",
      "\n",
      "\tAverage Annual Average Reward      = 1.175\n",
      "\tTotal Violations                   = 70.0\n",
      "****************************************\n",
      "\n",
      "LAST PHASE ITERATION #2:  TOKYO, 2001 \n",
      "Average Reward \t\t= 1.402\n",
      "Violation Counter \t= 1\n",
      "***MEASURING PERFORMANCE OF THE MODEL***\n",
      "\tBEST AVERAGE ANNUAL AVERAGE REWARD = 1.316\n",
      "\tBEST TOTAL VIOLATIONS              = 26.0\n",
      "\n",
      "\tAverage Annual Average Reward      = 1.490\n",
      "\tTotal Violations                   = 0.0\n",
      "****************************************\n",
      "\n",
      "LAST PHASE ITERATION #3:  TOKYO, 2001 \n",
      "Average Reward \t\t= 1.448\n",
      "Violation Counter \t= 0\n",
      "***MEASURING PERFORMANCE OF THE MODEL***\n",
      "\tBEST AVERAGE ANNUAL AVERAGE REWARD = 1.490\n",
      "\tBEST TOTAL VIOLATIONS              = 0.0\n",
      "\n",
      "\tAverage Annual Average Reward      = 1.445\n",
      "\tTotal Violations                   = 0.0\n",
      "****************************************\n",
      "\n",
      "LAST PHASE ITERATION #4:  TOKYO, 2008 \n",
      "Average Reward \t\t= 1.455\n",
      "Violation Counter \t= 0\n",
      "***MEASURING PERFORMANCE OF THE MODEL***\n",
      "\tBEST AVERAGE ANNUAL AVERAGE REWARD = 1.490\n",
      "\tBEST TOTAL VIOLATIONS              = 0.0\n",
      "\n",
      "\tAverage Annual Average Reward      = 1.472\n",
      "\tTotal Violations                   = 0.0\n",
      "****************************************\n",
      "\n",
      "LAST PHASE ITERATION #5:  TOKYO, 2002 \n",
      "Average Reward \t\t= 1.201\n",
      "Violation Counter \t= 6\n",
      "***MEASURING PERFORMANCE OF THE MODEL***\n",
      "\tBEST AVERAGE ANNUAL AVERAGE REWARD = 1.490\n",
      "\tBEST TOTAL VIOLATIONS              = 0.0\n",
      "\n",
      "\tAverage Annual Average Reward      = 1.356\n",
      "\tTotal Violations                   = 0.0\n",
      "****************************************\n",
      "\n",
      "LAST PHASE ITERATION #6:  TOKYO, 2008 \n",
      "Average Reward \t\t= 1.427\n",
      "Violation Counter \t= 0\n",
      "***MEASURING PERFORMANCE OF THE MODEL***\n",
      "\tBEST AVERAGE ANNUAL AVERAGE REWARD = 1.490\n",
      "\tBEST TOTAL VIOLATIONS              = 0.0\n",
      "\n",
      "\tAverage Annual Average Reward      = 1.429\n",
      "\tTotal Violations                   = 2.0\n",
      "****************************************\n",
      "\n",
      "LAST PHASE ITERATION #7:  TOKYO, 2000 \n",
      "Average Reward \t\t= 1.346\n",
      "Violation Counter \t= 3\n",
      "***MEASURING PERFORMANCE OF THE MODEL***\n",
      "\tBEST AVERAGE ANNUAL AVERAGE REWARD = 1.490\n",
      "\tBEST TOTAL VIOLATIONS              = 0.0\n",
      "\n",
      "\tAverage Annual Average Reward      = 1.455\n",
      "\tTotal Violations                   = 1.0\n",
      "****************************************\n",
      "\n",
      "LAST PHASE ITERATION #8:  TOKYO, 2002 \n",
      "Average Reward \t\t= 1.267\n",
      "Violation Counter \t= 3\n",
      "***MEASURING PERFORMANCE OF THE MODEL***\n",
      "\tBEST AVERAGE ANNUAL AVERAGE REWARD = 1.490\n",
      "\tBEST TOTAL VIOLATIONS              = 0.0\n",
      "\n",
      "\tAverage Annual Average Reward      = 1.376\n",
      "\tTotal Violations                   = 0.0\n",
      "****************************************\n",
      "\n",
      "LAST PHASE ITERATION #9:  TOKYO, 2001 \n",
      "Average Reward \t\t= 1.195\n",
      "Violation Counter \t= 9\n",
      "***MEASURING PERFORMANCE OF THE MODEL***\n",
      "\tBEST AVERAGE ANNUAL AVERAGE REWARD = 1.490\n",
      "\tBEST TOTAL VIOLATIONS              = 0.0\n",
      "\n",
      "\tAverage Annual Average Reward      = 1.482\n",
      "\tTotal Violations                   = 1.0\n",
      "****************************************\n"
     ]
    }
   ],
   "source": [
    "#END OF TRAINING PHASE - CHOOSING THE BEST MODEL INSTANCE\n",
    "#INCREASE GREEDY RATE\n",
    "#VALIDATE AFTER EVERY ITERATION\n",
    "\n",
    "# Use this model and its output as base standards for the last phase of training\n",
    "best_avg_avg_reward = -1000\n",
    "best_net_avg_reward = dqn.eval_net\n",
    "best_avg_v_counter = 1000\n",
    "best_net_v_counter = dqn.eval_net\n",
    "\n",
    "\n",
    "NO_OF_LAST_PHASE_ITERATIONS = 10\n",
    "EPSILON = 0.98\n",
    "LR = 0.00005\n",
    "\n",
    "print(\"EPSILON = \", EPSILON)\n",
    "print(\"LR      = \", LR)\n",
    "\n",
    "for iteration in range(NO_OF_LAST_PHASE_ITERATIONS):\n",
    "    LOCATION = 'tokyo'#random.choice(['tokyo','wakkanai','minamidaito'])\n",
    "    YEAR = random.choice(np.arange(2000,2010))\n",
    "    capm = CAPM(LOCATION,YEAR,shuffle=False, trainmode=False) #instantiate the CAPM class\n",
    "    capm.eno = ENO(LOCATION,YEAR, shuffle=False, day_balance=False) #instantiate the environment inside the CAPM class\n",
    "    capm.HMAX = capm.eno.SMAX #maximum power output of solar cell is set in CAPM object using the value in ENO object\n",
    "\n",
    "    print('\\nLAST PHASE ITERATION #{}:  {}, {} '.format(iteration, LOCATION.upper(), YEAR))\n",
    "    \n",
    "    \n",
    "    my_avg_reward = -1000\n",
    "    my_v_counter = 1000\n",
    "    \n",
    "    s, r, day_end, year_end = capm.reset()\n",
    "    yr_record = np.empty(4)\n",
    "\n",
    "    record = np.empty(4) #record for battery, henergy, reward and action\n",
    "    transition_rec = np.zeros((capm.eno.TIME_STEPS, N_STATES * 2 + 2)) #record all the transition in one day\n",
    "\n",
    "    while True:\n",
    "        a = dqn.choose_action(stdize(s))\n",
    "\n",
    "        # present state = [batt, enp, henergy]\n",
    "        record = np.vstack((record, [s[0],s[2],r, a])) # record battery, henergy, reward and action for troubleshooting\n",
    "        yr_record = np.vstack((yr_record, [s[0],s[2],r, a]))\n",
    "\n",
    "        # take action\n",
    "        s_, r, day_end, year_end = capm.step(a)\n",
    "        \n",
    "        temp_transitions = np.hstack((stdize(s), [a, r], stdize(s_)))\n",
    "        transition_rec[capm.eno.hr-1,:] = temp_transitions\n",
    "\n",
    "        if (day_end):\n",
    "            transition_rec[:,5] += r #broadcast reward to all states\n",
    "            decay_factor = [i for i in (LAMBDA**n for n in reversed(range(0, capm.eno.TIME_STEPS)))]\n",
    "            transition_rec[:,5] = transition_rec[:,5] * decay_factor #decay reward proportionately\n",
    "            dqn.store_day_transition(transition_rec)\n",
    "\n",
    "        if dqn.memory_counter > MEMORY_CAPACITY:\n",
    "            dqn.learn()\n",
    "\n",
    "        if dqn.nettoggle:\n",
    "            change_hr = capm.eno.day*24+capm.eno.hr #to mark when the DQN is updated.\n",
    "            dqn.nettoggle = not dqn.nettoggle\n",
    "\n",
    "        if (year_end):\n",
    "            break\n",
    "\n",
    "        # transition to new state\n",
    "        s = s_\n",
    "\n",
    "    record = np.delete(record, 0, 0) #remove the first row which is garbage\n",
    "    reward_rec = record[:,2] #extract reward information from the record array\n",
    "    reward_rec = reward_rec[::24] #only consider terminal rewards\n",
    "    print(\"Average Reward \\t\\t= {:.3f}\".format(np.mean(reward_rec)))\n",
    "    print(\"Violation Counter \\t= {}\".format(capm.violation_counter))\n",
    "    \n",
    "    \n",
    "    print(\"***MEASURING PERFORMANCE OF THE MODEL***\")\n",
    "    print(\"\\tBEST AVERAGE ANNUAL AVERAGE REWARD = {:.3f}\".format(best_avg_avg_reward))\n",
    "    print(\"\\tBEST TOTAL VIOLATIONS              = {}\".format(best_avg_v_counter))\n",
    "    LOCATION = 'tokyo'\n",
    "    results = np.empty(3)\n",
    "    for YEAR in np.arange(2010,2015):\n",
    "        capm = CAPM(LOCATION,YEAR,shuffle=False, trainmode=False) #instantiate the CAPM class\n",
    "        capm.eno = ENO(LOCATION,YEAR, shuffle=False, day_balance=False) #instantiate the environment inside the CAPM class\n",
    "        capm.HMAX = capm.eno.SMAX #maximum power output of solar cell is set in CAPM object using the value in ENO object\n",
    "\n",
    "        s, r, day_end, year_end = capm.reset()\n",
    "        yr_test_record = np.empty(4)\n",
    "\n",
    "        while True:\n",
    "            a = dqn.choose_greedy_action(stdize(s))\n",
    "            #state = [batt, enp, henergy, fcast]\n",
    "            yr_test_record = np.vstack((yr_test_record, [s[0],s[2],r, a])) #record battery, henergy, reward and action\n",
    "            # take action\n",
    "            s_, r, day_end, year_end = capm.step(a)\n",
    "            if year_end:\n",
    "                break\n",
    "            s = s_\n",
    "\n",
    "        yr_test_record = np.delete(yr_test_record, 0, 0) #remove the first row which is garbage\n",
    "        yr_test_reward_rec = yr_test_record[:,2]\n",
    "        yr_test_reward_rec = yr_test_reward_rec[::24] #annual average reward\n",
    "        results = np.vstack((results, [int(YEAR), np.mean(yr_test_reward_rec), int(capm.violation_counter)]))\n",
    "    results = np.delete(results,0,0)\n",
    "    my_avg_reward = np.mean(results[:,1]) #the average of annual average rewards\n",
    "    my_v_counter = np.sum(results[:,-1]) #total sum of violations\n",
    "    print(\"\\n\\tAverage Annual Average Reward      = {:.3f}\".format(my_avg_reward))\n",
    "    print(\"\\tTotal Violations                   = {}\".format(my_v_counter))\n",
    "\n",
    "    if (my_avg_reward > best_avg_avg_reward):\n",
    "            best_avg_avg_reward = my_avg_reward\n",
    "            best_net_avg_reward = dqn.eval_net\n",
    "\n",
    "    if (my_v_counter < best_avg_v_counter):\n",
    "        best_avg_v_counter = my_v_counter\n",
    "        best_net_v_counter = dqn.eval_net\n",
    "    elif (my_v_counter == best_avg_v_counter):\n",
    "        if (my_avg_reward > best_avg_avg_reward):\n",
    "            best_avg_v_counter = my_v_counter\n",
    "            best_net_v_counter = dqn.eval_net\n",
    "    print(\"****************************************\")\n",
    "\n",
    "    \n",
    "\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TESTING BEST MODEL BASED ON AVERAGE REWARD METRIC\n",
      "YEAR\t\tAVG_RWD\t\tVIOLATIONS\n",
      "2015 \t\t 1.4 \t\t 0\n",
      "2016 \t\t 1.46 \t\t 0\n",
      "2017 \t\t 1.43 \t\t 4\n",
      "2018 \t\t 1.45 \t\t 0\n"
     ]
    }
   ],
   "source": [
    "#TESTING BEST MODEL BASED ON AVERAGE REWARD METRIC\n",
    "dqn = DQN()\n",
    "dqn.eval_net = best_net_avg_reward\n",
    "LOCATION = 'tokyo'\n",
    "results = np.empty(3)\n",
    "for YEAR in np.arange(2015,2019):\n",
    "    capm = CAPM(LOCATION,YEAR,shuffle=False, trainmode=False) #instantiate the CAPM class\n",
    "    capm.eno = ENO(LOCATION,YEAR, shuffle=False, day_balance=False) #instantiate the environment inside the CAPM class\n",
    "    capm.HMAX = capm.eno.SMAX #maximum power output of solar cell is set in CAPM object using the value in ENO object\n",
    "\n",
    "    s, r, day_end, year_end = capm.reset()\n",
    "    yr_test_record = np.empty(4)\n",
    "\n",
    "    while True:\n",
    "        a = dqn.choose_greedy_action(stdize(s))\n",
    "\n",
    "        #state = [batt, enp, henergy, fcast]\n",
    "        yr_test_record = np.vstack((yr_test_record, [s[0],s[2],r, a])) #record battery, henergy, reward and action\n",
    "\n",
    "        # take action\n",
    "        s_, r, day_end, year_end = capm.step(a)\n",
    "\n",
    "        if year_end:\n",
    "            break\n",
    "\n",
    "        s = s_\n",
    "\n",
    "    yr_test_record = np.delete(yr_test_record, 0, 0) #remove the first row which is garbage\n",
    "\n",
    "#     #Plot the reward and battery for the entire year run\n",
    "#     title = LOCATION.upper() + ',' + str(YEAR)\n",
    "\n",
    "#     NO_OF_DAYS = capm.eno.NO_OF_DAYS\n",
    "    yr_test_reward_rec = yr_test_record[:,2]\n",
    "    yr_test_reward_rec = yr_test_reward_rec[::24]\n",
    "#     print('Average Reward for',title, '=', np.mean(yr_test_reward_rec))\n",
    "#     print('Violations for',title, '=', capm.violation_counter)\n",
    "    \n",
    "    results = np.vstack((results, [int(YEAR), np.mean(yr_test_reward_rec), int(capm.violation_counter)]))\n",
    "\n",
    "#     fig = plt.figure(figsize=(24,3))\n",
    "#     fig.suptitle(title, fontsize=15)\n",
    "\n",
    "#     #     ax1 = fig.add_subplot(211)\n",
    "#     #     ax1.plot(yr_test_reward_rec)\n",
    "#     #     ax1.set_title(\"\\n\\nYear Run Reward\")\n",
    "#     #     ax1.set_ylim([-3,1])\n",
    "\n",
    "#     ax2 = fig.add_subplot(111)\n",
    "#     ax2.plot(yr_test_record[:,0],'r')\n",
    "#     ax2.set_title(\"\\n\\nYear Run Battery\")\n",
    "#     ax2.set_ylim([0,1])\n",
    "#     plt.sca(ax2)\n",
    "#     plt.xticks(np.arange(0, NO_OF_DAYS*24, 50*24),np.arange(0,NO_OF_DAYS,50))\n",
    "\n",
    "#     fig.tight_layout()\n",
    "#     plt.show()\n",
    "\n",
    "results = np.delete(results,0,0)\n",
    "\n",
    "print(\"TESTING BEST MODEL BASED ON AVERAGE REWARD METRIC\")\n",
    "print('YEAR\\t\\tAVG_RWD\\t\\tVIOLATIONS')\n",
    "for x in np.arange(0,results.shape[0]):\n",
    "    print('{} \\t\\t {} \\t\\t {}'.format(int(results[x,0]), np.around(results[x,1],2), int(results[x,-1])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TESTING BASED ON VIOLATION COUNTER METRIC\n",
      "YEAR\t\tAVG_RWD\t\tVIOLATIONS\n",
      "2015 \t\t 1.4 \t\t 0\n",
      "2016 \t\t 1.46 \t\t 0\n",
      "2017 \t\t 1.43 \t\t 4\n",
      "2018 \t\t 1.45 \t\t 0\n"
     ]
    }
   ],
   "source": [
    "#TESTING BASED ON VIOLATION COUNTER METRIC\n",
    "dqn = DQN()\n",
    "dqn.eval_net = best_net_v_counter\n",
    "\n",
    "LOCATION = 'tokyo'\n",
    "results = np.empty(3)\n",
    "for YEAR in np.arange(2015,2019):\n",
    "    capm = CAPM(LOCATION,YEAR,shuffle=False, trainmode=False) #instantiate the CAPM class\n",
    "    capm.eno = ENO(LOCATION,YEAR, shuffle=False, day_balance=False) #instantiate the environment inside the CAPM class\n",
    "    capm.HMAX = capm.eno.SMAX #maximum power output of solar cell is set in CAPM object using the value in ENO object\n",
    "\n",
    "    s, r, day_end, year_end = capm.reset()\n",
    "    yr_test_record = np.empty(4)\n",
    "\n",
    "    while True:\n",
    "        a = dqn.choose_greedy_action(stdize(s))\n",
    "\n",
    "        #state = [batt, enp, henergy, fcast]\n",
    "        yr_test_record = np.vstack((yr_test_record, [s[0],s[2],r, a])) #record battery, henergy, reward and action\n",
    "\n",
    "        # take action\n",
    "        s_, r, day_end, year_end = capm.step(a)\n",
    "\n",
    "        if year_end:\n",
    "            break\n",
    "\n",
    "        s = s_\n",
    "\n",
    "    yr_test_record = np.delete(yr_test_record, 0, 0) #remove the first row which is garbage\n",
    "\n",
    "#     #Plot the reward and battery for the entire year run\n",
    "#     title = LOCATION.upper() + ',' + str(YEAR)\n",
    "\n",
    "#     NO_OF_DAYS = capm.eno.NO_OF_DAYS\n",
    "    yr_test_reward_rec = yr_test_record[:,2]\n",
    "    yr_test_reward_rec = yr_test_reward_rec[::24]\n",
    "#     print('Average Reward for',title, '=', np.mean(yr_test_reward_rec))\n",
    "#     print('Violations for',title, '=', capm.violation_counter)\n",
    "    \n",
    "    results = np.vstack((results, [int(YEAR), np.mean(yr_test_reward_rec), int(capm.violation_counter)]))\n",
    "\n",
    "#     fig = plt.figure(figsize=(24,3))\n",
    "#     fig.suptitle(title, fontsize=15)\n",
    "\n",
    "#     #     ax1 = fig.add_subplot(211)\n",
    "#     #     ax1.plot(yr_test_reward_rec)\n",
    "#     #     ax1.set_title(\"\\n\\nYear Run Reward\")\n",
    "#     #     ax1.set_ylim([-3,1])\n",
    "\n",
    "#     ax2 = fig.add_subplot(111)\n",
    "#     ax2.plot(yr_test_record[:,0],'r')\n",
    "#     ax2.set_title(\"\\n\\nYear Run Battery\")\n",
    "#     ax2.set_ylim([0,1])\n",
    "#     plt.sca(ax2)\n",
    "#     plt.xticks(np.arange(0, NO_OF_DAYS*24, 50*24),np.arange(0,NO_OF_DAYS,50))\n",
    "\n",
    "#     fig.tight_layout()\n",
    "#     plt.show()\n",
    "\n",
    "results = np.delete(results,0,0)\n",
    "\n",
    "print(\"TESTING BASED ON VIOLATION COUNTER METRIC\")\n",
    "print('YEAR\\t\\tAVG_RWD\\t\\tVIOLATIONS')\n",
    "for x in np.arange(0,results.shape[0]):\n",
    "    print('{} \\t\\t {} \\t\\t {}'.format(int(results[x,0]), np.around(results[x,1],2), int(results[x,-1])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #Plot the reward and battery for the entire year run on a day by day basis\n",
    "# title = LOCATION.upper() + ',' + str(YEAR)\n",
    "# TIME_AXIS = np.arange(0,capm.eno.TIME_STEPS)\n",
    "# for DAY in range(0,10):#capm.eno.NO_OF_DAYS):\n",
    "#     START = DAY*24\n",
    "#     END = START+24\n",
    "\n",
    "#     daytitle = title + ' - DAY ' + str(DAY)\n",
    "#     fig = plt.figure(figsize=(16,4))\n",
    "#     st = fig.suptitle(daytitle)\n",
    "\n",
    "#     ax2 = fig.add_subplot(121)\n",
    "#     ax2.plot(yr_test_record[START:END,1],'g')\n",
    "#     ax2.set_title(\"HARVESTED ENERGY\")\n",
    "#     plt.xlabel(\"Hour\")\n",
    "#     ax2.set_ylim([0,1])\n",
    "\n",
    "#     #plot battery for year run\n",
    "#     ax1 = fig.add_subplot(122)\n",
    "#     ax1.plot(TIME_AXIS,yr_test_record[START:END,0],'r') \n",
    "# #     ax1.plot(TIME_AXIS, np.ones(capm.eno.TIME_STEPS)*capm.BOPT/capm.BMAX,'r--')\n",
    "#     ax1.plot(TIME_AXIS, np.ones(capm.eno.TIME_STEPS)*capm.BOPT/capm.BMAX,'r--')\n",
    "#     ax1.text(0.1, 0.2, \"BINIT = %.2f\\n\" %(yr_test_record[START,0]),fontsize=11, ha='left')\n",
    "#     ax1.text(0.1, 0.4, \"TENP = %.2f\\n\" %(capm.BOPT/capm.BMAX-yr_test_record[END,0]),fontsize=11, ha='left')\n",
    "#     ax1.text(0.1, 0.3, \"BMEAN = %.2f\\n\" %(np.mean(yr_test_record[START:END,0])),fontsize=11, ha='left')\n",
    "\n",
    "\n",
    "\n",
    "#     ax1.set_title(\"YEAR RUN TEST\")\n",
    "#     if END < (capm.eno.NO_OF_DAYS*capm.eno.TIME_STEPS):\n",
    "#         ax1.text(0.1, 0, \"REWARD = %.2f\\n\" %(yr_test_record[END,2]),fontsize=13, ha='left')\n",
    "#     plt.xlabel(\"Hour\")\n",
    "#     ax1.set_ylabel('Battery', color='r',fontsize=12)\n",
    "#     ax1.set_ylim([0,1])\n",
    "\n",
    "#     #plot actions for year run\n",
    "#     ax1a = ax1.twinx()\n",
    "#     ax1a.plot(yr_test_record[START:END,3])\n",
    "#     ax1a.set_ylim([0,N_ACTIONS])\n",
    "#     ax1a.set_ylabel('Duty Cycle', color='b',fontsize=12)\n",
    "\n",
    "#     fig.tight_layout()\n",
    "#     st.set_y(0.95)\n",
    "#     fig.subplots_adjust(top=0.75)\n",
    "#     plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
